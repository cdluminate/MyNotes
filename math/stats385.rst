Stanford Stats 385, Theories of Deep Learning, 2017
===================================================

Course structure:

  1. Review of DL concepts
  2. Review theoretical approaches
  3. specific theoretical contributions
  4. postmortem

Reference: stats285, cs224n, cs229, cs231n

Lec 1: deep learning challenge: is there a theory?
--------------------------------------------------

What is a theory.

  (1) Vulgar meaning - any model, any set of formal arguments
  (2) precise meaning - models that explain and that predict

What can theory contribute? Analysis, prediction.

Can there be theories of deep learning?

Neural science.

  Visual neural science. simple cells (first layer), complex cells (pooling
  layer). gradmother cells (last layer)

Harmonic analysis.

  (1) optimal representations - eigenfunctions - fourier.
  (2) multiscal representation - wavelets - almost eigenfunctions.
  (3) geometric multiscale - Ridgelets, curvelets, shearlets.
  (4) scattering transform
  (5) sparse representation

Approximation theory.

  Class prediction rule: function of high-dimensional argument.
  Curse of dimensionality. Ridge functions. Compositional functions
  and deep composition.

Modern statistical theory (machine learning)

  (1) estimation of functions in high dimension
  (2) VC class/generalization bound
  (3) statistical models
  (4) regularization, overfitting, model selection
  (5) experimental design
  (6) observational and experimental data
  (7) ecological correlation / correlation vs causation.


Lec02: TODO
-----------

Reference
---------

1. https://stats385.github.io
