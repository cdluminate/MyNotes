#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
\usetheme{Boadilla}
% or ...

\setbeamercovered{transparent}
% or whatever (possibly just delete it)
\end_preamble
\use_default_options false
\maintain_unincluded_children no
\language english
\language_package default
\inputencoding utf8
\fontencoding auto
\font_roman "times" "default"
\font_sans "biolinum" "default"
\font_typewriter "lmtt" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures false
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Title
Adversarial Attack and Defense
\end_layout

\begin_layout Subtitle
\begin_inset Quotes eld
\end_inset

AI Security Issues
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Author
Mo Zhou
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
and
\end_layout

\end_inset

 Vishal M.
 Patel
\end_layout

\begin_layout Institute
Deep Learning (EN.520.638)
\begin_inset Newline newline
\end_inset

Spring 2025
\begin_inset Newline newline
\end_inset

Johns Hopkins University
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Outline
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Safety & Security Issues of AI
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Section 1:
 Safety & Security Issues of AI
\end_layout

\end_inset


\end_layout

\begin_layout Frame
There are lots of deep learning applications in real-world.
 But how secure are them?
 How robust are they when they face malicious users?
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Deep learning models have a wide range of use cases.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[1] ImageNet Classification with Deep Convolutional Neural Networks (Alexnet)
\end_layout

\begin_layout Standard
Example in computer vision:
 image classification.
\end_layout

\begin_layout Columns

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

0.65
\backslash
columnwidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted2.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

0.35
\backslash
columnwidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted1.png
	special width=\linewidth

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
Input:
 RGB image of shape (224,
 224,
 3)
\end_layout

\begin_layout Standard
Output:
 Category probability for 1000 classes (from ImageNet dataset).
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Deep learning models have a wide range of use cases.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[2] Fast R-CNN,
 https://arxiv.org/pdf/1504.08083
\end_layout

\end_deeper
\begin_layout Frame
Example in computer vision:
 object detection.
\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted3.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Frame
Input:
 RGB image.
\end_layout

\begin_layout Frame
Output:
 a number of (bounding boxes,
 classification result).
\end_layout

\begin_layout Frame
\begin_inset Formula $\rightarrow$
\end_inset

 
\begin_inset Quotes eld
\end_inset

what objects are in this image,
 and where are they?
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Deep learning models have a wide range of use cases.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[3] Language Models are Unsupervised Multitask Learners (GPT-2)
\end_layout

\end_deeper
\begin_layout Frame
Example in computational linguistics:
 next-word prediction.
\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Language Modeling
\end_layout

\end_inset


\end_layout

\begin_layout Block
Assume we have a natural language example 
\begin_inset Formula $x,$
\end_inset

 which is composed of a sequence of tokens 
\begin_inset Formula $(s_{1,}s_{2},\ldots,s_{n})$
\end_inset

.
 Since language has a natural sequential ordering,
 it is common to factorize the joint probabilities over tokens as the product of conditional probabilities,
 i.e.,
\end_layout

\begin_layout Block
\begin_inset Formula 
\[
p(x)=p(s_{1},s_{2},\ldots,s_{n})=\prod_{i=1}^{n}p(s_{n}|s_{1},\ldots,s_{n-1})
\]

\end_inset


\end_layout

\begin_layout Standard
Input:
 previous tokens (e.g.,
 an unfinished sentence)
\end_layout

\begin_layout Standard
Output:
 the next token (or word)
\end_layout

\begin_layout Standard
Auto-regressive:
 append the predicted token to input,
 and predict again.
\end_layout

\end_deeper
\begin_layout Subsection
Vulnerabilities exist in all deep learning models.
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Are deep learning models reliable?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[4] Explaining and Harnessing Adversarial Examples (ICLR 2015)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted5.png
	special width=\linewidth

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Are deep learning models reliable?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[5] Adversarial Patch,
 https://arxiv.org/pdf/1712.09665
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted4.png
	special width=\linewidth

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Are deep learning models reliable?
 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[6] Robust Physical-World Attacks on Deep Learning Visual Classification,
 CVPR 2018
\end_layout

\begin_layout Columns

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

0.5
\backslash
linewidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted6.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

0.5
\backslash
linewidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted7.png
	special width=\linewidth

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Language models too...
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[7] Jailbreaking Black Box Large Language Models in Twenty Queries
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted8.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Standard

\series bold
\bar under
But today we will mainly cover the computer vision models,
 in particular image classification models.
\end_layout

\end_deeper
\begin_layout Section
Adversarial Attack:
 Compromising an AI Model
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Section 2:
 Adversarial Attack:
 Compromising a Deep Learning Model
\end_layout

\end_inset


\end_layout

\begin_layout Frame
To manipulate the input to the deep learning model,
 and hence intentionally change the output of the model.
\end_layout

\begin_layout Frame
Take 
\series bold
face recognition
\series default
 models,
 they may be attacked for:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Quotes eld
\end_inset


\series bold
Impersonation
\series default

\begin_inset Quotes erd
\end_inset

:
 let the model recognize the user as another (high privilege) user.
\end_layout

\begin_deeper
\begin_layout Itemize
use case:
 payment authorization / phone unlock with face.
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
\begin_inset Quotes eld
\end_inset

Avoiding detection
\begin_inset Quotes erd
\end_inset


\series default
:
 let the model recognize the user as anybody else.
\end_layout

\begin_deeper
\begin_layout Itemize
use case:
 criminal avoids the camera surveillance system.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
How does 
\begin_inset Quotes eld
\end_inset

Adversarial Attack
\begin_inset Quotes erd
\end_inset

 happen?
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Columns

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

0.45
\backslash
linewidth
\end_layout

\end_inset


\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
When Training A Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Constants:
 input image.
\end_layout

\begin_layout Itemize
Variables:
 model parameters.
\end_layout

\begin_layout Standard
The optimization algorithm (e.g.,
 SGD,
 Adam) updates the 
\series bold
model parameters
\series default
.
\end_layout

\end_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

0.45
\backslash
linewidth
\end_layout

\end_inset


\end_layout

\begin_layout AlertBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
When Attacking A model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Constants:
 model parameters.
\end_layout

\begin_layout Itemize
Variables:
 input image.
\end_layout

\begin_layout Standard
The attack algorithm (e.g.,
 PGD,
 NES) updates the 
\series bold
input image
\series default
.
\end_layout

\end_deeper
\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Definition:
 
\begin_inset Quotes eld
\end_inset

Adversarial Example
\begin_inset Quotes erd
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Adversarial example is the input that triggers model misbehavior.
\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Adversarial Example
\end_layout

\end_inset


\end_layout

\begin_layout Block
Given an image 
\begin_inset Formula $x$
\end_inset

,
 the adversarial example 
\begin_inset Formula $x_{adv}$
\end_inset

 is the image itself with a perturbation 
\begin_inset Formula $\delta$
\end_inset

,
\begin_inset Formula 
\[
x_{adv}=x+\delta.
\]

\end_inset


\end_layout

\begin_layout Block
It triggers the model 
\begin_inset Formula $f(\cdot)$
\end_inset

 misbehavior,
 such as untargeted attack:
\begin_inset Formula 
\[
f(x_{adv})\neq f(x)
\]

\end_inset


\end_layout

\begin_layout Block
Or targeted attack:
\begin_inset Formula 
\[
f(x_{adv})=y_{target}\neq y_{original}
\]

\end_inset


\end_layout

\begin_layout Block
The perturbation needs to be imperceptible by human (or it is too easy to tell something wrong on the image).
 The literature usually use the 
\begin_inset Formula $L_{\infty}$
\end_inset

 norm or 
\begin_inset Formula $L_{2}$
\end_inset

 norm for this constraint.
\end_layout

\end_deeper
\begin_layout Subsection
Threat Model:
 White-box,
 Grey-box,
 and Black-box.
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Threat Models:
 White-box and Black-box
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
The amount of knowledge the attacker has.
 (computer security term)
\end_layout

\begin_layout Standard

\bar under
\begin_inset Formula $\uparrow$
\end_inset

 easier,
 but less practical.
\end_layout

\begin_layout Description
\begin_inset ERT
status open

\begin_layout Plain Layout

White Box:
\end_layout

\end_inset

 The attacker has the 
\series bold
full knowledge
\series default
 about the architecture,
 the weights (parameters) of the model to be attacked.
\end_layout

\begin_layout Description
\begin_inset ERT
status open

\begin_layout Plain Layout

Black Box:
\end_layout

\end_inset

 The attacker has 
\series bold
very limited knowledge
\series default
 about the model,
 e.g.,
 can only access the input and output of the model – all the rest are unknown.
\end_layout

\begin_layout Standard

\bar under
\begin_inset Formula $\downarrow$
\end_inset

more difficult,
 but more practical.
\end_layout

\end_deeper
\begin_layout Subsection
White-Box Adversarial Attacks
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
White-box Attacks
\end_layout

\end_inset


\end_layout

\begin_layout Frame
The attacker has the 
\series bold
full knowledge
\series default
 about the architecture,
 the weights (parameters) of the model to be attacked.
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Core idea:

\series default
 gradient-based optimization.
\end_layout

\begin_layout Itemize

\series bold
Intuition:

\series default
 Move the input in the direction that increases the loss (for untargeted) or towards the target class loss.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
White-box Attack:
 Fast Gradient Sign Method
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[8] Explaining and Harnessing Adversarial Examples
\end_layout

\end_deeper
\begin_layout Frame
This is a one-step algorithm,
 in the direction of the sign of the gradient.
\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Fast Gradient Sign Method (FGSM)
\end_layout

\end_inset


\begin_inset Formula 
\[
x_{adv}=x+\epsilon\cdot\text{sign}(\nabla_{x}J(\theta,x,y))
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
Problem:
 Fast but often not optimal.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
White-box Attack:
 Basic Iterative Method
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[9] adversarial machine learning at scale
\begin_inset Newline newline
\end_inset

[10] Towards Deep Learning Models Resistant to Adversarial Attacks
\end_layout

\end_deeper
\begin_layout Frame
This is an iterative algorithm that runs multiple small steps,
 with projection back into the allowed 
\begin_inset Formula $\epsilon$
\end_inset

-ball (using the chosen 
\begin_inset Formula $L_{p}$
\end_inset

 norm).
\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Basic Iterative Method
\end_layout

\end_inset


\begin_inset Formula 
\[
x_{adv}^{t+1}=\prod_{\epsilon}(x_{adv}^{t}+\alpha\cdot\text{sign}(\nabla_{x}J(\theta,x_{adv}^{t},y)))
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Frame

\color lightgray
Note,
 this is also called PGD sometimes based on whether the perturbation is randomly initialized or not.
\end_layout

\begin_deeper
\begin_layout Itemize
It is stronger than FGSM because the model is not linear,
 such that the algorithm can better exploit the input space for a better point for attack.
\end_layout

\begin_layout Itemize
PGD is often considered the standard strong baseline.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Black-box Adversarial Attacks
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Black-box Adversarial Attacks
\end_layout

\end_inset


\end_layout

\begin_layout Frame
The attacker has 
\series bold
very limited knowledge
\series default
 about the model,
 e.g.,
 can only access the input and output of the model – all the rest are unknown.
\end_layout

\begin_deeper
\begin_layout Itemize
Transferability and universal perturbations:
\end_layout

\begin_deeper
\begin_layout Itemize
find adversarial example on a known model A.
\end_layout

\begin_layout Itemize
apply the adversarial example to an unknown model B.
\end_layout

\end_deeper
\begin_layout Itemize
Score-based and decision-based method
\end_layout

\begin_deeper
\begin_layout Itemize
Repeatedly query the target model
\end_layout

\begin_layout Itemize
use the output to estimate gradients or find adversarial examples directly
\end_layout

\begin_layout Itemize
Repeat until maximum time of query or the goal is achieved.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Black/Gray-box:
 Adversarial Example Transferrability
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[11] Improving Transferability of Adversarial Examples with Input Diversity
\begin_inset Newline newline
\end_inset


\bar under
* Agnostic to model.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Quotes eld
\end_inset

Transferrability
\begin_inset Quotes erd
\end_inset

:
 Adversarial examples crafted for one model often fool other models (even with different architectures).
\end_layout

\begin_layout Frame
Attack Strategy:
 Train a local "substitute" model,
 craft white-box attacks,
 and transfer them to the target black-box model.
\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted10.png
	special width=0.7\linewidth

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Black/Gray-box:
 Universal Adversarial Perturbation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[12] Universal adversarial perturbations
\begin_inset Newline newline
\end_inset


\bar under
* Agnostic to images
\end_layout

\begin_layout Columns

\end_layout

\begin_deeper
\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

0.4
\backslash
linewidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted9.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Column
\begin_inset ERT
status open

\begin_layout Plain Layout

0.6
\backslash
linewidth
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted11.png
	special width=\linewidth

\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Black-box Attack:
 Natural Evolution Strategy (1/2)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[13] Black-box Adversarial Attacks with Limited Queries and Information
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $F(x)$
\end_inset

 be the objective function,
 and 
\begin_inset Formula $\pi(\theta|x)$
\end_inset

 be the search distribution,
 e.g.,
 Gaussian 
\begin_inset Formula $\delta\sim\mathcal{N}(0,I)$
\end_inset

,
 so that 
\begin_inset Formula $\theta=x+\sigma\delta$
\end_inset

.
 Here is a math trick:
\begin_inset Formula 
\begin{align*}
\mathbb{E}_{\pi(\theta|x)}[F(\theta)] & =\int F(\theta)\pi(\theta|x)d\theta\\
\nabla_{x}\mathbb{E}_{\pi(\theta|x)}[F(\theta)] & =\nabla_{x}\int F(\theta)\pi(\theta|x)d\theta\\
 & =\int F(\theta)\nabla_{x}\pi(\theta|x)d\theta\\
 & =\int F(\theta){\color{red}\frac{{\normalcolor \pi(\theta|x)}}{\pi(\theta|x)}}{\color{red}\nabla_{x}\pi(\theta|x)}d\theta\\
 & =\int\pi(\theta|x)F(\theta){\color{red}\nabla_{x}\log(\pi(\theta|x))}d\theta\\
 & =\mathbb{E}_{\pi(\theta|x)}\big[F(\theta)\nabla_{x}\log(\pi(\theta|x))\big]
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Black-box Attack:
 Natural Evolution Strategy (2/2)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
[13] Black-box Adversarial Attacks with Limited Queries and Information
\end_layout

\begin_layout Standard
When we use Gaussian as the search distribution,
 based on the previous page,
 we have
\begin_inset Formula 
\[
\nabla\mathbb{E}[F(\theta)]\approx\frac{1}{\sigma n}\sum_{i=1}^{n}\delta_{i}F(\theta+\sigma\delta_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
That means the gradient can be estimated through a population of randomly sampled 
\begin_inset Formula $n$
\end_inset

 points.
 We can adopt this estimated gradient and perform basic iterative method mentioned before.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted12.png
	special width=0.5\linewidth

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Comparison between white-box and black-box
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="3">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
White-Box Attack
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Black-Box Attack
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Effectiveness
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
high
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
much less than white-box
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Efficiency (cost)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
low
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
much higher (e.g.
 payed API calls)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Practical?
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
much better than white-box
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
,
\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Frame
There are also some more challenging scenarios for physical world attacks.
 They can survive the camera system as well compared to the attacks in the pure digital format.
 That's beyond the scope of discussion today.
\end_layout

\begin_layout Section
Adversarial Defense:
 Prevent the Attacks
\end_layout

\begin_layout Subsection
Adversarial Defense
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Section 3:
 Adversarial Defense
\end_layout

\end_inset


\end_layout

\begin_layout Frame
How to prevent the attacks?
\end_layout

\begin_layout Frame
i.e.,
 How to make the deep learning model resistant to those 
\begin_inset Quotes eld
\end_inset

imperceptible perturbations
\begin_inset Quotes erd
\end_inset

?
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Previous Attempts for Adversarial Defense
\end_layout

\end_inset


\end_layout

\begin_layout Frame
Adversarial Training:
 Augmenting training data with adversarial examples.
\end_layout

\begin_layout Frame
Input Transformation/Preprocessing:
 Modifying inputs before feeding them to the model (e.g.,
 JPEG compression,
 spatial smoothing,
 randomization).
\end_layout

\begin_layout Frame
Model Modification:
 Changing the network architecture or training procedure (e.g.,
 defensive distillation - though largely broken).
\end_layout

\begin_layout Frame
Detection:
 Training separate detectors to identify adversarial inputs.
\end_layout

\begin_layout Frame
Certified defenses.
\end_layout

\begin_layout Frame
Goal:
 Provide mathematical guarantees of robustness within a specific perturbation bound,
 such as random smoothing.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Adversarial Training (Madry Defense)
\end_layout

\end_inset


\end_layout

\begin_layout Frame
How it works:
 Generate adversarial examples (often using PGD) on-the-fly during training and include them in mini-batches.
\end_layout

\begin_layout Frame
Pros:
 Currently the most effective empirical defense strategy.
\end_layout

\begin_layout Frame
Cons:
 Computationally expensive,
 can slightly hurt accuracy on clean data ("robustness vs.
 accuracy trade-off").
\end_layout

\begin_layout Frame

\end_layout

\begin_layout Section
Adversarial Attack & Defense in Other Tasks
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Section 4:
 Attack & Defense in Other Tasks
\end_layout

\end_inset


\end_layout

\begin_layout Frame
For instance,
 in deep metric learning.
\end_layout

\begin_layout Section*
Summary
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Summary
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Deep learning models are pretty fragile.
\end_layout

\begin_layout Standard
Deep learning models are surprisingly brittle.
\end_layout

\begin_layout Standard
Adversarial attacks exploit model internals (gradients) or transferability.
\end_layout

\begin_layout Standard
Adversarial training is the leading defense,
 but challenges remain.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Comment:

\series default
 Good to know about those attacks and defense methods.
\end_layout

\begin_layout Standard
LLM safety (e.g.,
 jailbreak) is a more heated discussion nowadays.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Thanks!
\end_layout

\end_inset


\end_layout

\begin_layout Frame
Q/A Session.
\end_layout

\begin_layout Frame
The references can be found at the subtitles in the slides.
\end_layout

\end_body
\end_document
