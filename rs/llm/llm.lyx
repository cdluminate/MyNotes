#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
%\usetheme{Warsaw}
\usetheme{Boadilla}
% or ...

%\usecolortheme{orchis}
\setbeamertemplate{footline}[frame number]{}
\usefonttheme[onlymath]{serif}

%\setbeamercovered{transparent}
% or whatever (possibly just delete it)
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "biolinum" "default"
\font_typewriter "default" "default"
\font_math "libertine-ntxm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Brief Review on Large Language Models
\end_layout

\begin_layout Subtitle
Simple algorithms that scale well are the core of deep learning.
 — Kaiming He
\end_layout

\begin_layout Author
Mo Zhou
\begin_inset Newline newline
\end_inset

Johns Hopkins University
\end_layout

\begin_layout Date
Sept.
 6 2023
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Table of Contents
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Natural Langauge Processing
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
NLP: Lexical Tokenization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
The task of splitting a text into meaningful segments, called tokens.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted3.png
	special width=0.6\linewidth

\end_inset


\end_layout

\begin_deeper
\begin_layout AlertBlock
There is no standard way of tokenization.
 Modern models still use different tokenizers – some are simple and native,
 while some use intricated ones.
 
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] SpaCy Linguistic Features: https://spacy.io/usage/linguistic-features
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Natural Language Processing
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
A couple of foundamental aspects.
\end_layout

\begin_layout Itemize
Part-of-speech-tagging (Language Parsing)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted1.png
	special width=1.0\linewidth

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
[1] SpaCy Linguistic Features: https://spacy.io/usage/linguistic-features
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Natural Language Processing
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Dependency Tree Parsing (Language Parsing)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted2.png
	special width=1.0\linewidth

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] SpaCy Linguistic Features: https://spacy.io/usage/linguistic-features
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Natural Language Processing
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Understanding tasks requires a stronger language representation.
\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
GLUE/SuperGLUE Benchmarks
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
https://gluebenchmark.com/tasks
\end_layout

\begin_layout Itemize
https://super.gluebenchmark.com/tasks
\end_layout

\end_deeper
\begin_layout ExampleBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Example: Semantic Textual Similarity Benchmark (STS-B)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
(1) The bird is bathing in the sink.
\end_layout

\begin_layout Itemize
(2) Birdie is washing itself in the water basin.
\end_layout

\begin_layout Standard
Measure 
\series bold
semantic similarity
\series default
 between two sentences.
\end_layout

\begin_layout Standard
Evaluation is 
\series bold
Pearson correlation
\series default
 w.r.t.
 human.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Word Vectors
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Word Vectors: Words Represented by Context
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
How to represent the meaning of a single word?
\end_layout

\begin_layout Itemize
Before the introduction of word2vec, people use 
\series bold
WordNet
\series default
.
\begin_inset Newline newline
\end_inset

(ImageNet/ILSVRC class identifiers use WordNet.)
\end_layout

\begin_layout Itemize
WordNet representation (one-hot vector) does not encode similarity.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

A word's meaning is given by the words that frequently appear close-by.
\begin_inset Quotes erd
\end_inset

 (statistical NLP)
\end_layout

\begin_layout Itemize
We use the many contexts of 
\begin_inset Formula $w$
\end_inset

 to build up the representation of 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Itemize
Used as input tokens for RNN/LSTM/GRU for a long while.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset

 [1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Word2Vec
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Use similarity between word 
\begin_inset Formula $c$
\end_inset

 and context 
\begin_inset Formula $o$
\end_inset

 to calculate 
\begin_inset Formula $P(o|c)$
\end_inset

, then adjust word vectors to maximize it.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted4.png
	special width=0.6\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Likelyhood -> NLL objective function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $L(\theta)=\prod_{t=1}^{T}\prod_{j\in\text{"window"}}P(w_{t+j}|w_{t};\theta)\quad\Rightarrow\quad J(\theta)=-\frac{1}{T}\log L(\theta)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Frame

\end_layout

\begin_layout Frame
We use two vectors per word 
\begin_inset Formula $w$
\end_inset

 — 
\begin_inset Formula $v_{w}$
\end_inset

 for center word; 
\begin_inset Formula $u_{w}$
\end_inset

 for context word.
 For a center word 
\begin_inset Formula $c$
\end_inset

 and context word 
\begin_inset Formula $o$
\end_inset

,
\begin_inset Formula 
\[
P(o|c)=\frac{\exp(u_{o}^{T}v_{c})}{\sum_{w\in V}\exp(u_{w}^{T}v_{c})}
\]

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset

 [1] Efficient Estimation of Word Representations in Vector Space, 1301.3781
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Word2vec: extensions and further work
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\begin_inset Quotes eld
\end_inset

Learning from context
\begin_inset Quotes erd
\end_inset

 -> computer vision?
\end_layout

\begin_layout Itemize
The previously shown method is the skip-gram (SG) variant – predicting context
 words given center word.
\end_layout

\begin_layout Itemize
We can also use the continuous bag of words (CBoW) variant – predicting
 center word given context words.
 
\end_layout

\begin_layout Itemize
Negative sampling (a true pair + several 
\begin_inset Quotes eld
\end_inset

noise
\begin_inset Quotes erd
\end_inset

 pairs) can be used for less computational overhead in normalization term.
\end_layout

\begin_layout Itemize
SoTA for a long period of time
\begin_inset Newline newline
\end_inset


\bar under
GloVe: Global Vectors for Word Representation, EMNLP2014
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset

 [1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Language Modeling
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Language Modeling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Language Modeling
\series default
 is the task of predicting what word comes next.
 i.e., 
\begin_inset Formula 
\[
P(x_{t+1}|x_{t},\ldots,x_{2},x_{1})
\]

\end_inset


\end_layout

\begin_layout Itemize
Can also assign a probability to a piece of text
\begin_inset Formula 
\[
P(x_{1},\ldots,x_{T})=\prod_{t=1}^{T}P(x_{t}|x_{t-1},\ldots,x_{1})
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] Alex Graves, Supervised Sequence Labelling with Recurrent Neural Networks
 (Book)
\end_layout

\begin_layout Frame
[2] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
N-Gram Language Models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Pretty old.
 But good to know.
 The intuitions behind them do not change for decades.
\end_layout

\begin_layout ExampleBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
n-gram examples
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
unigrams: “the”, “students”, “opened”, ”their”
\end_layout

\begin_layout Itemize
bigrams: “the students”, “students opened”, “opened their”
\end_layout

\begin_layout Itemize
trigrams: “the students opened”, “students opened their”
\end_layout

\begin_layout Itemize
four-grams: “the students opened their”
\end_layout

\end_deeper
\end_deeper
\begin_layout Frame
The we can use Markov model, using the assumption that 
\begin_inset Formula $x_{t+1}$
\end_inset

 only depends on the preceding 
\begin_inset Formula $n-1$
\end_inset

 words.
\begin_inset Formula 
\[
P(x_{t+1}|x_{t},\ldots,x_{1})=P(x_{t+1}|x_{t},\ldots,x_{t-n+2})=\frac{P(x_{t+1},x_{t},\ldots,x_{t-n+2})}{P(x_{t},\ldots,x_{t-n+2})}
\]

\end_inset


\end_layout

\begin_layout Frame
Such model can be statically approximated by counting on an corpus.
\end_layout

\begin_layout Frame
Then you can generate language from it in the auto-regressive manner.
\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset

 [1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Recurrent Neural Network
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Language Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
They have became the past since the introduction of transformers – 
\series bold
but they incubated attention
\series default
.
\end_layout

\begin_layout Itemize
Markov model
\begin_inset Newline newline
\end_inset

A larger window will greatly increase model size, and introduce sparsity
 issue.
\end_layout

\begin_layout Itemize
Fixed-window neural language model
\begin_inset Newline newline
\end_inset

MLP for predicting 
\begin_inset Formula $x_{t+1}$
\end_inset

 based on the previous window.
\begin_inset Newline newline
\end_inset

No longer need to store all n-grams.
\begin_inset Newline newline
\end_inset

But fixed window is still too small.
\end_layout

\begin_layout Itemize
We need a neural architecture that can process variable length input.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] Y.
 Bengio, A Neural Probabilistic Language Model (2000/2003)
\end_layout

\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Recurrent Neural Network
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Core idea: apply the same weights 
\begin_inset Formula $W$
\end_inset

 repeatedly across the whole sequence.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Graphics
	filename pasted5.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] A.
 Graves, Generating Sequences With Recurrent Neural Networks
\end_layout

\begin_layout Frame
[2] Alex Graves, Supervised Sequence Labelling with Recurrent Neural Networks
 (Book)
\end_layout

\begin_layout Frame
[3] Improved Semantic Representations From Tree-Structured Long Short-Term
 Memory Networks, ACL2015
\end_layout

\begin_layout Frame
[4] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Simple RNN Language Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Can process any-length input.
 Trained using cross-entropy loss.
\begin_inset Newline newline
\end_inset

But slow since input tokens are not processed in parallel, and difficult
 to access information for long range.
 Also suffers from vanishing/exploding gradient issue.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted6.png
	special width=0.6\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Frame
[2] On the difficulty of training recurrent neural networks, 1211.5063
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Long-Short Term Memory (LSTM)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Forget gate, input gate, and output gate.
 Cell state and hidden state.
 Dominant between 2013-2015.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename pasted7.png
	special width=0.8\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
small
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Alleviated long-term dependency issue.
 But what about even longer distance dependency?
\end_layout

\begin_layout Itemize
LSTM is much slower than RNN.
 The input sequence is not processed in parallel.
\end_layout

\begin_layout Itemize
People removed several gates from the model and then it becomes Gated Recurrent
 Unit (GRU).
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Section
Large Language Models
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Large Language Models
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
GPT-3: Path to Foundation Models
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Make Titles Informative.
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
On first slide.
\end_layout

\begin_layout Corollary
On second slide.
\end_layout

\begin_layout Corollary

\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Make Titles Informative.
 
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout ColumnsTopAligned
\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Column
5cm
\end_layout

\begin_layout Theorem
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout

1-
\end_layout

\end_inset

In left column.
\end_layout

\begin_layout Column
5cm
\end_layout

\begin_layout Corollary
\begin_inset Argument 1
status collapsed

\begin_layout Plain Layout

2-
\end_layout

\end_inset

In right column.
\begin_inset Newline newline
\end_inset

New line
\end_layout

\end_deeper
\end_deeper
\end_body
\end_document
