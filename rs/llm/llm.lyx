#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass beamer
\begin_preamble
%\usetheme{Warsaw}
\usetheme{Boadilla}
% or ...

%\usecolortheme{orchis}
\setbeamertemplate{footline}[frame number]{}
\usefonttheme[onlymath]{serif}

%\setbeamercovered{transparent}
% or whatever (possibly just delete it)
\end_preamble
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "biolinum" "default"
\font_typewriter "default" "default"
\font_math "libertine-ntxm" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 2
\tocdepth 2
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Brief Review on Large Language Models
\end_layout

\begin_layout Subtitle
Simple algorithms that scale well are the core of deep learning.
 — Kaiming He
\end_layout

\begin_layout Author
Mo Zhou
\begin_inset Newline newline
\end_inset

Johns Hopkins University
\end_layout

\begin_layout Date
Sept.
 6 2023
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Table of Contents
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Section
Natural Langauge Processing
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
NLP: Lexical Tokenization
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
The task of splitting a text into meaningful segments, called tokens.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted3.png
	special width=0.6\linewidth

\end_inset


\end_layout

\begin_deeper
\begin_layout AlertBlock
There is no standard way of tokenization.
 Modern models still use different tokenizers – some are simple and native,
 while some use intricated ones.
 
\begin_inset Argument 2
status open

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] SpaCy Linguistic Features: https://spacy.io/usage/linguistic-features
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Natural Language Processing
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
A couple of foundamental aspects.
\end_layout

\begin_layout Itemize
Part-of-speech-tagging (Language Parsing)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted1.png
	special width=1.0\linewidth

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
[1] SpaCy Linguistic Features: https://spacy.io/usage/linguistic-features
\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Natural Language Processing
\end_layout

\end_inset


\begin_inset Separator latexpar
\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Dependency Tree Parsing (Language Parsing)
\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename pasted2.png
	special width=1.0\linewidth

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] SpaCy Linguistic Features: https://spacy.io/usage/linguistic-features
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Natural Language Processing
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Understanding tasks requires a stronger language representation.
\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
GLUE/SuperGLUE Benchmarks
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
https://gluebenchmark.com/tasks
\end_layout

\begin_layout Itemize
https://super.gluebenchmark.com/tasks
\end_layout

\end_deeper
\begin_layout ExampleBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Example: Semantic Textual Similarity Benchmark (STS-B)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
(1) The bird is bathing in the sink.
\end_layout

\begin_layout Itemize
(2) Birdie is washing itself in the water basin.
\end_layout

\begin_layout Standard
Measure 
\series bold
semantic similarity
\series default
 between two sentences.
\end_layout

\begin_layout Standard
Evaluation is 
\series bold
Pearson correlation
\series default
 w.r.t.
 human.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Word Vectors
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Word Vectors: Words Represented by Context
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
How to represent the meaning of a single word?
\end_layout

\begin_layout Itemize
Before the introduction of word2vec, people use 
\series bold
WordNet
\series default
.
\begin_inset Newline newline
\end_inset

(ImageNet/ILSVRC class identifiers use WordNet.)
\end_layout

\begin_layout Itemize
WordNet representation (one-hot vector) does not encode similarity.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset

A word's meaning is given by the words that frequently appear close-by.
\begin_inset Quotes erd
\end_inset

 (statistical NLP)
\end_layout

\begin_layout Itemize
We use the many contexts of 
\begin_inset Formula $w$
\end_inset

 to build up the representation of 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Itemize
Used as input tokens for RNN/LSTM/GRU for a long while.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset

 [1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Word2Vec
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Use similarity between word 
\begin_inset Formula $c$
\end_inset

 and context 
\begin_inset Formula $o$
\end_inset

 to calculate 
\begin_inset Formula $P(o|c)$
\end_inset

, then adjust word vectors to maximize it.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted4.png
	special width=0.6\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Likelyhood -> NLL objective function
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula $L(\theta)=\prod_{t=1}^{T}\prod_{j\in\text{"window"}}P(w_{t+j}|w_{t};\theta)\quad\Rightarrow\quad J(\theta)=-\frac{1}{T}\log L(\theta)$
\end_inset


\end_layout

\end_deeper
\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Frame

\end_layout

\begin_layout Frame
We use two vectors per word 
\begin_inset Formula $w$
\end_inset

 — 
\begin_inset Formula $v_{w}$
\end_inset

 for center word; 
\begin_inset Formula $u_{w}$
\end_inset

 for context word.
 For a center word 
\begin_inset Formula $c$
\end_inset

 and context word 
\begin_inset Formula $o$
\end_inset

,
\begin_inset Formula 
\[
P(o|c)=\frac{\exp(u_{o}^{T}v_{c})}{\sum_{w\in V}\exp(u_{w}^{T}v_{c})}
\]

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset

 [1] Efficient Estimation of Word Representations in Vector Space, 1301.3781
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Word2vec: extensions and further work
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
\begin_inset Quotes eld
\end_inset

Learning from context
\begin_inset Quotes erd
\end_inset

 -> computer vision?
\end_layout

\begin_layout Itemize
The previously shown method is the skip-gram (SG) variant – predicting context
 words given center word.
\end_layout

\begin_layout Itemize
We can also use the continuous bag of words (CBoW) variant – predicting
 center word given context words.
 
\end_layout

\begin_layout Itemize
Negative sampling (a true pair + several 
\begin_inset Quotes eld
\end_inset

noise
\begin_inset Quotes erd
\end_inset

 pairs) can be used for less computational overhead in normalization term.
\end_layout

\begin_layout Itemize
SoTA word representations for a long period of time
\begin_inset Newline newline
\end_inset


\bar under
GloVe: Global Vectors for Word Representation, EMNLP2014
\begin_inset Newline newline
\end_inset

ELMo: Extending a Parser to Distant Domains Using a Few Dozen Partially
 Annotated Examples, ACL2018
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset

 [1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Language Modeling
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Language Modeling
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Language Modeling
\series default
 is the task of predicting what word comes next.
 i.e., 
\begin_inset Formula 
\[
P(x_{t+1}|x_{t},\ldots,x_{2},x_{1})
\]

\end_inset


\end_layout

\begin_layout Itemize
Can also assign a probability to a piece of text
\begin_inset Formula 
\[
P(x_{1},\ldots,x_{T})=\prod_{t=1}^{T}P(x_{t}|x_{t-1},\ldots,x_{1})
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] Alex Graves, Supervised Sequence Labelling with Recurrent Neural Networks
 (Book)
\end_layout

\begin_layout Frame
[2] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
N-Gram Language Models
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Pretty old.
 But good to know.
\end_layout

\begin_layout ExampleBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
n-gram examples
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
unigrams: “the”, “students”, “opened”, ”their”
\end_layout

\begin_layout Itemize
bigrams: “the students”, “students opened”, “opened their”
\end_layout

\begin_layout Itemize
trigrams: “the students opened”, “students opened their”
\end_layout

\begin_layout Itemize
four-grams: “the students opened their”
\end_layout

\end_deeper
\end_deeper
\begin_layout Frame
The we can use Markov model, using the assumption that 
\begin_inset Formula $x_{t+1}$
\end_inset

 only depends on the preceding 
\begin_inset Formula $n-1$
\end_inset

 words.
\begin_inset Formula 
\[
P(x_{t+1}|x_{t},\ldots,x_{1})=P(x_{t+1}|x_{t},\ldots,x_{t-n+2})=\frac{P(x_{t+1},x_{t},\ldots,x_{t-n+2})}{P(x_{t},\ldots,x_{t-n+2})}
\]

\end_inset


\end_layout

\begin_layout Frame
Such model can be statically approximated by counting on an corpus.
\end_layout

\begin_layout Frame
Then you can generate language from it in the auto-regressive manner.
\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset

 [1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Recurrent Neural Network
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Language Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
They have became the past since the introduction of transformers – 
\series bold
but they incubated attention
\series default
.
\end_layout

\begin_layout Itemize
Markov model
\begin_inset Newline newline
\end_inset

A larger window will greatly increase model size, and introduce sparsity
 issue.
\end_layout

\begin_layout Itemize
Fixed-window neural language model
\begin_inset Newline newline
\end_inset

MLP for predicting 
\begin_inset Formula $x_{t+1}$
\end_inset

 based on the previous window.
\begin_inset Newline newline
\end_inset

No longer need to store all n-grams.
\begin_inset Newline newline
\end_inset

But fixed window is still too small.
\end_layout

\begin_layout Itemize
We need a neural architecture that can process variable length input.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] Y.
 Bengio, A Neural Probabilistic Language Model (2000/2003)
\end_layout

\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Recurrent Neural Network
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Core idea: apply the same weights 
\begin_inset Formula $W$
\end_inset

 repeatedly across the whole sequence.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Graphics
	filename pasted5.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] A.
 Graves, Generating Sequences With Recurrent Neural Networks
\end_layout

\begin_layout Frame
[2] Alex Graves, Supervised Sequence Labelling with Recurrent Neural Networks
 (Book)
\end_layout

\begin_layout Frame
[3] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Simple RNN Language Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Can process any-length input.
 Trained using cross-entropy loss.
\begin_inset Newline newline
\end_inset

But slow since input tokens are not processed in parallel, and difficult
 to access information for long range.
 Also suffers from vanishing gradient and exploding gradient issue (grad
 clipping) issue.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted6.png
	special width=0.6\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Frame
[2] On the difficulty of training recurrent neural networks, 1211.5063
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Long-Short Term Memory (LSTM)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Forget gate, input gate, and output gate.
 Cell state and hidden state.
 Dominant between 2013-2015.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename pasted7.png
	special width=0.8\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
small
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Alleviated long-term dependency issue.
 But what about even longer distance dependency?
\end_layout

\begin_layout Itemize
LSTM is much slower than RNN.
 The input sequence is not processed in parallel.
\end_layout

\begin_layout Itemize
People removed several gates from the model and then it becomes Gated Recurrent
 Unit (GRU).
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Frame
[2] Improved Semantic Representations From Tree-Structured Long Short-Term
 Memory Networks, ACL2015
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Sequence To Sequence
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Neural Machine Translation (NMT)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
The sequence-to-sequence (seq2seq) model is an example of conditional language
 model.
 The first commercially successful NLP deep learning domain.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Graphics
	filename pasted8.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] http://web.stanford.edu/class/cs224n/
\end_layout

\begin_layout Frame
[2] Y.
 Bengio, Neural Machine Translation by Jointly Learning to Align and Translate,
 1409.0473
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Information Bottleneck & Attention
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Sequence-to-sequence with attention mechanism.
 Core Idea: on each decoder step, use direct connection to the encoder to
 focus on a particular part of the source sequence.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Graphics
	filename pasted9.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] Y.
 Bengio, Neural Machine Translation by Jointly Learning to Align and Translate,
 1409.0473
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Attention Is All You Need (NMT)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Transformer: eschewing recurrence and instead relying entirely on an attention
 mechanism.
 Solved many previously existing issues, e.g., parallelism, interaction distance.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename pasted10.png
	special width=0.4\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] Attention Is All You Need, 1706.03762
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Pre-training Paradigm
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Only Word Embeddings is Pretrained
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
The past tense (before 2017): Start with pretrained word emb, then learn
 to incorporate context in LSTM/Transformer on 
\series bold
specific task
\series default
.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename pasted11.png
	special width=0.4\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_layout ExampleBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Example
\end_layout

\end_inset


\end_layout

\begin_layout ExampleBlock
Consider I 
\series bold
record
\series default
 the 
\series bold
record
\series default
: the two instances of record mean different things.
\end_layout

\begin_layout ExampleBlock
-> See ELMo
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Pretraining in Modern NLP
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle

\series bold
All parameters are initialized via pretraining
\series default
 — hiding parts of the input from the model, and train the model to reconstruct
 those parts.
 Does not require datasets for specific task.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename pasted12.png
	special width=0.35\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
This practice leads to exceptionally strong:
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
representations of language
\end_layout

\begin_layout Itemize
parameter initializations
\end_layout

\begin_layout Itemize
probability distributions
\end_layout

\begin_layout Standard
Even if the pre-training task is simply next word prediction [1].
\end_layout

\end_deeper
\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] Quoc V.
 Le, Semi-supervised Sequence Learning, 1511.01432
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Feasible Pretraining Task Depends on Architecture
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
We don't know which architecture is the best.
\end_layout

\begin_layout Itemize

\series bold
Encoder-Only
\series default
 Architecture
\begin_inset Newline newline
\end_inset

Can obtain bidirectional context – but accessing future means it is infeasible
 for language modeling.
\end_layout

\begin_layout Itemize

\series bold
Encoder-Decoder
\series default
 Architecture
\begin_inset Newline newline
\end_inset

What is the best way to pretrain them?
\end_layout

\begin_layout Itemize

\series bold
Decoder-Only
\series default
 Architecture
\begin_inset Newline newline
\end_inset

Suitable for LMs.
 Nice for generation.
 Cannot condition on future words.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Subsection
Encoder-Only Pretraining
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Pretraining for Encoder-Only Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
It has got bi-directional context (can condition on the future), so cannot
 do language modeling.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename pasted13.png
	special width=0.5\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout ExampleBlock
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Reconstructing masked input: Masked LM
\end_layout

\end_inset


\end_layout

\begin_layout ExampleBlock
Stanford University is located in __________, California.
\end_layout

\begin_layout ExampleBlock
I put ___ fork down on the table.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] Bidirectional Encoder Representations from Transformers, 2018
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Masked LM — BERT
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
Bidirectional Encoder Representations from Transformers, 2018
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vspace{-0.3cm}
\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename pasted14.png
	special width=0.9\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\backslash
vspace{-0.9cm}
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout ColumnsTopAligned
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Column
5cm
\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Task #1: Masked LM
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Randomly replace 15% of all WordPiece tokens.
\end_layout

\begin_deeper
\begin_layout Enumerate
80%: w/ [MASK] token
\end_layout

\begin_layout Enumerate
10%: w/ random token
\end_layout

\begin_layout Enumerate
with itself 10% of the time
\end_layout

\end_deeper
\begin_layout Itemize
Then predict the masked word.
\end_layout

\end_deeper
\begin_layout Column
5cm
\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Task #2: Next Sentence Pred
\end_layout

\end_inset


\end_layout

\begin_layout Block
Bert pretraining input is a pair of sentence
\end_layout

\begin_deeper
\begin_layout Itemize
Sentence B is the next sentence of A 50% of the time
\end_layout

\begin_layout Itemize
Sentence B is not next sentence of A 50% of the time
\end_layout

\end_deeper
\begin_layout Block
Beneficial for some downstream tasks like QA and NLI.
\end_layout

\begin_layout Block
Demonstrated not necessary by later works.
\end_layout

\end_deeper
\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
More on BERT
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Model Size
\end_layout

\end_inset


\end_layout

\begin_layout Block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
small
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
BERT-base: 12 layers, 768-dim hidden states, 12 attention heads, 110 million
 params.
\end_layout

\begin_layout Itemize
BERT-large: 24 layers, 1024-dim hidden states, 16 attention heads, 340 million
 params.
\end_layout

\begin_layout Itemize
PyTorch-ResNet50-ILSVRC: 25.6M params
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Issues
\end_layout

\end_inset


\end_layout

\begin_layout Block
Does not naturally lead to autoregressive methods.
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Extensions
\end_layout

\end_inset


\end_layout

\begin_layout Block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
small
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
RoBERTa: mainly just train BERT for longer and remove next sentence prediction.
\begin_inset Newline newline
\end_inset


\bar under
More compute, more data can improve pretraining even when not changing the
 underlying Transformer encoder.
\end_layout

\begin_layout Itemize
SpanBERT: masking contiguous spans of words makes pretraining task harder.
\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] RoBERTa: A Robustly Optimized BERT Pretraining Approach, 1907.11692
\end_layout

\begin_layout Frame
[2] SpanBERT: Improving Pre-training by Representing and Predicting Spans,
 1907.10529
\end_layout

\begin_layout Subsection
Encoder-Decoder Pretraining
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Pretraining for Encoder-Decoder Model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
We can do something like language modeling, but a prefix of every input
 is provided to encoder and is not predicted.
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{center}
\end_layout

\end_inset


\begin_inset Graphics
	filename pasted15.png
	special width=0.36\linewidth

\end_inset


\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
end{center}
\end_layout

\end_inset


\begin_inset Formula 
\begin{align*}
h_{1,}\ldots,h_{T} & ={\color{blue}\text{Encoder}}(w_{1},\ldots,w_{T})\\
h_{T+1},\ldots,h_{2T} & =\text{{\color{red}Decoder}}(w_{T+1},\ldots,w_{2T};h_{1},\ldots,h_{T})
\end{align*}

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The encoder portion benefits from bidirectional context.
\end_layout

\begin_layout Itemize
The decoder portion trains the whole model through language modeling.
 (triangular attn mask)
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
vfill
\backslash
scriptsize
\end_layout

\end_inset


\end_layout

\begin_layout Frame
[1] T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text
 Transformer, 1910.10683
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Text-to-Text w/ Span-Corruption Pretraining
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text
 Transformer, 1910.10683
\end_layout

\end_deeper
\begin_layout Frame
\begin_inset Graphics
	filename pasted16.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Frame
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
hrulefill
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\size small
Every task considered is cast as feeding (task-specific) text prefix and
 input, and then generating some target text (teacher-forcing objective).
\end_layout

\begin_layout Frame
\begin_inset Graphics
	filename pasted17.png
	special width=\linewidth

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
More on T5
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout FrameSubtitle
T5 Can be finetuned to answer a wide range of questions, retrieving knowledge
 from its parameters.
\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Model Size
\end_layout

\end_inset


\end_layout

\begin_layout Block
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\backslash
small
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="2" columns="6">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Size
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Small
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Base
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Large
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
11B
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
#Params
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
60M
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
220M
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
770M
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
3B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
11B
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Block
\begin_inset Argument 2
status open

\begin_layout Plain Layout
Flan-T5: Scaling Instruction-Finetuned Language Models, 2210.11416
\end_layout

\end_inset


\end_layout

\begin_layout Block

\size footnotesize
Instruction/chain-of-thought finetuning, while scaling number of tasks and
 model size.
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\end_layout

\end_deeper
\begin_layout Block
\begin_inset Graphics
	filename pasted18.png
	special width=0.7\linewidth

\end_inset


\end_layout

\end_deeper
\begin_layout Section
Large Language Models
\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Large Language Models
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
LLM :: Pre-Training
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
LLM :: Scaling Law
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
GPT-3: Path to Foundation Models
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
LLM :: Fine-Tuning
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
LLM :: Evaluation Metrics
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
LLM :: Parameter Efficient Fine-Tuning
\end_layout

\end_inset


\end_layout

\begin_layout Frame
Prefix Tuning
\end_layout

\begin_layout Frame
Prompt Tuning
\end_layout

\begin_layout Frame
LoRA: Low-Rank Adaptation (easier than prefix tuning)
\end_layout

\begin_layout Frame
Learns a low-rank 
\begin_inset Quotes eld
\end_inset

diff
\begin_inset Quotes erd
\end_inset

 between the pretrained and fine-tuned weight matrices.
\end_layout

\begin_layout Frame
[1] http://web.stanford.edu/class/cs224n/slides/cs224n-2023-lecture9-pretraining.pd
f
\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Make Titles Informative.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Theorem
On first slide.
\end_layout

\begin_layout Corollary
On second slide.
\end_layout

\begin_layout Corollary

\end_layout

\end_deeper
\begin_layout Standard
\begin_inset Separator parbreak
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Make Titles Informative.
 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout ColumnsTopAligned

\end_layout

\begin_deeper
\begin_layout Column
5cm
\end_layout

\begin_layout Theorem
In left column.
\end_layout

\begin_layout Column
5cm
\end_layout

\begin_layout Corollary
In right column.
\begin_inset Newline newline
\end_inset

New line
\end_layout

\end_deeper
\end_deeper
\end_body
\end_document
