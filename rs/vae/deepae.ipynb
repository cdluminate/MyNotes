{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "205a2ca3-65a0-4e3f-ad8b-7f62639137fc",
   "metadata": {},
   "source": [
    "# https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/08-deep-autoencoders.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f373c667-7d53-40f7-97f0-9fb6faa3afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from lightning.pytorch.callbacks import Callback, LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "\n",
    "# Tensorboard extension (for visualization purposes later)\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = os.environ.get(\"PATH_DATASETS\", \"data\")\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/tutorial9\")\n",
    "\n",
    "# Setting the seed\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7236c5-f3bb-4c43-9405-c4d41b8c053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial9/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"cifar10_64.ckpt\", \"cifar10_128.ckpt\", \"cifar10_256.ckpt\", \"cifar10_384.ckpt\"]\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(\"Downloading %s...\" % file_url)\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\n",
    "                \"Something went wrong. Please try to download the files manually,\"\n",
    "                \" or contact the author with the full output including the following error:\\n\",\n",
    "                e,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb474ea-3cc9-4131-bf46-b79fb280be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations applied on each image => only make them a tensor\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "L.seed_everything(42)\n",
    "train_set, val_set = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "train_loader = data.DataLoader(train_set, batch_size=256, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = data.DataLoader(val_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = data.DataLoader(test_set, batch_size=256, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "\n",
    "def get_train_images(num):\n",
    "    return torch.stack([train_dataset[i][0] for i in range(num)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538a4b4-24d7-4996-b2a9-0f6938e92799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"Encoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of input channels of the image. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the first convolutional layers. Deeper layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the encoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),  # 32x32 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 16x16 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),  # 8x8 => 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(),  # Image grid to single feature vector\n",
    "            nn.Linear(2 * 16 * c_hid, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338a56a-3066-47f5-ac85-57af33944ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_input_channels: int, base_channel_size: int, latent_dim: int, act_fn: object = nn.GELU):\n",
    "        \"\"\"Decoder.\n",
    "\n",
    "        Args:\n",
    "           num_input_channels : Number of channels of the image to reconstruct. For CIFAR, this parameter is 3\n",
    "           base_channel_size : Number of channels we use in the last convolutional layers. Early layers might use a duplicate of it.\n",
    "           latent_dim : Dimensionality of latent representation z\n",
    "           act_fn : Activation function used throughout the decoder network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(nn.Linear(latent_dim, 2 * 16 * c_hid), act_fn())\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 4x4 => 8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),  # 8x8 => 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(\n",
    "                c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2\n",
    "            ),  # 16x16 => 32x32\n",
    "            nn.Tanh(),  # The input images is scaled between -1 and 1, hence the output has to be bounded as well\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b5842-0fd7-4ea2-85a7-a891e1111243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channel_size: int,\n",
    "        latent_dim: int,\n",
    "        encoder_class: object = Encoder,\n",
    "        decoder_class: object = Decoder,\n",
    "        num_input_channels: int = 3,\n",
    "        width: int = 32,\n",
    "        height: int = 32,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters of autoencoder\n",
    "        self.save_hyperparameters()\n",
    "        # Creating encoder and decoder\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        # Example input array needed for visualizing the graph of the network\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"The forward function takes in an image and returns the reconstructed image.\"\"\"\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "\n",
    "    def _get_reconstruction_loss(self, batch):\n",
    "        \"\"\"Given a batch of images, this function returns the reconstruction loss (MSE in our case).\"\"\"\n",
    "        x, _ = batch  # We do not need the labels\n",
    "        x_hat = self.forward(x)\n",
    "        loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1, 2, 3]).mean(dim=[0])\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        # Using a scheduler is optional but can be helpful.\n",
    "        # The scheduler reduces the LR if the validation performance hasn't improved for the last N epochs\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.2, patience=20, min_lr=5e-5)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log(\"test_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3f255-a546-4b98-9eeb-fee5dca75932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_imgs(img1, img2, title_prefix=\"\"):\n",
    "    # Calculate MSE loss between both images\n",
    "    loss = F.mse_loss(img1, img2, reduction=\"sum\")\n",
    "    # Plot images for visual comparison\n",
    "    grid = torchvision.utils.make_grid(torch.stack([img1, img2], dim=0), nrow=2, normalize=True, range=(-1, 1))\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    plt.title(f\"{title_prefix} Loss: {loss.item():4.2f}\")\n",
    "    plt.imshow(grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    # Load example image\n",
    "    img, _ = train_dataset[i]\n",
    "    img_mean = img.mean(dim=[1, 2], keepdims=True)\n",
    "\n",
    "    # Shift image by one pixel\n",
    "    SHIFT = 1\n",
    "    img_shifted = torch.roll(img, shifts=SHIFT, dims=1)\n",
    "    img_shifted = torch.roll(img_shifted, shifts=SHIFT, dims=2)\n",
    "    img_shifted[:, :1, :] = img_mean\n",
    "    img_shifted[:, :, :1] = img_mean\n",
    "    compare_imgs(img, img_shifted, \"Shifted -\")\n",
    "\n",
    "    # Set half of the image to zero\n",
    "    img_masked = img.clone()\n",
    "    img_masked[:, : img_masked.shape[1] // 2, :] = img_mean\n",
    "    compare_imgs(img, img_masked, \"Masked -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739eebb6-d765-455c-b975-5590a693c042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(Callback):\n",
    "    def __init__(self, input_imgs, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs  # Images to reconstruct during training\n",
    "        # Only save those images every N epochs (otherwise tensorboard gets quite large)\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            # Reconstruct images\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "            # Plot and add to tensorboard\n",
    "            imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1, 1))\n",
    "            trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40edca7-cf19-4075-9f96-6cccaab6e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cifar(latent_dim):\n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, \"cifar10_%i\" % latent_dim),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=500,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True),\n",
    "            GenerateCallback(get_train_images(8), every_n_epochs=10),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "    )\n",
    "    trainer.logger._log_graph = True  # If True, we plot the computation graph in tensorboard\n",
    "    trainer.logger._default_hp_metric = None  # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"cifar10_%i.ckpt\" % latent_dim)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = Autoencoder(base_channel_size=32, latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    # Test best model on validation and test set\n",
    "    val_result = trainer.test(model, dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\": test_result, \"val\": val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff103a3-0d5d-4b69-8ffe-56e9dc9841de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "for latent_dim in [64, 128, 256, 384]:\n",
    "    model_ld, result_ld = train_cifar(latent_dim)\n",
    "    model_dict[latent_dim] = {\"model\": model_ld, \"result\": result_ld}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f561feb9-5797-48ac-9d96-3010759223a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = sorted(k for k in model_dict)\n",
    "val_scores = [model_dict[k][\"result\"][\"val\"][0][\"test_loss\"] for k in latent_dims]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(\n",
    "    latent_dims, val_scores, \"--\", color=\"#000\", marker=\"*\", markeredgecolor=\"#000\", markerfacecolor=\"y\", markersize=16\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(latent_dims, labels=latent_dims)\n",
    "plt.title(\"Reconstruction error over latent dimensionality\", fontsize=14)\n",
    "plt.xlabel(\"Latent dimensionality\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.minorticks_off()\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3b0492-3dc1-462d-9877-f73a37467cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(model, input_imgs):\n",
    "    # Reconstruct images\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        reconst_imgs = model(input_imgs.to(model.device))\n",
    "    reconst_imgs = reconst_imgs.cpu()\n",
    "\n",
    "    # Plotting\n",
    "    imgs = torch.stack([input_imgs, reconst_imgs], dim=1).flatten(0, 1)\n",
    "    grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, range=(-1, 1))\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(7, 4.5))\n",
    "    plt.title(\"Reconstructed from %i latents\" % (model.hparams.latent_dim))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2171acdd-ebdc-46d9-85e4-05a858f9a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_imgs = get_train_images(4)\n",
    "for latent_dim in model_dict:\n",
    "    visualize_reconstructions(model_dict[latent_dim][\"model\"], input_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263bd9c5-185f-42e0-9d75-4a8df29b863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_imgs = torch.rand(2, 3, 32, 32) * 2 - 1\n",
    "visualize_reconstructions(model_dict[256][\"model\"], rand_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063406cf-808f-400a-b8e1-34cdec7b484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_imgs = torch.zeros(4, 3, 32, 32)\n",
    "\n",
    "# Single color channel\n",
    "plain_imgs[1, 0] = 1\n",
    "# Checkboard pattern\n",
    "plain_imgs[2, :, :16, :16] = 1\n",
    "plain_imgs[2, :, 16:, 16:] = -1\n",
    "# Color progression\n",
    "xx, yy = torch.meshgrid(torch.linspace(-1, 1, 32), torch.linspace(-1, 1, 32))\n",
    "plain_imgs[3, 0, :, :] = xx\n",
    "plain_imgs[3, 1, :, :] = yy\n",
    "\n",
    "visualize_reconstructions(model_dict[256][\"model\"], plain_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5800080b-503a-477c-9faf-87c8e86232b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_dict[256][\"model\"]\n",
    "latent_vectors = torch.randn(8, model.hparams.latent_dim, device=model.device)\n",
    "with torch.no_grad():\n",
    "    imgs = model.decoder(latent_vectors)\n",
    "    imgs = imgs.cpu()\n",
    "\n",
    "grid = torchvision.utils.make_grid(imgs, nrow=4, normalize=True, range=(-1, 1), pad_value=0.5)\n",
    "grid = grid.permute(1, 2, 0)\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.imshow(grid)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc40e5e-f36c-4cdd-845e-38f4c117b1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the following model throughout this section.\n",
    "# If you want to try a different latent dimensionality, change it here!\n",
    "model = model_dict[128][\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f561b3e-a256-435e-b968-670ffa4d9cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_imgs(model, data_loader):\n",
    "    # Encode all images in the data_laoder using model, and return both images and encodings\n",
    "    img_list, embed_list = [], []\n",
    "    model.eval()\n",
    "    for imgs, _ in tqdm(data_loader, desc=\"Encoding images\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            z = model.encoder(imgs.to(model.device))\n",
    "        img_list.append(imgs)\n",
    "        embed_list.append(z)\n",
    "    return (torch.cat(img_list, dim=0), torch.cat(embed_list, dim=0))\n",
    "\n",
    "\n",
    "train_img_embeds = embed_imgs(model, train_loader)\n",
    "test_img_embeds = embed_imgs(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf6652-5538-4151-a436-e74b2dc4ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_images(query_img, query_z, key_embeds, K=8):\n",
    "    # Find closest K images. We use the euclidean distance here but other like cosine distance can also be used.\n",
    "    dist = torch.cdist(query_z[None, :], key_embeds[1], p=2)\n",
    "    dist = dist.squeeze(dim=0)\n",
    "    dist, indices = torch.sort(dist)\n",
    "    # Plot K closest images\n",
    "    imgs_to_display = torch.cat([query_img[None], key_embeds[0][indices[:K]]], dim=0)\n",
    "    grid = torchvision.utils.make_grid(imgs_to_display, nrow=K + 1, normalize=True, range=(-1, 1))\n",
    "    grid = grid.permute(1, 2, 0)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.imshow(grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca8154-78bf-41b8-8460-59265a88fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the closest images for the first N test images as example\n",
    "for i in range(8):\n",
    "    find_similar_images(test_img_embeds[0][i], test_img_embeds[1][i], key_embeds=train_img_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfdf09b-26b6-40f4-b305-d3a095a5f592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the following model throughout this section.\n",
    "# If you want to try a different latent dimensionality, change it here!\n",
    "model = model_dict[128][\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd18424-a63f-4725-a450-b7c6d70dac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary writer\n",
    "writer = SummaryWriter(\"tensorboard/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d540579-7429-4c00-94bc-24333c7fce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the embedding projector in tensorboard is computationally heavy.\n",
    "# Reduce the image amount below if your computer struggles with visualizing all 10k points\n",
    "NUM_IMGS = len(test_set)\n",
    "\n",
    "writer.add_embedding(\n",
    "    test_img_embeds[1][:NUM_IMGS],  # Encodings per image\n",
    "    metadata=[test_set[i][1] for i in range(NUM_IMGS)],  # Adding the labels per image to the plot\n",
    "    label_img=(test_img_embeds[0][:NUM_IMGS] + 1) / 2.0,\n",
    ")  # Adding the original images to the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9404e225-b1d3-4f7d-ad4d-54ea860e66d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next line to start the tensorboard\n",
    "%tensorboard --logdir tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59946cf1-3030-40a3-a35f-efc82804a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing the summary writer\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
