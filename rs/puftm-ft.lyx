#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{microtype}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Prevent Fine-Tuning by The User
\end_layout

\begin_layout Standard
Given a linear layer 
\begin_inset Formula $W\cdot x=y$
\end_inset

 where 
\begin_inset Formula $W$
\end_inset

 is network parameter matrix, 
\begin_inset Formula $x$
\end_inset

 is input vector, and 
\begin_inset Formula $y$
\end_inset

 is the pre-activation output vector.
 Let the loss function be 
\begin_inset Formula $L$
\end_inset

, and 
\begin_inset Formula $\delta\triangleq\partial L/\partial y$
\end_inset

, then we have the following backward pass:
\begin_inset Formula 
\[
\frac{\partial L}{\partial W}=\delta\cdot x^{T}\qquad\frac{\partial L}{\partial x}=W^{T}\cdot\delta
\]

\end_inset


\end_layout

\begin_layout Standard
Now, we randomly pick a vector 
\begin_inset Formula $z\neq0$
\end_inset

 from the null space of matrix 
\begin_inset Formula $W$
\end_inset

, so that 
\begin_inset Formula $W\cdot z=0$
\end_inset

.
 By adding 
\begin_inset Formula $z$
\end_inset

 on top of 
\begin_inset Formula $x$
\end_inset

, the output will not be changed:
\begin_inset Formula 
\[
W\cdot(x+z)=W\cdot x+W\cdot z=y+0
\]

\end_inset

However, the backward pass will be different from the original version:
\begin_inset Formula 
\[
\frac{\partial L}{\partial W}=\delta\cdot x^{T}+\delta\cdot z^{T}\qquad\frac{\partial L}{\partial x}=W^{T}\cdot\delta
\]

\end_inset

Note, 
\begin_inset Formula $\delta\cdot z^{T}\neq0$
\end_inset

, thus the gradient will difer from the expected one.
 Maybe this will greatly impact the beginning of the fine-tuning process.
\end_layout

\begin_layout Standard
One remaininig issue of the vector 
\begin_inset Formula $z$
\end_inset

 is that it changes the model architecture (computation graph).
 But in fact it can be merged into the model weights to avoid any architectures
 change.
 To do so, we can simply add vector 
\begin_inset Formula $z$
\end_inset

 to the bias term of the previous linear layer.
 So that 
\begin_inset Formula $\text{ReLU}(y_{-1}+b+b_{z})=x+z$
\end_inset

, where 
\begin_inset Formula $b_{z}$
\end_inset

 is our bias edits, 
\begin_inset Formula $b$
\end_inset

 is the original previous layer bias.
\end_layout

\begin_layout Standard
This manipulation relies on the assumption that the layer with parameter
 
\begin_inset Formula $W$
\end_inset

 is not full rank.
 As long as this layer is reducing dimensionality, it must not be full rank.
\end_layout

\begin_layout Standard
In summary, to prevent fine-tuning process, we can edit a bias term 
\begin_inset Formula $b$
\end_inset

 into 
\begin_inset Formula $b+b_{z}$
\end_inset

, so that
\begin_inset Formula 
\begin{align*}
 & \text{ReLU}(y_{-1}+b)=x\\
 & \text{ReLU}(y_{-1}+b+b_{z})=x+z\\
 & W\cdot z=0
\end{align*}

\end_inset

Or, given 
\begin_inset Formula $x\in X$
\end_inset

, as long as we can find 
\begin_inset Formula $\forall b_{z}$
\end_inset

, so that
\begin_inset Formula 
\[
W\cdot[\text{ReLU}(y_{-1}+b+b_{z})-\text{ReLU}(y_{-1}+b)]=0
\]

\end_inset


\end_layout

\begin_layout Standard
Now we find the bias edit 
\begin_inset Formula $b_{z}$
\end_inset

 to make the first several iterations of the fine-tuning process suffer.
 But note, as long as the model is fine-tuned for enough iteration, the
 model parameters will eventually get back to normal.
 How do we mitigate this? Since 
\begin_inset Formula $z$
\end_inset

 is in the null space of 
\begin_inset Formula $W$
\end_inset

, we can greatly increase the magnitude of 
\begin_inset Formula $z$
\end_inset

 (e.g., multiply by 1e5?).
 Then it will cause lots of numerical stability issues during the backward
 pass (gradient explosion).
\end_layout

\begin_layout Standard

\series bold
Conclusion:
\series default
 The neural network has a very strong self-recovery capability.
 Even if a part of the parameters is seriously damaged, it can quickly recover
 to a decent accuracy after several iterations of training.
 This phenomenon is rather clear with momemtum optimizers such as Adam.
 The network recovers much slower with SGD, but it will not change the fact
 that the accuracy will eventually get back to a decent level.
 Thus, unless the underlying mechanism of such self-recovory can be broken,
 this is a dead end.
\end_layout

\end_body
\end_document
