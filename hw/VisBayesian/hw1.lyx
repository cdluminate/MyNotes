#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{microtype}
\usepackage{times}
\usepackage[margin=1in]{geometry}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HW #1
\end_layout

\begin_layout Author
Mo Zhou <mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Lecture 2
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard

\bar under
What are orthogonal basis functions? How can an input image patch be expressed
 as a combination of orthogonal basis functions?
\end_layout

\begin_layout Standard
An orthogonal set of basis function is
\begin_inset Formula 
\[
\{b_{i}(x):i=1,\ldots,N\}
\]

\end_inset

where 
\begin_inset Formula $\sum_{x}\{b_{i}(x)\}^{2}=1$
\end_inset

, and 
\begin_inset Formula $\sum_{x}b_{i}(x)b_{j}(x)=0$
\end_inset

 if 
\begin_inset Formula $i\neq j$
\end_inset

.
 For example, sinusoids can be used as orthogonal basis function.
\end_layout

\begin_layout Standard
An input image patch 
\begin_inset Formula $I(x)$
\end_inset

 can be expressed by
\begin_inset Formula 
\[
I(x)=\sum_{i}a_{i}b_{i}(x)
\]

\end_inset

where the coefficient 
\begin_inset Formula $a_{i}=\sum_{x}I(x)b_{i}(x)$
\end_inset

 because the bases are orthogonal.
 Hence, the image patch 
\begin_inset Formula $I(x)$
\end_inset

 can be expressed by the coefficients 
\begin_inset Formula $\{a_{i}\}$
\end_inset

.
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard

\bar under
Give two examples of orthogonal basis functions.
\end_layout

\begin_layout Standard
(1) Sinusoids 
\begin_inset Formula $f(x)$
\end_inset


\begin_inset Formula 
\[
f(x)=a\sin(\omega x+c)
\]

\end_inset


\end_layout

\begin_layout Standard
(2) Haar Bases 
\begin_inset Formula $\psi_{n,k}$
\end_inset


\begin_inset Formula 
\[
\psi_{n,k}(t)=2^{\frac{n}{2}}\psi(2^{n}t-k),\qquad t\in R
\]

\end_inset

where the Haar wavelet's mother wavelet function 
\begin_inset Formula $\psi(t)$
\end_inset

 is defined as
\begin_inset Formula 
\[
\psi(t)=\begin{cases}
1 & 0\leq t\leq\frac{1}{2}\\
-1 & \frac{1}{2}\leq t<1\\
0 & o.w.
\end{cases}
\]

\end_inset


\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard

\bar under
Give a method for estimating a set of basis vectors given a training set
 of images.
 What form do these basis functions take if the image is shift-invariant?
\end_layout

\begin_layout Standard
Given the correlation matrix 
\begin_inset Formula $K(x,y)$
\end_inset

, we have 
\begin_inset Formula 
\[
\sum_{y}K(x,y)e_{i}(y)=\lambda_{i}e_{i}(x).
\]

\end_inset

Since Images are shift-invariant, namely 
\begin_inset Formula $K(x,y)=F(x-y)$
\end_inset

, we have
\begin_inset Formula 
\[
\sum_{y}F(x-y)e_{i}(y)=\lambda_{i}e_{i}(x).
\]

\end_inset


\end_layout

\begin_layout Subsection
Q4
\end_layout

\begin_layout Standard

\bar under
How can we represent images in terms of a linear combination of over-complete
 basis functions by imposing a sparsity constraint?
\end_layout

\begin_layout Standard
We impose L-1 sparsity on the optimization problem for finding the coefficients:
\begin_inset Formula 
\[
E[\bm{a}]=\sum_{x}\Big\{ I(x)-\sum_{i}a_{i}b_{i}(x)\Big\}^{2}+\lambda\sum_{i}\big|a_{i}\big|
\]

\end_inset

Then the solution is 
\begin_inset Formula $\hat{\bm{a}}=\arg\min E[\bm{a}]$
\end_inset

.
\end_layout

\begin_layout Subsection
Q5
\end_layout

\begin_layout Standard

\bar under
What is the miracle of sparsity? Describe L1 sparsity and show, for a simple
 example, how it results in a sparse representation.
\end_layout

\begin_layout Standard
Consider encoding an input 
\begin_inset Formula $y$
\end_inset

 given 
\begin_inset Formula $2N$
\end_inset

 bases functions 
\begin_inset Formula $\{\bar{b}_{i}\}$
\end_inset

 with minimum 
\begin_inset Formula $a=\sum_{i}\bar{a}_{i}$
\end_inset

, the set
\begin_inset Formula 
\[
\{y:\|y-\sum_{i}\bar{a}_{i}\bar{b}_{i}\|\quad s.t.\ \sum_{i}\bar{a}_{i}=a\}
\]

\end_inset

specifies the convex hull of the bases with radius 
\begin_inset Formula $a$
\end_inset

.
 Then, for an input data 
\begin_inset Formula $y$
\end_inset

 where 
\begin_inset Formula $|y|=1$
\end_inset

, solving for 
\begin_inset Formula $\bar{a}_{i}$
\end_inset

 corresponds to finding the closest point 
\begin_inset Formula $y_{p}$
\end_inset

 on the convex hull.
 When sparsity is enforced, the radius 
\begin_inset Formula $a$
\end_inset

 of the convex hull will also be penalized.
 Hence, 
\begin_inset Formula $y$
\end_inset

 is projected to a point 
\begin_inset Formula $y_{p}$
\end_inset

 on the boundary of the convex hull.
\end_layout

\begin_layout Standard
With an increasing 
\begin_inset Formula $\lambda$
\end_inset

, the penalty for the radius of the convex hull also increases, and hence
 causing the radius to get smaller.
 In this case, more bases will have zero coefficients.
\end_layout

\begin_layout Subsection
Q6
\end_layout

\begin_layout Standard

\bar under
Discuss the relative advantages of Principal Component Analysis and Sparse
 Coding for face recognition.
\end_layout

\begin_layout Standard
In case of face recognition, faces can be aligned to remove shift-invariance,
 and the PCA bases will not be sinusoids.
 PCA is also fast and accurate.
\end_layout

\begin_layout Standard
Sparse Coding requires less space to store the representation of images.
\end_layout

\begin_layout Section
Lecture 3
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard

\bar under
What is the k-means algorithm? What are the means, the assignment variable,
 and k? What are its convergence properties? What are the advantages of
 k-means++?
\end_layout

\begin_layout Standard
K-means algorithm represents each image by one basis function only.
 In other words, K-means algorithm finds the centers of clusters in the
 given image dataset.
\end_layout

\begin_layout Standard
The means are the centers of clusters.
 The assignment variable is the cluster index assignments for the data.
 The 
\begin_inset Formula $k$
\end_inset

 is a hyper-parameter specifying the number of clusters.
\end_layout

\begin_layout Standard
According to 
\begin_inset Quotes eld
\end_inset

Convergence Properties of the K-Means Algorithms
\begin_inset Quotes erd
\end_inset

 of Yoshua Bengio, K-means algorithm can be described either as a gradient
 descent algorithm or by slightly extending the mathematics of the EM algorithm
 to this hard threshold case.
 K-means algorithm actually minimizes the quantization error using the very
 fast newton algorithm.
\end_layout

\begin_layout Standard
K-means++ can avoid some poor clusterings found by the standard K-means
 algorithm.
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard

\bar under
How can k-means be used to learn a set of dictionary elements for image
 patches?
\end_layout

\begin_layout Standard
Mini-Epitomes, as an extended K-means algorithm can be used to learn dictionary
 for image patches.
 The steps are as follows:
\end_layout

\begin_layout Enumerate
Select mini-epitome 
\begin_inset Formula $k$
\end_inset

 with probability 
\begin_inset Formula $P(l_{i}=k)=\pi_{k}$
\end_inset

.
\end_layout

\begin_layout Enumerate
Select position 
\begin_inset Formula $p$
\end_inset

 within epitome uniformly.
\end_layout

\begin_layout Enumerate
Generate the patch 
\begin_inset Formula $x_{i}$
\end_inset

 as 
\begin_inset Formula $P(x_{i}|l_{i},p_{i})=\mathcal{N}(x_{i};\alpha_{i}T_{p_{i}}\mu_{l_{i}},\sigma^{2}I)$
\end_inset

.
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Subsection
Q4
\end_layout

\end_body
\end_document
