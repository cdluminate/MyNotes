#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\end_preamble
\use_default_options true
\begin_modules
customHeadersFooters
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HW#2
\end_layout

\begin_layout Author
Mo Zhou <mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Lec 9
\end_layout

\begin_layout Subsection
What is semantic segmentation? How can Deep Networks be used for it? How
 can it be improved by using Markov Random Fields?
\end_layout

\begin_layout Description
Semantic-Segmentation assigns a label to all pixels in the image.
\end_layout

\begin_layout Description
DNNs can be used for it based on training datasets with per-pixel annotation.
 Partial-annotation is also acceptable because it could be formulated by
 an EM algorithm.
\end_layout

\begin_layout Description
MRFs can improve semantic segmentation because DNNs ignore the spetial context
 while giving esitimates for the class labels for each pixel.
 In contrast, the spetial context (or temporal context) can be modeled with
 Markov Random Fields.
\end_layout

\begin_layout Subsection
What is a Gibbs distribution? What us the energy function? What is the normaliza
tion constant?
\end_layout

\begin_layout Description
Gibbs-distribution.
 A kind of probability function, also called Boltzmann distribution.
\begin_inset Formula 
\[
P(\bm{x})=\frac{\exp(-E(\bm{x}))}{\sum_{x}\exp(-E(\bm{x}))}
\]

\end_inset

where 
\begin_inset Formula $\bm{x}$
\end_inset

 is a vector input, 
\begin_inset Formula $E(\cdot)$
\end_inset

 is the energy function.
\end_layout

\begin_layout Description
Energy-Function.
 
\begin_inset Formula 
\[
E(\bm{x})=\sum_{i\in W}\phi_{i}(x_{i})+\sum_{i\in W}\sum_{j\in N(i)}\psi_{ij}(x_{i},x_{j})
\]

\end_inset

where 
\begin_inset Formula $\phi_{i}$
\end_inset

 is the unary potentials (unary evidence), and 
\begin_inset Formula $\psi_{ij}$
\end_inset

 is the pairwise potentials (context terms), 
\begin_inset Formula $i$
\end_inset

 is the pixels of the image 
\begin_inset Formula $W$
\end_inset

, and 
\begin_inset Formula $x_{i}$
\end_inset

 is the pixel label.
 The 
\begin_inset Formula $N(i)$
\end_inset

 is the neighborhood of pixel 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Description
Normalization-Constant.
 The normalization constant ensures the function sums to one in order to
 be a probablistic distribution.
\end_layout

\begin_layout Subsection
Describe a Markov Random Field (MRF).
 What is the Markov condition?
\end_layout

\begin_layout Description
MRF is a model specified by a distribution 
\begin_inset Formula $P(\bm{x}|\bm{z})$
\end_inset

 defined over discrete-valued random variables 
\begin_inset Formula $\bm{x}=\{x_{i}|i\in W\}$
\end_inset

:
\begin_inset Formula 
\[
P(\bm{x}|\bm{z})=\frac{1}{Z(\bm{z})}\exp\big\{-\sum_{i\in W}\phi_{i}(x_{i},\bm{z})-\sum_{i\in W}\sum_{j\in N(i)}\psi_{ij}(x_{i},x_{j})\big\}
\]

\end_inset

And the goal of MRF is to estimate the properties of the distribution such
 as the MAP estimator and the marginals.
\end_layout

\begin_layout Description
Markov-Condition.
 A variable is conditionally indepedent of all other variables given its
 neighbors, namely 
\begin_inset Formula $x_{i}\perp x_{W\backslash N(i)}|x_{N(i)}$
\end_inset

.
 This is also called the local markov property.
\end_layout

\begin_layout Subsection
Give two examples of MRFs from the course.
\end_layout

\begin_layout Standard
CRFs are special cases of MRF.
 There are Grid CRF and Fully-connected CRF in the course notes as example.
 Line process is also an MRF formulation of the weak-membrane model.
\end_layout

\begin_layout Section
Lec 10
\end_layout

\begin_layout Subsection
What is the core idea of mean field theory (MFT)? How can MFT be used to
 convert a discrete optimization problem into a continuous one? What is
 the Kullback-Leibler divergence and how is it used in mean field theory?
\end_layout

\begin_layout Description
Core-Idea of MFT is to find a distribution 
\begin_inset Formula $Q(\bm{x})$
\end_inset

 which approximates 
\begin_inset Formula $P(\bm{x})$
\end_inset

 and for which 
\begin_inset Formula $\hat{\bm{x}}=\arg\max Q(\bm{x})$
\end_inset

 can be estimated, instead of directly estimating 
\begin_inset Formula $\hat{\bm{x}}=\arg\max P(\bm{x})$
\end_inset

.
 The distribution should be factorizable so it will be easy to find the
 argmax: 
\begin_inset Formula $Q(\bm{x})=\prod_{i\in W}q_{i}(x_{i})$
\end_inset

.
\end_layout

\begin_layout Description
Convert.
 The mean field free energies are functions of continuous variables (since
 discrete variables have been replaced by continuous probability distributions),
 which enables us to compute gradients of the gree energy.
 This allows us to use steepest descent algorithms and its many variants.
\end_layout

\begin_layout Description
KL-Divergence.
 
\begin_inset Formula 
\[
D_{KL}(Q||P)=\sum_{x}Q(x)\log\frac{Q(x)}{P(x)}
\]

\end_inset

measures the divergence between the estimated 
\begin_inset Formula $Q$
\end_inset

 and 
\begin_inset Formula $P$
\end_inset

.
 It is equivalent to the free energy plus the lower bound 
\begin_inset Formula $D_{KL}(Q||P)=F_{MFT}(Q)+\log Z$
\end_inset

.
 And the free energy can be minimized using gradiengt methods.
\end_layout

\begin_layout Subsection
What is the MFT free energy? Is it convex or not? What strategy can be used
 to improve performance if it is not convex?
\end_layout

\begin_layout Description
MFT free energy is
\begin_inset Formula 
\[
F_{MFT}(\bm{q})=\sum_{ij}\sum_{x_{i},x_{j}}q_{i}(x_{i})q_{j}(x_{j})\psi_{ij}(x_{i},x_{j})+\sum_{i\in W}\sum_{x_{i}}q_{i}(x_{i})\phi_{i}(x_{i},\bm{z})+\sum_{i\in W}\sum_{x_{i}}q_{i}(x_{i})\log q_{i}(x_{i})
\]

\end_inset

which is equivalent to the KL divergence between 
\begin_inset Formula $Q$
\end_inset

 and 
\begin_inset Formula $P$
\end_inset

 plus a lower bound 
\begin_inset Formula $\log Z$
\end_inset

.
\end_layout

\begin_layout Description
Convex.
 MFT free energy is monotically decreasing, but not necessarily convex,
 because 
\begin_inset Formula 
\begin{align*}
\frac{\partial F_{MFT}}{\partial t} & =\sum_{i}\frac{\partial F_{MFT}}{\partial q_{i}}\frac{dq_{i}}{dt}=-\sum_{i}\{\frac{\partial F_{MFT}}{\partial q_{i}}\}^{2}\\
\frac{\partial F_{MFT}}{\partial t^{2}} & =-2\sum_{i}(\frac{\partial F_{MFT}}{\partial q_{i}}\frac{\partial F_{MFT}}{\partial q_{i}\partial t})
\end{align*}

\end_inset

and the second order derivative is not always greater or equal than zero.
 The concavity depends on 
\begin_inset Formula $\psi_{ij}$
\end_inset

.
\end_layout

\begin_layout Description
If-not-convex.
 We can add an extra term 
\begin_inset Formula $\lambda\sum q_{i}^{2}$
\end_inset

 to ensure convexity.
\end_layout

\begin_layout Subsection
Specify an MFT algorithm.
 What conditions guarantee that an MFT algorithm converges to a local minimum
 of the free energy?
\end_layout

\begin_layout Description

\series bold
MFT-Algorithm.

\series default
 A algorithm based on discrete approximation is 
\begin_inset Formula 
\[
q_{i}^{t+1}=q_{i}^{t}-\Delta\frac{\partial F_{MFT}}{\partial q_{i}}
\]

\end_inset

There is also a class of discrete iterative algorithms in the 
\begin_inset Formula $q^{t+1}=f(q^{t})$
\end_inset

 form.
 Algorithms can be derived using variational bounding and CCCP.
\end_layout

\begin_layout Description
Condition.
 An MFT algorithm is guaranteed to converge at a local minimum as long as
 
\begin_inset Formula $F_{MFT}(q^{t+1})\leq F_{MFT}(q^{t})$
\end_inset

.
\end_layout

\begin_layout Subsection
When can MFT be applied to MRFâ€™s with long range interactions efficiently?
\end_layout

\begin_layout Subsection
What is Deterministic Annealing? What is its justification? Is it guaranteed
 to converge to a global optimum?
\end_layout

\begin_layout Description
Deterministic-Annealing is a continuation method which can improve mean
 field theory.
 This specifies a family of free energy functions 
\begin_inset Formula $F(\cdot,T)$
\end_inset

 parameterized by temperature 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Description
Justification.
 The fixed value 
\begin_inset Formula $T=1$
\end_inset

 corresponds to the problem that we want to solve.
 And the key idea is that the energy functions get more convex as 
\begin_inset Formula $T$
\end_inset

 increases.
 So the algorithm finds a minimum of 
\begin_inset Formula $F(\cdot,T)$
\end_inset

 for large 
\begin_inset Formula $T$
\end_inset

 and gives it initial conditions for minimizing 
\begin_inset Formula $F(\cdot,T)$
\end_inset

 at smaller 
\begin_inset Formula $T$
\end_inset

.
\end_layout

\begin_layout Description
Guarantee.
 There is no guarantee that deterministic annealing will converge to the
 global optimum.
 But empirically it yields good results.
\end_layout

\begin_layout Section
Lec 11
\end_layout

\begin_layout Subsection
Describe the belief propagation algorithm? What are the messages and how
 do they relate to the marginal probabilities? Under what conditions is
 it guaranteed to converge to the correct solution?
\end_layout

\begin_layout Subsection
What is the Bethe free energy and how does it relate to belief propagation?
 How can the messages in belief propagation be justified?
\end_layout

\begin_layout Subsection
Under what conditions does the Bethe free energy reduce to the MFT free
 energy?
\end_layout

\begin_layout Section
Lec 12
\end_layout

\begin_layout Subsection
What are the advantages of formulating probability distributions in terms
 of graphs?
\end_layout

\begin_layout Subsection
What is the dynamic programming algorithm? For what class of problems does
 dynamic programming apply? For these problems, how does it reduce the complexit
y of computation?
\end_layout

\begin_layout Subsection
What is the difference between direct and indirect influence?
\end_layout

\begin_layout Subsection
What are Stochastic Grammars? What are AND/OR graphs?
\end_layout

\begin_layout Section
Lec 13
\end_layout

\begin_layout Subsection
What is the correspondence problem in binocular stereo? What is the disparity
 and how does it relate to depth?
\end_layout

\begin_layout Subsection
How can be stereo be formulated in terms of a Markov Random Field (MRF)?
 and what are the properties of that MRF?
\end_layout

\begin_layout Subsection
What is the epipolar line constraint, and how does it simplify the correspondenc
e problem? What inference algorithm can be used to solve it? What inference
 algorithm can be used if the epipolar line constraint is not used?
\end_layout

\begin_layout Subsection
Why is Belief Propagation a sensible algorithm to use for binocular stereo?
\end_layout

\end_body
\end_document
