#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\end_preamble
\use_default_options true
\begin_modules
customHeadersFooters
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle fancy
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HW#2
\end_layout

\begin_layout Author
Mo Zhou <mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Lec 9
\end_layout

\begin_layout Subsection
What is semantic segmentation? How can Deep Networks be used for it? How
 can it be improved by using Markov Random Fields?
\end_layout

\begin_layout Description
Semantic-Segmentation assigns a label to all pixels in the image.
\end_layout

\begin_layout Description
DNNs can be used for it based on training datasets with per-pixel annotation.
 Partial-annotation is also acceptable because it could be formulated by
 an EM algorithm.
\end_layout

\begin_layout Description
MRFs can improve semantic segmentation because DNNs ignore the spetial context
 while giving esitimates for the class labels for each pixel.
 In contrast, the spetial context (or temporal context) can be modeled with
 Markov Random Fields.
\end_layout

\begin_layout Subsection
What is a Gibbs distribution? What us the energy function? What is the normaliza
tion constant?
\end_layout

\begin_layout Description
Gibbs-distribution.
 A kind of probability function, also called Boltzmann distribution.
\begin_inset Formula 
\[
P(\bm{x})=\frac{\exp(-E(\bm{x}))}{\sum_{x}\exp(-E(\bm{x}))}
\]

\end_inset

where 
\begin_inset Formula $\bm{x}$
\end_inset

 is a vector input, 
\begin_inset Formula $E(\cdot)$
\end_inset

 is the energy function.
\end_layout

\begin_layout Description
Energy-Function.
 
\begin_inset Formula 
\[
E(\bm{x})=\sum_{i\in W}\phi_{i}(x_{i})+\sum_{i\in W}\sum_{j\in N(i)}\psi_{ij}(x_{i},x_{j})
\]

\end_inset

where 
\begin_inset Formula $\phi_{i}$
\end_inset

 is the unary potentials (unary evidence), and 
\begin_inset Formula $\psi_{ij}$
\end_inset

 is the pairwise potentials (context terms), 
\begin_inset Formula $i$
\end_inset

 is the pixels of the image 
\begin_inset Formula $W$
\end_inset

, and 
\begin_inset Formula $x_{i}$
\end_inset

 is the pixel label.
 The 
\begin_inset Formula $N(i)$
\end_inset

 is the neighborhood of pixel 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\begin_layout Description
Normalization-Constant.
 The normalization constant ensures the function sums to one in order to
 be a probablistic distribution.
\end_layout

\begin_layout Subsection
Describe a Markov Random Field (MRF).
 What is the Markov condition?
\end_layout

\begin_layout Description
MRF is a model specified by a distribution 
\begin_inset Formula $P(\bm{x}|\bm{z})$
\end_inset

 defined over discrete-valued random variables 
\begin_inset Formula $\bm{x}=\{x_{i}|i\in W\}$
\end_inset

:
\begin_inset Formula 
\[
P(\bm{x}|\bm{z})=\frac{1}{Z(\bm{z})}\exp\big\{-\sum_{i\in W}\phi_{i}(x_{i},\bm{z})-\sum_{i\in W}\sum_{j\in N(i)}\psi_{ij}(x_{i},x_{j})\big\}
\]

\end_inset

And the goal of MRF is to estimate the properties of the distribution such
 as the MAP estimator and the marginals.
\end_layout

\begin_layout Description
Markov-Condition.
 A variable is conditionally indepedent of all other variables given its
 neighbors, namely 
\begin_inset Formula $x_{i}\perp x_{W\backslash N(i)}|x_{N(i)}$
\end_inset

.
 This is also called the local markov property.
\end_layout

\begin_layout Subsection
Give two examples of MRFs from the course.
\end_layout

\begin_layout Standard
CRFs are special cases of MRF.
 There are Grid CRF and Fully-connected CRF in the course notes as example.
\end_layout

\begin_layout Section
Lec 10
\end_layout

\begin_layout Subsection
What is the core idea of mean field theory (MFT)? How can MFT be used to
 convert a discrete optimization problem into a continuous one? What is
 the Kullback-Leibler divergence and how is it used in mean field theory?
\end_layout

\begin_layout Subsection
What is the MFT free energy? Is it convex or not? What strategy can be used
 to improve performance if it is not convex?
\end_layout

\begin_layout Subsection
Specify an MFT algorithm.
 What conditions guarantee that an MFT algorithm converges to a local minimum
 of the free energy?
\end_layout

\begin_layout Subsection
When can MFT be applied to MRFâ€™s with long range interactions efficiently?
\end_layout

\begin_layout Subsection
What is Deterministic Annealing? What is its justification? Is it guaranteed
 to converge to a global optimum?
\end_layout

\begin_layout Section
Lec 11
\end_layout

\begin_layout Subsection
Describe the belief propagation algorithm? What are the messages and how
 do they relate to the marginal probabilities? Under what conditions is
 it guaranteed to converge to the correct solution?
\end_layout

\begin_layout Subsection
What is the Bethe free energy and how does it relate to belief propagation?
 How can the messages in belief propagation be justified?
\end_layout

\begin_layout Subsection
Under what conditions does the Bethe free energy reduce to the MFT free
 energy?
\end_layout

\begin_layout Section
Lec 12
\end_layout

\begin_layout Subsection
What are the advantages of formulating probability distributions in terms
 of graphs?
\end_layout

\begin_layout Subsection
What is the dynamic programming algorithm? For what class of problems does
 dynamic programming apply? For these problems, how does it reduce the complexit
y of computation?
\end_layout

\begin_layout Subsection
What is the difference between direct and indirect influence?
\end_layout

\begin_layout Subsection
What are Stochastic Grammars? What are AND/OR graphs?
\end_layout

\begin_layout Section
Lec 13
\end_layout

\begin_layout Subsection
What is the correspondence problem in binocular stereo? What is the disparity
 and how does it relate to depth?
\end_layout

\begin_layout Subsection
How can be stereo be formulated in terms of a Markov Random Field (MRF)?
 and what are the properties of that MRF?
\end_layout

\begin_layout Subsection
What is the epipolar line constraint, and how does it simplify the correspondenc
e problem? What inference algorithm can be used to solve it? What inference
 algorithm can be used if the epipolar line constraint is not used?
\end_layout

\begin_layout Subsection
Why is Belief Propagation a sensible algorithm to use for binocular stereo?
\end_layout

\end_body
\end_document
