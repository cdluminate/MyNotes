#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{times}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HW #3
\end_layout

\begin_layout Author
Mo Zhou
\begin_inset Newline newline
\end_inset

<mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Exponential Models with Hidden Variables
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard
What is an exponential model with hidden variables? What are the potentials,
 the parameters, and the normalization term Z?.
 What are the derivatives of Z with respect to the parameters of the exponential
 model? How can the EM algorithm be used for doing maximum likelihood estimation
 for an exponential model with hidden variables?
\end_layout

\begin_layout Description
Exponential-Distribution.
 The general form of an exponential distribution is
\begin_inset Formula 
\[
p(\bm{x}|\bm{\lambda})=\frac{1}{Z[\bm{\lambda}]}\exp\{\bm{\lambda}\cdot\bm{\phi}(\bm{x})\}
\]

\end_inset

where 
\begin_inset Formula $Z[\bm{\lambda}]$
\end_inset

 is the normalization factor, 
\begin_inset Formula $\bm{\lambda}=(\lambda_{1},\lambda_{2},\ldots,\lambda_{M})$
\end_inset

 are the parameters and 
\begin_inset Formula 
\[
\bm{\phi}(\bm{x})=(\phi_{1}(\bm{x}),\phi_{2}(\bm{x}),\ldots,\phi_{M}(\bm{x}))
\]

\end_inset

 are the statistics (potential).
 Accordingly, an exponential model w/ hidden variables is
\begin_inset Formula 
\[
p(\bm{d},\bm{h}|\bm{\lambda})=\frac{1}{Z[\bm{\lambda}]}\exp\{\bm{\lambda}\cdot\bm{\phi}(\bm{d},\bm{h})\}
\]

\end_inset

where 
\begin_inset Formula $Z[\bm{\lambda}]=\sum_{\bm{d,h}}\exp\{\bm{\lambda\cdot\phi}(\bm{d,h})\}$
\end_inset

 is the normalization term.
 The parameters are 
\begin_inset Formula 
\[
\bm{\lambda}=(\lambda_{1},\lambda_{2},\ldots,\lambda_{M},\mu_{1},\mu_{2},\ldots,\mu_{M-1}).
\]

\end_inset

The potentials are 
\begin_inset Formula 
\[
\bm{\phi(d,h)}=(\phi(d_{i},h_{i})_{i=1,\ldots,M},\psi(h_{i},h_{i+1})_{i=1,\ldots,M-1})
\]

\end_inset


\end_layout

\begin_layout Description
Derivatives.
 The derivates of 
\begin_inset Formula $Z$
\end_inset

 w.r.t.
 the parameters is
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\bm{\lambda}}\log Z[\bm{\lambda}] & =\frac{\partial}{\partial\bm{\lambda}}\log\sum_{\bm{h,d}}\exp\{\bm{\lambda\cdot\phi}(\bm{h,d})\}\\
 & =\frac{1}{\sum_{\bm{h,d}}\exp\{\bm{\lambda\cdot\phi}(\bm{h,d})\}}\sum_{\bm{h,d}}\bm{\phi}(\bm{h,d})\cdot\exp\{\bm{\lambda\cdot\phi}(\bm{h,d})\}\\
 & =\sum_{\bm{h,d}}p(\bm{h,d}|\bm{\lambda})\cdot\bm{\phi}(\bm{h,d})
\end{align*}

\end_inset


\end_layout

\begin_layout Description
EM-Algorithm-MLE.
 Given the dataset 
\begin_inset Formula $D=\{\bm{d}^{m}:m=1,\ldots,M\}$
\end_inset

, we want to find the MLE
\begin_inset Formula 
\[
\hat{\bm{\lambda}=\arg\max_{\lambda}\prod_{m=1}^{M}p(\bm{d}^{m}|\bm{\lambda})=\arg\max_{\lambda}\prod_{m=1}^{M}\sum_{\bm{h}^{m}}p(\bm{d}^{m},\bm{h}^{m}|\bm{\lambda})}.
\]

\end_inset

To solve this, we introduce distrinution 
\begin_inset Formula $Q_{m}(\bm{h}^{m})$
\end_inset

, and the free energy can be expressed as
\begin_inset Formula 
\[
F[\bm{\lambda}:\{Q_{m}(\bm{h}^{m})\}]=\sum_{m-1}^{M}\Big\{-\log p(\bm{d}^{m}|\bm{\lambda})+\sum_{\bm{h}^{m}}Q_{m}(\bm{h}^{m})\log\frac{Q_{m}(\bm{h}^{m})}{p(\bm{h}^{m}|\bm{d}^{m},\bm{\lambda})}\Big\}
\]

\end_inset

Then the EM algorithm minimizes free energy w.r.t 
\begin_inset Formula $\lambda$
\end_inset

 and the 
\begin_inset Formula $\{Q_{m}(\cdot)\}$
\end_inset

 alternatively:
\begin_inset Formula 
\begin{align*}
\bm{\lambda}^{t+1} & =\arg\min_{\lambda}F[\bm{\lambda}:\{Q_{m}^{t}(\cdot)\}]=\arg\min_{\lambda}\big\{-\sum_{\bm{h}^{m}}Q_{m}^{t}(\bm{h}^{m})\log p(\bm{h}^{m},\bm{d}^{m}|\bm{\lambda})\big\}\\
Q_{m}^{t+1}(\bm{h}^{m}) & =p(\bm{h}^{m}|\bm{d}^{m},\bm{\lambda}^{t})
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard
Now suppose that the exponential model can be expressed as a probability
 distribution on variables defined on a graph.
 How can the EM computations be done if the graph has no closed loops? What
 if the graph has closed loops? Does the EM algorithm attempt to make the
 statistics of the data equal to the expected statistics of the model?
\end_layout

\begin_layout Description
EM-Computations.
 When there is no closed loop in the graph, we can compute
\begin_inset Formula 
\begin{align*}
P(\bm{h}^{m}|\bm{d}^{m},\bm{\lambda}^{t}) & =\frac{P(\bm{h}^{m},\bm{d}^{m}|\bm{\lambda}^{t})}{P(\bm{d}^{m}|\bm{\lambda}^{t})}\\
P(\bm{d}^{m}|\bm{\lambda}^{t}) & =\frac{1}{Z[\bm{\lambda}]}\sum_{\bm{h}^{m}}\exp\{\bm{\lambda}\cdot\bm{\phi}(\bm{d}^{m},\bm{h}^{m})\}
\end{align*}

\end_inset

If the graph has closed loop, it can be approximated by BP (sum-product).
\end_layout

\begin_layout Description
Statics-Equal.
 For the update rule for 
\begin_inset Formula $\bm{\lambda}^{t+1}$
\end_inset

, we can convert the problem into minimizing
\begin_inset Formula 
\[
G(\bm{\lambda})=-\sum_{m=1}^{M}Q_{m}^{t}(\bm{h}^{m})\cdot\bm{\lambda}\cdot\bm{\phi}(\bm{h}^{m},\bm{d}^{m})-\sum_{m=1}^{M}\log Z[\bm{\lambda}].
\]

\end_inset

And the function 
\begin_inset Formula $G(\bm{\lambda})$
\end_inset

 is a convex function of 
\begin_inset Formula $\bm{\lambda}$
\end_inset

, because 
\begin_inset Formula $\log Z[\cdot]$
\end_inset

 is convex.
 Hence, the global minimum 
\begin_inset Formula $\hat{\bm{\lambda}}$
\end_inset

 occurs at the zero-gradient point where 
\begin_inset Formula $\partial G(\bm{\hat{\lambda}})/\partial\bm{\lambda}=0$
\end_inset

, namely
\begin_inset Formula 
\[
\frac{1}{M}\sum_{m=1}^{M}Q_{m}^{t}(\bm{h}^{m})\bm{\phi}(\bm{h}^{m},\bm{d}^{m})=\sum_{\bm{h,d}}\bm{\phi}(\bm{h},\bm{d})P(\bm{h},\bm{d}|\bm{\lambda})
\]

\end_inset

which is exactly when the expected statistics w.r.t.
 data 
\begin_inset Formula $\bm{d}^{m}$
\end_inset

 and 
\begin_inset Formula $Q_{m}(\cdot)$
\end_inset

 equals the expected statistics of the model.
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard
Describe how a Hidden Markov Model (HMM) can be expressed as an exponential
 model with hidden variables? Does this correspond to a graph with closed
 loops? What inference algorithms are used?
\end_layout

\begin_layout Description
HMM.
 Almost every named distribution can be expressed as an exponential distribution.
 So exponential model with hidden variables is a generalization of the Hidden
 Markov Models.
 HMM correspond to a graph without closed loop.
 Inference algorithm is DP (dynamic programming).
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Lighting Models
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard
What is the Lambertian lighting model? What is the albedo? What is the Generaliz
ed Bas Relief (GBR) ambiguity? How does it relate to the ambiguity between
 convex and concave shapes? What are cast and attached shadows? Does the
 GBR hold for cast and attached shadows?
\end_layout

\begin_layout Description
Lambertian-Model.
 The Lambertian reflectance model is the simplest way to model how images
 are generated from three dimensional objects illuminated by various light
 sources.
 The linear Lambertian model is
\begin_inset Formula 
\[
I(\vec{x})=a(\vec{x})\vec{n}(\vec{x})\cdot\vec{s}
\]

\end_inset

where 
\begin_inset Formula $I(\vec{x})$
\end_inset

 is the image, 
\begin_inset Formula $a(\vec{x})$
\end_inset

 is the albedo, 
\begin_inset Formula $\vec{n}(\vec{x})$
\end_inset

 is the surface normal, 
\begin_inset Formula $\vec{s}$
\end_inset

 is the light source.
\end_layout

\begin_layout Description
Albedo.
 It is the measure of the diffuse reflection of solar radiation out of the
 total solar radiation and measured on a scale from 0 to 1.
\end_layout

\begin_layout Description
GBR.
 There is ambiguity in the estimation of shape from multiple images with
 unknown lighting and fixed viewpoint.
 The GBR ambiguity assumes that objects have Lambertian reflectance functions
 but allows for shadows (cast and attached) and multiple light sources (but
 no interflections).
 GBR includes the convex versus concave amgiguity and the bas-relief ambiguity
 as special cases and is of practical importance for photometric stereo.
 Given a surface 
\begin_inset Formula $z(x,y)$
\end_inset

, the surfaces related linearly to 
\begin_inset Formula $z$
\end_inset

 are
\begin_inset Formula 
\begin{align*}
\tilde{z}(x,y) & =ax+by+cz(x,y)\\
G=\begin{bmatrix}1 & 0 & 0\\
0 & 1 & 0\\
a & b & c
\end{bmatrix} & \qquad G^{-1}=\frac{1}{c}\begin{bmatrix}c & 0 & 0\\
0 & c & 0\\
-a & -b & 1
\end{bmatrix}
\end{align*}

\end_inset

They form a sub-group of 
\begin_inset Formula $GL(3)$
\end_inset

.
\end_layout

\begin_layout Description
Convex-Concave.
 There is a well-known preceptual ambiguity – it is impossible to distinguish
 between a convex object lit from above and a concave object lit from below.
 Humans resolve this ambiguity by tending to perceive objects to be convex.
 Thus, the linear model must be modified to account for shadows.
\end_layout

\begin_layout Description
Shadows.
 There are two types of shadows: (i) attached shadows, and (ii) cast shadows
 (where light source is occluded).
 The cast shadows are more challenging.
 GBR holds for cast and attached shadows.
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard
What is photometric stereo? How can it be formulated in terms of Singular
 Value Decomposition? What are the ambiguities?
\end_layout

\begin_layout Description
Photometric-Stereo.
 Given several images of a Lambertian object under varying lighting, assuming
 single directional source, we have 
\begin_inset Formula 
\begin{align*}
M & =LS\\
\begin{bmatrix}I_{11} & \cdots & I_{1p}\\
\vdots &  & \vdots\\
I_{f1} & \cdots & I_{fp}
\end{bmatrix}_{f\times p} & =\begin{bmatrix}l_{1x} & l_{1y} & l_{1z}\\
 & \vdots\\
l_{fx} & l_{fy} & l_{fz}
\end{bmatrix}_{f\times3}\begin{bmatrix}n_{x1} &  & n_{xp}\\
n_{y1} & \cdots & n_{yp}\\
n_{z1} &  & n_{zp}
\end{bmatrix}_{3\times p}
\end{align*}

\end_inset

We can solve for 
\begin_inset Formula $S$
\end_inset

 if 
\begin_inset Formula $L$
\end_inset

 is known (Woodham).
 This algorithm can be extended to more complex reflection models (if known)
 through the use of a lookup table.
\end_layout

\begin_layout Description
Factorization.
 We can use SVD to find a rank 3 approximation for
\begin_inset Formula 
\[
M=U\Sigma V^{T}
\]

\end_inset

So we define 
\begin_inset Formula $\Sigma_{3}=\text{diag}(\sigma_{1},\sigma_{2},\sigma_{3})$
\end_inset

, where 
\begin_inset Formula $\sigma_{1},\sigma_{2},\sigma_{3}$
\end_inset

 are the largest singular values of 
\begin_inset Formula $M$
\end_inset

, and
\begin_inset Formula 
\[
\hat{L}=U\sqrt{\Sigma_{3}},\quad\hat{S}=\sqrt{\Sigma_{3}}V^{T},\quad M\approx\hat{L}\hat{S}
\]

\end_inset


\end_layout

\begin_layout Description
Ambiguity.
 Factorization is not unique, since
\begin_inset Formula 
\[
\hat{M}=(\hat{L}A^{-1})(A\hat{S})
\]

\end_inset

where 
\begin_inset Formula $A$
\end_inset

 is 
\begin_inset Formula $3\times3$
\end_inset

 invertible.
 However, we can reduce such ambiguity by imposing intergrability.
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard
How many bases are required to describe the image of an object if shadows
 are ignored? How can principal component analysis (PCA) be used to estimate
 the number of bases? What theory predicts the number of bases for convex
 objects? And how many bases are predicted?
\end_layout

\begin_layout Description
Shadows-Ignored.
 The linear Lambertian model (i.e.
 ignoring shadows) implies that the image of an object lies in a three-dimension
al space.
 This implies that the image can be modeled as
\begin_inset Formula 
\[
I(\vec{x})=\sum_{i=1}^{3}\alpha_{i}e_{i}(\vec{x})
\]

\end_inset

 Analysis shows that the first five eigenvectors typically contain 90 percent
 of the energy (sum of all the eigenvalues).
\end_layout

\begin_layout Description
PCA.
 The linear model can be investigated empirically by taking photographs
 of an object from different lighting conditions.
 To get a series of images 
\begin_inset Formula $\{I^{\mu}(\vec{x})\}$
\end_inset

.
 We can then compute the correlation matrix
\begin_inset Formula 
\[
K(\vec{x},\vec{x}')=\frac{1}{N}\sum_{\mu=1}^{N}I^{\mu}(\vec{x})I^{\mu}(\vec{x}')
\]

\end_inset

Then calculate the eigenvectors and eigenvalues
\begin_inset Formula 
\[
\sum_{\vec{x}'}K(\vec{x},\vec{x}')e(\vec{x}')=\lambda e(\vec{x})
\]

\end_inset

This analysis shows that the first five eigenvectors typically contain 90
 percent of the energy (sum of all eigenvalues).
 This plots
\begin_inset Formula 
\[
\frac{\sum_{i=1}^{n}\lambda_{i}}{\sum_{i=1}^{N}\lambda_{i}}
\]

\end_inset

as a function of 
\begin_inset Formula $n$
\end_inset

 where 
\begin_inset Formula $N$
\end_inset

 is the total number of eigenvalues.
\end_layout

\begin_layout Description
Theory.
 Independently done in the computer vision community by Basri and Jacobs,
 and in the computer graphics community by Ramamoothi and Hanrahan – showed
 that for convex objects 
\bar under
nine
\bar default
 eigenvectors captured much for the intensity variations, and if the lightingwas
 restricted to come from the frontal hemisphere then only 
\bar under
five
\bar default
 were needed.
 These theoretical studies where done by using spherical harmonics (fourier
 theory on a sphere) and showing that only a limited number were neede.
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
AdaBoost
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard
What is a weak classifier? What is a strong classifier?
\end_layout

\begin_layout Description
Weak-Classifier.
 A (binary) classifier that is only slightly more accurate than random classific
ation.
\end_layout

\begin_layout Description
Strong-Classifier.
 A classifier that achieves arbitrarily good accuracy.
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard
What function does AdaBoost minimize? What is its relationship to the loss
 function for binary classification? How does the update rule correspond
 to weighting missclassified samples more highly? Can a weak classifier
 be selected twice by AdaBoost?
\end_layout

\begin_layout Description
AdaBoost.
 AdaBoost is a method for combining many weak classifiers to make a strong
 classifier.
 Given a set of weak classifiers 
\begin_inset Formula $\{\phi_{\mu}(x):\mu=1,\ldots,M\}$
\end_inset

, labelled data 
\begin_inset Formula $X=\{(x^{i},y^{i}):i=1,\ldots,N\}$
\end_inset

 with 
\begin_inset Formula $y^{i}\in\{\pm1\}$
\end_inset

.
 The output of AdaBoost is a strong classifier 
\begin_inset Formula $S(x)=\text{sign}(\sum_{\mu=1}^{M}\lambda_{\mu}\phi_{\mu}(x))$
\end_inset

, where the 
\begin_inset Formula $\{\lambda_{\mu}\}$
\end_inset

 are weights to be learned.
 The function AdaBoost wants to minimize is the empirical risk.
\end_layout

\begin_layout Description
Loss-Function.
 The empirical risk of AdaBoost is a weighted sum of the binary classification
 loss functions, parameterized by 
\begin_inset Formula $\lambda$
\end_inset

.
 
\end_layout

\begin_layout Description
Update-Rule.
 For each weak classifier 
\begin_inset Formula $\phi_{\mu}(\cdot)$
\end_inset

, we deivice the data into two sets (i) 
\begin_inset Formula $W_{\mu}^{+}=\{i:y^{i}\phi_{\mu}(x^{i})=1\}$
\end_inset

, namely the set of data which 
\begin_inset Formula $\phi_{\mu}(\cdot)$
\end_inset

 classifies correctly; (ii) 
\begin_inset Formula $W_{\mu}^{-}=\{i:y^{i}\phi_{\mu}(x^{i})=-1\}$
\end_inset

, namely the set of data which it gets wrong.
 Then at each time step 
\begin_inset Formula $t$
\end_inset

, we define a set of 
\begin_inset Quotes eld
\end_inset

weights
\begin_inset Quotes erd
\end_inset

 for the training data:
\begin_inset Formula 
\[
D_{i}^{t}=\frac{\exp\{-y^{i}\sum_{\mu=1}^{M}\lambda_{\mu}^{t}\phi_{\mu}(x^{i})\}}{\sum_{i=1}^{N}\exp\{-y^{i}\sum_{\mu=1}^{M}\lambda_{\mu}^{t}\phi_{\mu}(x^{i})\}}
\]

\end_inset

These weights are all positive and sum to 1, i.e., 
\begin_inset Formula $\sum_{i}D_{i}^{t}=1$
\end_inset

.
 At 
\begin_inset Formula $t=0$
\end_inset

, all the weights take value 
\begin_inset Formula $1/N$
\end_inset

.
 Otherwise, the weights are largest for the data which is incorrectly classified
 by the classifer 
\begin_inset Formula $\text{sign}(\sum_{\mu=1}^{M}\lambda_{\mu}^{t}\phi_{\mu}(x))$
\end_inset

 (our current estimate of the strong classifier) and smallest for those
 which are correctly classified (to see this, look at the sign of the exponent).
 These weights are a way to take into account the weak classifiers we have
 already selected and the weights we have assigned them when we try to add
 a new classifier.
\end_layout

\begin_layout Description
Select-Twice.
 No, a weak classifier cannot be selected twise.
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard
What are cascades? How are they used for face and text detection? What types
 of image features and weak classifiers are used by AdaBoost to perform
 face detection? What types of features are used by AdaBoost to perform
 text detection?
\end_layout

\begin_layout Description
Cascades.
 Simple, boosted classifiers can reject many of negative sub-windows while
 detecting all positive instances.
 Series of such simple classifiers can achieve good detection performance
 while eliminating the need for further processing of negative sub-windows.
\end_layout

\begin_layout Description
Feature-Face.
 For face recognition, simple features are used.
 There are 3 rectangular feature types: (1) two-rectangle feature type (horizont
al or vertical), (2) three-rectangle feature type, (3) four-rectangle feature
 type.
 Using a 24x24 pixel base detection window, with all the possible combination
 of horizontal and vertical location and scale of these feature types, the
 full set of features has 49,396 features.
 The motivation behind using regtangular features, as opposed to more expressive
 steerable filters is due to their extrreme computational efficiency.
\end_layout

\begin_layout Description
Features-Text.
 The features used for text detection is different from those used in face
 detection.
 There are multiple sets of features used for this purpose: (1) 1-st order
 derivatives for the image data; (2) histogram of intensity and gradient;
 (3) edge linking features (using statistics of the length of the linked
 edges).
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Support Vector Machines
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard
What is the margin of an SVM? How does SVM deal with non-separable data?
 What is the primal formulation of SVM? How does the SVM objective function
 relate to the empirical risk? What term helps prevent over-fitting to the
 training data? What is the hinge loss? How does learning an SVM differ
 from learning Gaussian distributions for the positive and negative data
 examples and then applying the log-likelihood rule?
\end_layout

\begin_layout Description
SVM.
 Support Vector Machine is a modern approach to linear separation.
 Suppose we have data 
\begin_inset Formula $\{(\vec{x}_{u},y_{u}):u=1,\ldots,N\}$
\end_inset

 where 
\begin_inset Formula $y_{u}\in\{-1,1\}$
\end_inset

.
 We define a hyperplane 
\begin_inset Formula $\langle\vec{x}:\vec{x}\cdot\vec{a}+b=0\rangle$
\end_inset

, where 
\begin_inset Formula $|\vec{a}|=1$
\end_inset

.
 The signed distance of a point 
\begin_inset Formula $\vec{x}$
\end_inset

 to the plane is then 
\begin_inset Formula $\vec{a}\cdot\vec{x}+b$
\end_inset

.
 If we project the line 
\begin_inset Formula $\vec{x}(\vec{\lambda})=\vec{x}+\lambda\vec{a}$
\end_inset

, it hits plane when 
\begin_inset Formula $\vec{a}\cdot(\vec{x}+\lambda\vec{a})=-b$
\end_inset

.
 Follows that 
\begin_inset Formula $\lambda=-(\vec{a}\cdot\vec{x}+b)/|\vec{a}|^{2}$
\end_inset

, and if 
\begin_inset Formula $|\vec{a}|=1$
\end_inset

, then 
\begin_inset Formula $\lambda=-(\vec{a}\cdot\vec{x}+b)$
\end_inset

.
 In SVM we seek a classifier with biggest margin:
\begin_inset Formula 
\[
\max_{\vec{a},b,|\vec{a}|=1}C\qquad s.t.\quad y_{u}(\vec{x}_{u}\cdot\vec{a}+b)\geq C,\ \forall u\geq1\ldots N
\]

\end_inset

Namely, the positive examples are at least distance 
\begin_inset Formula $C$
\end_inset

 (margin) above the plane, and negative examples are at least 
\begin_inset Formula $C$
\end_inset

 below the plane.
 Having a large margin is good for generalization because there is less
 chance of an accidental alignment.
\end_layout

\begin_layout Description
Non-seperable-data.
 Perfect separation is not always possible.
 Let's allow for some data points to be misclassified.
 We define the slack variables 
\begin_inset Formula $\{z_{1,}\ldots,z_{n}\}$
\end_inset

 allowing data points to move in direction 
\begin_inset Formula $\vec{a}$
\end_inset

, so that they are on the right side of the margin.
 Then the criterion is
\begin_inset Formula 
\[
\max_{\vec{a},b,|\vec{a}|=1}C\qquad s.t.\quad y_{u}(\vec{x}_{u}\cdot\vec{a}+b)\geq C(q-z_{u}),\quad\forall u\in\{1,N\}\quad s.t.\quad z_{u}\geq0,\quad\forall u
\]

\end_inset

Alternatively, 
\begin_inset Formula $y_{u}\{(\vec{x}_{u}+Cz_{u}\vec{a})\cdot\vec{a}+b\}\geq C$
\end_inset

, which is like moving 
\begin_inset Formula $\vec{x}_{u}$
\end_inset

 to 
\begin_inset Formula $\vec{x}_{u}+z_{u}\vec{a}$
\end_inset

.
 But, we must pay a penalty for using slack variables.
 For example, a penalty 
\begin_inset Formula $\sum_{u=1}^{N}z_{u}$
\end_inset

.
 If 
\begin_inset Formula $z_{u}=0$
\end_inset

, then the data point is correctly classified and is past hte margin.
 If 
\begin_inset Formula $z_{u}>0$
\end_inset

, then wthe data is on the wrong side of the margin, and so had to be removed.
\end_layout

\begin_layout Description
Primal-Formulation.
 Here the task is to esimate several quantities simultaneously: (1) the
 plane 
\begin_inset Formula $\vec{a},b$
\end_inset

; (2) the margin 
\begin_inset Formula $C$
\end_inset

; (3) the slack variables 
\begin_inset Formula $\{z_{u}\}$
\end_inset

.
 We need a criterion that maximizes the margin and minimizes the amount
 of slack variables used.
 We absorb 
\begin_inset Formula $C$
\end_inset

 into 
\begin_inset Formula $\vec{a}$
\end_inset

 by 
\begin_inset Formula $\vec{a}\rightarrow a/c$
\end_inset

 and remove the constraint 
\begin_inset Formula $|\vec{a}|=1$
\end_inset

.
 Hnece, 
\begin_inset Formula $C=1/|\vec{a}|$
\end_inset

.
 Then, the max-margin criterion of SVM is
\begin_inset Formula 
\begin{align*}
\min & \frac{1}{2}\sum\vec{a}\cdot\vec{a}+\gamma\sum_{u}z_{u}\\
s.t. & y_{u}(\vec{x}_{u}\cdot\vec{a}+b)\geq1-z_{u}\quad\forall uz_{u}\geq0
\end{align*}

\end_inset

First, we need to solve the quadratic primal problem using Lagrange multipliers:
\begin_inset Formula 
\[
L_{p}(\vec{a},b,z;\alpha,\tau)=\frac{1}{2}\vec{a}\cdot\vec{a}+\gamma\sum_{u}z_{u}-\sum_{u}\alpha_{u}\{y_{u}(\vec{x}_{u}\cdot\vec{a}+b)-(1-z_{u})\}-\sum_{u}\tau_{u}z_{u}
\]

\end_inset

where the 
\begin_inset Formula $\{\alpha_{u}\}$
\end_inset

 and 
\begin_inset Formula $\{\tau_{u}\}$
\end_inset

 are Lagrange parameters needed to enforce the inequality constraints.
\end_layout

\begin_layout Description
Empirical-Risk/overfit/hinge.
 Suppose we look at the primal function 
\begin_inset Formula $L_{p}$
\end_inset

.
 Consider the constraint 
\begin_inset Formula $y_{u}(\vec{x}\cdot\vec{a}+b)-1>0$
\end_inset

.
 If this constraint is satisfied, then it is best to set the slack variable
 
\begin_inset Formula $z_{u}=0$
\end_inset

, because otherwise we pay a penalty 
\begin_inset Formula $\gamma$
\end_inset

 for it.
 If the constraint is not satisfied, then we set the slack variable to be
 
\begin_inset Formula $z_{u}=1-y_{u}(\vec{x}_{u}\cdot\vec{a})$
\end_inset

 because this is the smallest value of the slack variable which satisfies
 the constraint.
 We can summarize this by paying a hinge loss penalty 
\begin_inset Formula $\max\{0,1-y_{u}(\vec{x}_{u}\cdot\vec{a}+b)\}$
\end_inset

.
 
\bar under
The hinge loss is 
\begin_inset Formula $\max\{0,\cdot\}$
\end_inset

.

\bar default
 If the constraint is satisfied, then the maximum is 0, but if not, the
 maximum is 
\begin_inset Formula $1-y_{u}(\vec{x_{u}}\cdot\vec{a})$
\end_inset

, which is minimum value of the slack variable (to make the constraint satisfied.
).
 This gives an energy function:
\begin_inset Formula 
\[
L(\vec{a},b)=\frac{1}{2}\vec{a}\cdot\vec{a}+\gamma\sum_{u}\max\{0,1-y_{u}(\vec{x}_{u}\cdot\vec{a}+b)\}
\]

\end_inset

Note that the hinge loss is a convex loss function.
 So we can re-express the max-margin criterion as the sum of the empirical
 risk (with hinge los function):
\begin_inset Formula 
\[
\frac{L_{p}}{\gamma N}=\frac{1}{2\gamma N}\big|\vec{a}\big|^{2}+\frac{1}{N}\sum_{u=1}^{N}\max\big\{0,1-y_{u}(\vec{x}_{u}\cdot\vec{a}+b)\big\}
\]

\end_inset

The first term is a regularizer.
 It penalizes decision rules 
\begin_inset Formula $\hat{y}(\vec{x})=\text{sign}(\vec{x}\cdot\vec{a}+b)$
\end_inset

 which have large 
\begin_inset Formula $|\vec{a}|$
\end_inset

.
 This is done in order to help generalization (
\bar under
namely the first term helps reduce overfitting
\bar default
).
\end_layout

\begin_layout Description
Different-From-Gaussian.
 SVM learns a optimal classification hyperplane with max-margin criterion,
 while Gaussian ML classification learns Gaussian distributions instead.
 SVM is trained using optimization methods different than EM algorithm for
 estimating the Gaussian distributions.
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard
What is a kernel? How does it relate to feature vectors? What type of kernel
 makes an SVM behave like a nearest neighbor classifier?
\end_layout

\begin_layout Description
Kernel.
 Kernel function 
\begin_inset Formula $\phi(x)$
\end_inset

 maps feature vector 
\begin_inset Formula $x$
\end_inset

 into a higher-dimensional feature space.
\end_layout

\begin_layout Description
Nearest-Neighbor.
 SVM using the RBF kernel is similar to the K-nearest neighbor classifier.
 The decision boundary for 1-NN algorithm is the union of the Voronoi cells
 of each training instance.
 As for SVM, when RBF kernel is used while there is no regularization, the
 decision boundary will also be an approximation of the union of the Voronoi
 cells.
\end_layout

\begin_layout Description
Reference: https://stats.stackexchange.com/questions/26792/svm-using-rbf-and-neare
st-neighbor-classification-method
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard
The primal formulation is given by 
\begin_inset ERT
status open

\begin_layout Plain Layout

$ L_p(
\backslash
vec a, b, 
\backslash
{z_i
\backslash
}; 
\backslash
{
\backslash
alpha_i, 
\backslash
mu_i
\backslash
}) = (1/2) |
\backslash
vec a|^2 + 
\backslash
gamma 
\backslash
sum _{i=1}^m z_i - 
\backslash
sum _{i=1}^m 
\backslash
alpha _i 
\backslash
{ y_i (
\backslash
vec a 
\backslash
cdot 
\backslash
vec x_i + b) - (1 -z_i)
\backslash
} - 
\backslash
sum _{i=1}^m 
\backslash
mu _i z_i.$
\end_layout

\end_inset

 Explain the meaning of all the terms and variables in this equation.
 What constraints do the variables satisfy?  Calculate the form of the solution
 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
vec a$
\end_layout

\end_inset

 by minimizing 
\begin_inset ERT
status open

\begin_layout Plain Layout

$L_p$
\end_layout

\end_inset

 with respect to 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
vec a$
\end_layout

\end_inset

.
 What are the support vectors? How can the dual formulation be obtained
 by eliminating 
\begin_inset ERT
status open

\begin_layout Plain Layout

$
\backslash
vec a, b, 
\backslash
{z_i
\backslash
}$
\end_layout

\end_inset

 from 
\begin_inset ERT
status open

\begin_layout Plain Layout

$L_p$
\end_layout

\end_inset

.
 How can the primal problem be solved by exploiting the dual formulation?
\end_layout

\begin_layout Description
Notations-Constraints.
 The primal formulation is
\begin_inset Formula 
\[
L_{p}(\vec{a},b,\{z_{i}\};\{\alpha_{i},\mu_{i}\})=(1/2)|\vec{a}|^{2}+\gamma\sum_{i=1}^{m}z_{i}-\sum_{i=1}^{m}\alpha_{i}\{y_{i}(\vec{a}\cdot\vec{x}_{i}+b)-(1-z_{i})\}-\sum_{i=1}^{m}\mu_{i}z_{i}
\]

\end_inset

where the 
\begin_inset Formula $\vec{a}$
\end_inset

 is the normal vector of the classification hyperplane, and 
\begin_inset Formula $|\vec{a}|=1$
\end_inset

; 
\begin_inset Formula $b$
\end_inset

 is the constant offset of the classification hyperplane; so that 
\begin_inset Formula $\vec{x}\cdot\vec{a}$
\end_inset

 is the vertical distance between point 
\begin_inset Formula $\vec{x}$
\end_inset

 and the hyperplane, and 
\begin_inset Formula $\vec{x}\cdot\vec{a}+b=0$
\end_inset

 means the point 
\begin_inset Formula $\vec{x}$
\end_inset

 lies on the classification hyperplane.
 The data is 
\begin_inset Formula $\{(\vec{x}_{i},y_{i}):i=1,\ldots,m\}$
\end_inset

, where 
\begin_inset Formula $y_{i}\in\{-1,1\}$
\end_inset

 and 
\begin_inset Formula $m$
\end_inset

 is the dataset size.
 The 
\begin_inset Formula $\{z_{i}\}$
\end_inset

 are the slack variables and 
\begin_inset Formula $z_{i}\geq0$
\end_inset

.
 The 
\begin_inset Formula $\gamma$
\end_inset

 is a hyper-parameter to be tuned manually.
 The 
\begin_inset Formula $\{\alpha_{i}\}$
\end_inset

 and 
\begin_inset Formula $\{\mu_{i}\}$
\end_inset

 are Lagrange parameters for the Lagrangian multiplier, and 
\begin_inset Formula $\alpha_{i}\geq0$
\end_inset

, 
\begin_inset Formula $\mu_{i}\geq0$
\end_inset

.
\end_layout

\begin_layout Description
Form-of-Solution.
 
\end_layout

\end_body
\end_document
