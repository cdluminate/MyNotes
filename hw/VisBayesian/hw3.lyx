#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{times}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HW #3
\end_layout

\begin_layout Author
Mo Zhou
\begin_inset Newline newline
\end_inset

<mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Exponential Models with Hidden Variables
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard
What is an exponential model with hidden variables? What are the potentials,
 the parameters, and the normalization term Z?.
 What are the derivatives of Z with respect to the parameters of the exponential
 model? How can the EM algorithm be used for doing maximum likelihood estimation
 for an exponential model with hidden variables?
\end_layout

\begin_layout Description
Exponential-Distribution.
 The general form of an exponential distribution is
\begin_inset Formula 
\[
p(\bm{x}|\bm{\lambda})=\frac{1}{Z[\bm{\lambda}]}\exp\{\bm{\lambda}\cdot\bm{\phi}(\bm{x})\}
\]

\end_inset

where 
\begin_inset Formula $Z[\bm{\lambda}]$
\end_inset

 is the normalization factor, 
\begin_inset Formula $\bm{\lambda}=(\lambda_{1},\lambda_{2},\ldots,\lambda_{M})$
\end_inset

 are the parameters and 
\begin_inset Formula 
\[
\bm{\phi}(\bm{x})=(\phi_{1}(\bm{x}),\phi_{2}(\bm{x}),\ldots,\phi_{M}(\bm{x}))
\]

\end_inset

 are the statistics (potential).
 Accordingly, an exponential model w/ hidden variables is
\begin_inset Formula 
\[
p(\bm{d},\bm{h}|\bm{\lambda})=\frac{1}{Z[\bm{\lambda}]}\exp\{\bm{\lambda}\cdot\bm{\phi}(\bm{d},\bm{h})\}
\]

\end_inset

where 
\begin_inset Formula $Z[\bm{\lambda}]=\sum_{\bm{d,h}}\exp\{\bm{\lambda\cdot\phi}(\bm{d,h})\}$
\end_inset

 is the normalization term.
 The parameters are 
\begin_inset Formula 
\[
\bm{\lambda}=(\lambda_{1},\lambda_{2},\ldots,\lambda_{M},\mu_{1},\mu_{2},\ldots,\mu_{M-1}).
\]

\end_inset

The potentials are 
\begin_inset Formula 
\[
\bm{\phi(d,h)}=(\phi(d_{i},h_{i})_{i=1,\ldots,M},\psi(h_{i},h_{i+1})_{i=1,\ldots,M-1})
\]

\end_inset


\end_layout

\begin_layout Description
Derivatives.
 The derivates of 
\begin_inset Formula $Z$
\end_inset

 w.r.t.
 the parameters is
\begin_inset Formula 
\begin{align*}
\frac{\partial}{\partial\bm{\lambda}}\log Z[\bm{\lambda}] & =\frac{\partial}{\partial\bm{\lambda}}\log\sum_{\bm{h,d}}\exp\{\bm{\lambda\cdot\phi}(\bm{h,d})\}\\
 & =\frac{1}{\sum_{\bm{h,d}}\exp\{\bm{\lambda\cdot\phi}(\bm{h,d})\}}\sum_{\bm{h,d}}\bm{\phi}(\bm{h,d})\cdot\exp\{\bm{\lambda\cdot\phi}(\bm{h,d})\}\\
 & =\sum_{\bm{h,d}}p(\bm{h,d}|\bm{\lambda})\cdot\bm{\phi}(\bm{h,d})
\end{align*}

\end_inset


\end_layout

\begin_layout Description
EM-Algorithm-MLE.
 Given the dataset 
\begin_inset Formula $D=\{\bm{d}^{m}:m=1,\ldots,M\}$
\end_inset

, we want to find the MLE
\begin_inset Formula 
\[
\hat{\bm{\lambda}=\arg\max_{\lambda}\prod_{m=1}^{M}p(\bm{d}^{m}|\bm{\lambda})=\arg\max_{\lambda}\prod_{m=1}^{M}\sum_{\bm{h}^{m}}p(\bm{d}^{m},\bm{h}^{m}|\bm{\lambda})}.
\]

\end_inset

To solve this, we introduce distrinution 
\begin_inset Formula $Q_{m}(\bm{h}^{m})$
\end_inset

, and the free energy can be expressed as
\begin_inset Formula 
\[
F[\bm{\lambda}:\{Q_{m}(\bm{h}^{m})\}]=\sum_{m-1}^{M}\Big\{-\log p(\bm{d}^{m}|\bm{\lambda})+\sum_{\bm{h}^{m}}Q_{m}(\bm{h}^{m})\log\frac{Q_{m}(\bm{h}^{m})}{p(\bm{h}^{m}|\bm{d}^{m},\bm{\lambda})}\Big\}
\]

\end_inset

Then the EM algorithm minimizes free energy w.r.t 
\begin_inset Formula $\lambda$
\end_inset

 and the 
\begin_inset Formula $\{Q_{m}(\cdot)\}$
\end_inset

 alternatively:
\begin_inset Formula 
\begin{align*}
\bm{\lambda}^{t+1} & =\arg\min_{\lambda}F[\bm{\lambda}:\{Q_{m}^{t}(\cdot)\}]=\arg\min_{\lambda}\big\{-\sum_{\bm{h}^{m}}Q_{m}^{t}(\bm{h}^{m})\log p(\bm{h}^{m},\bm{d}^{m}|\bm{\lambda})\big\}\\
Q_{m}^{t+1}(\bm{h}^{m}) & =p(\bm{h}^{m}|\bm{d}^{m},\bm{\lambda}^{t})
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard
Now suppose that the exponential model can be expressed as a probability
 distribution on variables defined on a graph.
 How can the EM computations be done if the graph has no closed loops? What
 if the graph has closed loops? Does the EM algorithm attempt to make the
 statistics of the data equal to the expected statistics of the model?
\end_layout

\begin_layout Description
EM-Computations.
 When there is no closed loop in the graph, we can compute
\begin_inset Formula 
\begin{align*}
P(\bm{h}^{m}|\bm{d}^{m},\bm{\lambda}^{t}) & =\frac{P(\bm{h}^{m},\bm{d}^{m}|\bm{\lambda}^{t})}{P(\bm{d}^{m}|\bm{\lambda}^{t})}\\
P(\bm{d}^{m}|\bm{\lambda}^{t}) & =\frac{1}{Z[\bm{\lambda}]}\sum_{\bm{h}^{m}}\exp\{\bm{\lambda}\cdot\bm{\phi}(\bm{d}^{m},\bm{h}^{m})\}
\end{align*}

\end_inset

If the graph has closed loop, it can be approximated by BP (sum-product).
\end_layout

\begin_layout Description
Statics-Equal.
 For the update rule for 
\begin_inset Formula $\bm{\lambda}^{t+1}$
\end_inset

, we can convert the problem into minimizing
\begin_inset Formula 
\[
G(\bm{\lambda})=-\sum_{m=1}^{M}Q_{m}^{t}(\bm{h}^{m})\cdot\bm{\lambda}\cdot\bm{\phi}(\bm{h}^{m},\bm{d}^{m})-\sum_{m=1}^{M}\log Z[\bm{\lambda}].
\]

\end_inset

And the function 
\begin_inset Formula $G(\bm{\lambda})$
\end_inset

 is a convex function of 
\begin_inset Formula $\bm{\lambda}$
\end_inset

, because 
\begin_inset Formula $\log Z[\cdot]$
\end_inset

 is convex.
 Hence, the global minimum 
\begin_inset Formula $\hat{\bm{\lambda}}$
\end_inset

 occurs at the zero-gradient point where 
\begin_inset Formula $\partial G(\bm{\hat{\lambda}})/\partial\bm{\lambda}=0$
\end_inset

, namely
\begin_inset Formula 
\[
\frac{1}{M}\sum_{m=1}^{M}Q_{m}^{t}(\bm{h}^{m})\bm{\phi}(\bm{h}^{m},\bm{d}^{m})=\sum_{\bm{h,d}}\bm{\phi}(\bm{h},\bm{d})P(\bm{h},\bm{d}|\bm{\lambda})
\]

\end_inset

which is exactly when the expected statistics w.r.t.
 data 
\begin_inset Formula $\bm{d}^{m}$
\end_inset

 and 
\begin_inset Formula $Q_{m}(\cdot)$
\end_inset

 equals the expected statistics of the model.
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard
Describe how a Hidden Markov Model (HMM) can be expressed as an exponential
 model with hidden variables? Does this correspond to a graph with closed
 loops? What inference algorithms are used?
\end_layout

\begin_layout Description
HMM.
 Almost every named distribution can be expressed as an exponential distribution.
 So exponential model with hidden variables is a generalization of the Hidden
 Markov Models.
 HMM correspond to a graph without closed loop.
 Inference algorithm is DP (dynamic programming).
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Lighting Models
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard
What is the Lambertian lighting model? What is the albedo? What is the Generaliz
ed Bas Relief (GBR) ambiguity? How does it relate to the ambiguity between
 convex and concave shapes? What are cast and attached shadows? Does the
 GBR hold for cast and attached shadows?
\end_layout

\begin_layout Description
Lambertian-Model.
 The Lambertian reflectance model is the simplest way to model how images
 are generated from three dimensional objects illuminated by various light
 sources.
 The linear Lambertian model is
\begin_inset Formula 
\[
I(\vec{x})=a(\vec{x})\vec{n}(\vec{x})\cdot\vec{s}
\]

\end_inset

where 
\begin_inset Formula $I(\vec{x})$
\end_inset

 is the image, 
\begin_inset Formula $a(\vec{x})$
\end_inset

 is the albedo, 
\begin_inset Formula $\vec{n}(\vec{x})$
\end_inset

 is the surface normal, 
\begin_inset Formula $\vec{s}$
\end_inset

 is the light source.
\end_layout

\begin_layout Description
Albedo.
 It is the measure of the diffuse reflection of solar radiation out of the
 total solar radiation and measured on a scale from 0 to 1.
\end_layout

\begin_layout Description
GBR.
 There is ambiguity in the estimation of shape from multiple images with
 unknown lighting and fixed viewpoint.
 The GBR ambiguity assumes that objects have Lambertian reflectance functions
 but allows for shadows (cast and attached) and multiple light sources (but
 no interflections).
 GBR includes the convex versus concave amgiguity and the bas-relief ambiguity
 as special cases and is of practical importance for photometric stereo.
 Given a surface 
\begin_inset Formula $z(x,y)$
\end_inset

, the surfaces related linearly to 
\begin_inset Formula $z$
\end_inset

 are
\begin_inset Formula 
\begin{align*}
\tilde{z}(x,y) & =ax+by+cz(x,y)\\
G=\begin{bmatrix}1 & 0 & 0\\
0 & 1 & 0\\
a & b & c
\end{bmatrix} & \qquad G^{-1}=\frac{1}{c}\begin{bmatrix}c & 0 & 0\\
0 & c & 0\\
-a & -b & 1
\end{bmatrix}
\end{align*}

\end_inset

They form a sub-group of 
\begin_inset Formula $GL(3)$
\end_inset

.
\end_layout

\begin_layout Description
Convex-Concave.
 There is a well-known preceptual ambiguity – it is impossible to distinguish
 between a convex object lit from above and a concave object lit from below.
 Humans resolve this ambiguity by tending to perceive objects to be convex.
 Thus, the linear model must be modified to account for shadows.
\end_layout

\begin_layout Description
Shadows.
 There are two types of shadows: (i) attached shadows, and (ii) cast shadows
 (where light source is occluded).
 The cast shadows are more challenging.
 GBR holds for cast and attached shadows.
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard
What is photometric stereo? How can it be formulated in terms of Singular
 Value Decomposition? What are the ambiguities?
\end_layout

\begin_layout Description
Photometric-Stereo.
 Given several images of a Lambertian object under varying lighting, assuming
 single directional source, we have 
\begin_inset Formula 
\begin{align*}
M & =LS\\
\begin{bmatrix}I_{11} & \cdots & I_{1p}\\
\vdots &  & \vdots\\
I_{f1} & \cdots & I_{fp}
\end{bmatrix}_{f\times p} & =\begin{bmatrix}l_{1x} & l_{1y} & l_{1z}\\
 & \vdots\\
l_{fx} & l_{fy} & l_{fz}
\end{bmatrix}_{f\times3}\begin{bmatrix}n_{x1} &  & n_{xp}\\
n_{y1} & \cdots & n_{yp}\\
n_{z1} &  & n_{zp}
\end{bmatrix}_{3\times p}
\end{align*}

\end_inset

We can solve for 
\begin_inset Formula $S$
\end_inset

 if 
\begin_inset Formula $L$
\end_inset

 is known (Woodham).
 This algorithm can be extended to more complex reflection models (if known)
 through the use of a lookup table.
\end_layout

\begin_layout Description
Factorization.
 We can use SVD to find a rank 3 approximation for
\begin_inset Formula 
\[
M=U\Sigma V^{T}
\]

\end_inset

So we define 
\begin_inset Formula $\Sigma_{3}=\text{diag}(\sigma_{1},\sigma_{2},\sigma_{3})$
\end_inset

, where 
\begin_inset Formula $\sigma_{1},\sigma_{2},\sigma_{3}$
\end_inset

 are the largest singular values of 
\begin_inset Formula $M$
\end_inset

, and
\begin_inset Formula 
\[
\hat{L}=U\sqrt{\Sigma_{3}},\quad\hat{S}=\sqrt{\Sigma_{3}}V^{T},\quad M\approx\hat{L}\hat{S}
\]

\end_inset


\end_layout

\begin_layout Description
Ambiguity.
 Factorization is not unique, since
\begin_inset Formula 
\[
\hat{M}=(\hat{L}A^{-1})(A\hat{S})
\]

\end_inset

where 
\begin_inset Formula $A$
\end_inset

 is 
\begin_inset Formula $3\times3$
\end_inset

 invertible.
 However, we can reduce such ambiguity by imposing intergrability.
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard
How many bases are required to describe the image of an object if shadows
 are ignored? How can principal component analysis (PCA) be used to estimate
 the number of bases? What theory predicts the number of bases for convex
 objects? And how many bases are predicted?
\end_layout

\begin_layout Description
Shadows-Ignored.
 The linear Lambertian model (i.e.
 ignoring shadows) implies that the image of an object lies in a three-dimension
al space.
 This implies that the image can be modeled as
\begin_inset Formula 
\[
I(\vec{x})=\sum_{i=1}^{3}\alpha_{i}e_{i}(\vec{x})
\]

\end_inset

 Analysis shows that the first five eigenvectors typically contain 90 percent
 of the energy (sum of all the eigenvalues).
\end_layout

\begin_layout Description
PCA.
 The linear model can be investigated empirically by taking photographs
 of an object from different lighting conditions.
 To get a series of images 
\begin_inset Formula $\{I^{\mu}(\vec{x})\}$
\end_inset

.
 We can then compute the correlation matrix
\begin_inset Formula 
\[
K(\vec{x},\vec{x}')=\frac{1}{N}\sum_{\mu=1}^{N}I^{\mu}(\vec{x})I^{\mu}(\vec{x}')
\]

\end_inset

Then calculate the eigenvectors and eigenvalues
\begin_inset Formula 
\[
\sum_{\vec{x}'}K(\vec{x},\vec{x}')e(\vec{x}')=\lambda e(\vec{x})
\]

\end_inset

This analysis shows that the first five eigenvectors typically contain 90
 percent of the energy (sum of all eigenvalues).
 This plots
\begin_inset Formula 
\[
\frac{\sum_{i=1}^{n}\lambda_{i}}{\sum_{i=1}^{N}\lambda_{i}}
\]

\end_inset

as a function of 
\begin_inset Formula $n$
\end_inset

 where 
\begin_inset Formula $N$
\end_inset

 is the total number of eigenvalues.
\end_layout

\begin_layout Description
Theory.
 Independently done in the computer vision community by Basri and Jacobs,
 and in the computer graphics community by Ramamoothi and Hanrahan – showed
 that for convex objects 
\bar under
nine
\bar default
 eigenvectors captured much for the intensity variations, and if the lightingwas
 restricted to come from the frontal hemisphere then only 
\bar under
five
\bar default
 were needed.
 These theoretical studies where done by using spherical harmonics (fourier
 theory on a sphere) and showing that only a limited number were neede.
\end_layout

\begin_layout Section
AdaBoost
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard
What is a weak classifier? What is a strong classifier?
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard
What function does AdaBoost minimize? What is its relationship to the loss
 function for binary classification? How does the update rule correspond
 to weighting missclassified samples more highly? Can a weak classifier
 be selected twice by AdaBoost?
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard
What are cascades? How are they used for face and text detection? What types
 of image features and weak classifiers are used by AdaBoost to perform
 face detection? What types of features are used by AdaBoost to perform
 text detection?
\end_layout

\begin_layout Section
Support Vector Machines
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard
What is the margin of an SVM? How does SVM deal with non-separable data?
 What is the primal formulation of SVM? How does the SVM objective function
 relate to the empirical risk? What term helps prevent over-fitting to the
 training data? What is the hinge loss? How does learning an SVM differ
 from learning Gaussian distributions for the positive and negative data
 examples and then applying the log-likelihood rule?
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard
What is a kernel? How does it relate to feature vectors? What type of kernel
 makes an SVM behave like a nearest neighbor classifier?
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout

The primal formulation is given by $ L_p(
\backslash
vec a, b, 
\backslash
{z_i
\backslash
}; 
\backslash
{
\backslash
alpha_i, 
\backslash
mu_i
\backslash
}) = (1/2) |
\backslash
vec a|^2 + 
\backslash
gamma 
\backslash
sum _{i=1}^m z_i - 
\backslash
sum _{i=1}^m 
\backslash
alpha _i 
\backslash
{ y_i (
\backslash
vec a 
\backslash
cdot 
\backslash
vec x_i + b) - (1 -z_i)
\backslash
} - 
\backslash
sum _{i=1}^m 
\backslash
mu _i z_i.$ Explain the meaning of all the terms and variables in this equation.
 What constraints do the variables satisfy?  Calculate the form of the solution
 $
\backslash
vec a$ by minimizing $L_p$ with respect to $
\backslash
vec a$.
 What are the support vectors? How can the dual formulation be obtained
 by eliminating $
\backslash
vec a, b, 
\backslash
{z_i
\backslash
}$ from $L_p$.
 How can the primal problem be solved by exploiting the dual formulation?
\end_layout

\end_inset


\end_layout

\end_body
\end_document
