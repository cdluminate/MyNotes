#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
%\usepackage{gentium}
\usepackage{times}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Rama Machine Intelligence 2022 Spring Course Notes
\end_layout

\begin_layout Author
M Zhou
\end_layout

\begin_layout Section
Misc and History
\end_layout

\begin_layout Standard
Why Machine Intel instead of Artifical Intel? Coverage: traditional AI.
 The past AI models are interpretable, however deep learning (working like
 blackbox) does not.
 Bias of deep learning models.
\end_layout

\begin_layout Standard
Lessons learned from the brain: memory and simulation are key to decision
 making.
\end_layout

\begin_layout Standard
Definition of AI.
 Narrow AI v.s.
 General AI.
\end_layout

\begin_layout Standard
Issues with current 
\begin_inset Quotes eld
\end_inset

AI
\begin_inset Quotes erd
\end_inset

 approaches: data hungry, black box, generalizability (domain shift), vulnerabil
ity, bias, fairness, privacy.
 May these lead to AI winters?
\end_layout

\begin_layout Standard
LeCun 89 LeNet
\end_layout

\begin_layout Enumerate
COMPUTING MACHINERY AND INTELLIGENCE.
 Alan Turing
\end_layout

\begin_layout Enumerate
Turing’s Imitation Game: a discussion with the benefit of hind-sight
\end_layout

\begin_layout Enumerate
Stuart Russell & Peter Norvig, AI: A Modern Approach
\end_layout

\begin_layout Subsection
What is AI
\end_layout

\begin_layout Standard
Rational decisions.
\end_layout

\begin_layout Enumerate
Rational: maximally achieving pre-defined goals
\end_layout

\begin_layout Enumerate
Rationality only concerns what decisions are made, not the thought process
 behind them
\end_layout

\begin_layout Enumerate
goals are expressed in terms of the utility of outcomes
\end_layout

\begin_layout Enumerate
being rational means maximizing your expected utility
\end_layout

\begin_layout Standard
Brain – good at making rational decisions, but not perfect.
 Memory and simulation are key to decision making.
\end_layout

\begin_layout Standard
Game Agents.
 (deep blue 1997).
 Reinforcement learning.
\end_layout

\begin_layout Standard
simulated agents.
 robotics.
 human-ai interaction.
 tools for predictions and dicisions.
 natural language.
\end_layout

\begin_layout Standard
Definition of AI.
 
\end_layout

\begin_layout Standard
AI history.
\end_layout

\begin_layout Subsection
Directions in AI
\end_layout

\begin_layout Standard
Search: problem solving, game strategies, browsers (search engine)
\end_layout

\begin_layout Standard
Language: natural language processing, language understanding
\end_layout

\begin_layout Standard
Vision: computer vision.
\end_layout

\begin_layout Standard
Expertise: expert systems, recommenders.
\end_layout

\begin_layout Subsection
Connectionism
\end_layout

\begin_layout Standard
Increasingly dominant.
\end_layout

\begin_layout Subsection
Issues with current AI
\end_layout

\begin_layout Enumerate
Data Hungry.
\end_layout

\begin_layout Enumerate
Black box.
\end_layout

\begin_layout Enumerate
Generalizability.
\end_layout

\begin_layout Enumerate
Vulnerability.
\end_layout

\begin_layout Enumerate
Bias, Fairness.
\end_layout

\begin_layout Enumerate
Privacy.
\end_layout

\begin_layout Enumerate
Explainability.
\end_layout

\begin_layout Enumerate
Going Big?
\end_layout

\begin_layout Section
Intelligent Agents
\end_layout

\begin_layout Standard
An agent is a computer system that is capable of independent action on behalf
 of its user or owner (figure out what needs to be done to satisfy design
 objectives, rather than constantly being told).
\end_layout

\begin_layout Standard
A multiagent system is one that consists of a number of agents, which interact
 with one-another.
 TO successfully interact, they will require the ability to cooperate, coordinat
e, and negotiate with each other, much as people do.
\end_layout

\begin_layout Standard
Two key problems: agent design, and society design.
\end_layout

\begin_layout Standard
An 
\series bold
agent
\series default
 should strive to 
\begin_inset Quotes eld
\end_inset

do the right thing
\begin_inset Quotes erd
\end_inset

, based on what it can perceive and the actions it can perform.
 The right action is the one that will casue the agent to be most successful.
 Performance measure in this case is an objective criterion for success
 of an agent's behavior.
\end_layout

\begin_layout Standard
For each possible percept sequence, a 
\series bold
rational agent
\series default
 should select an action that maximizes its performance measure (in expectation)
 given the evidence provided by the percept sequence and whatever build-in
 knowledge the agent has.
\end_layout

\begin_layout Subsection
Agents
\end_layout

\begin_layout Standard
An agent perceives tis environment via sensors and acts upon that environment
 through its actuators.
\end_layout

\begin_layout Subsection
Characterizing Task Environment
\end_layout

\begin_layout Standard
PEAS: performance measure, environment, actuators, sensors.
 This is a qualitative setting.
\end_layout

\begin_layout Subsection
Task environments
\end_layout

\begin_layout Enumerate
fully observable or partially observable
\end_layout

\begin_layout Enumerate
deterministic or stochastic
\end_layout

\begin_layout Enumerate
episodic or sequential
\end_layout

\begin_layout Enumerate
static or dynamic (does it change while the agent is thinking)
\end_layout

\begin_layout Enumerate
discrete or continuous
\end_layout

\begin_layout Enumerate
single agent or multi-agent
\end_layout

\begin_layout Subsection
Agent Types
\end_layout

\begin_layout Description
table-driven-agents use a percept sequence/action table in memory to find
 the enxt action.
 implemented by a large lookup table
\end_layout

\begin_layout Description
simple-refelx-agents based on condition-action rules, implemeneted with
 an appropriate production system.
 they are steateless devices which do not have memory of past world states.
\end_layout

\begin_layout Description
agents-with-memory model-based reflex agents.
 they have internal state, which is used to keep track of past states of
 the world
\end_layout

\begin_layout Description
agents-with-goals are agents that, in addition to state information, have
 goal information that describes desirable situations.
 agents of this kind take future events into consideration
\end_layout

\begin_layout Description
utility-based-agents base their decisions on classis axiomatic utility theory
 in order to act rationally
\end_layout

\begin_layout Description
learning-agents they have the ability to improve performance through learning
\end_layout

\begin_layout Section
Problem Solving by Searching
\end_layout

\begin_layout Subsection
running time of an algorithm
\end_layout

\begin_layout Standard
The big O notation
\end_layout

\begin_layout Subsection
search problem
\end_layout

\begin_layout Standard
A search problem consists of (1) a state space; (2) a successor function;
 (3) a start state and a goal test;
\end_layout

\begin_layout Standard
A solution is a sequence of actions (a plan) which transforms the start
 state to a goal state.
\end_layout

\begin_layout Standard
example: the 8-puzzle 
\begin_inset Quotes eld
\end_inset

sliding tile puzzle
\begin_inset Quotes erd
\end_inset

, TSP, VLSI layout, robot navigation, protein design, etc.
\end_layout

\begin_layout Subsection
state space graph and search tree
\end_layout

\begin_layout Standard
each state only occurs once in graph.
\end_layout

\begin_layout Standard
general tree search algorithm.
 imoprtant aspects: fringe, expansion, exploration strategy.
\end_layout

\begin_layout Standard

\series bold
depth-first search.
 DFS
\series default
.
 strategy: expand a deepest node first.
 implementation: fringe is a LIFO stack.
 branching factor 
\begin_inset Formula $b$
\end_inset

, maximum depth 
\begin_inset Formula $m$
\end_inset

, the number of nodes in the entire tree is 
\begin_inset Formula $O(b^{m})$
\end_inset

.
 It takes time 
\begin_inset Formula $O(b^{m})$
\end_inset

, space of fringe 
\begin_inset Formula $O(bm)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
breadth-first search.
 BFS.
 
\series default
strategy: expand a shallowest node frist.
 implementation: fringe is a FIFO queue.
 shallowest solution depth 
\begin_inset Formula $s$
\end_inset

, it takes time 
\begin_inset Formula $O(b^{s})$
\end_inset

, fringe takes space 
\begin_inset Formula $O(b^{s})$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
iterative deepening search.
 (IDS)
\series default
 is the perferred uniformed search method when there is a large search space
 and the depth of the solution is not known.
 get DFS's space advantage with BFS's time/shallow-solution advantages.
 Run DFS with depth limit 1, if no solution run DFS w/ depth limit 2, and
 so on.
 time 
\begin_inset Formula $O(b^{d})$
\end_inset

, space 
\begin_inset Formula $O(bd)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
uniform cost search (UCS)
\series default
.
 strategy expand a cheapest node first.
 implementation: fringe is a priority queue (priority: cumulative cost).
 This is optimal according to A*.
\end_layout

\begin_layout Standard

\series bold
search heuristic
\series default
: a function that estimates how close a state is to a goal.
 specific to particular search problem.
\end_layout

\begin_layout Standard

\series bold
Greedy best-first search:
\series default
 expand a node that you think is closest to a goal state.
 best-first may take you straght to the wrong goal.
 May easily fall into local optimum.
 evaluates heuristic function at node 
\begin_inset Formula $n$
\end_inset

.
 GBF search expands the node that appears to have shortest path to goal,
 with a hope that these nodes may lead to solution quickly.
 This is similar to DFS, which prefers to follow a single path to goal (guided
 by the heuristic).
 It is not complete (may fall in loops).
 Time 
\begin_inset Formula $O(b^{m})$
\end_inset

, Space 
\begin_inset Formula $O(b^{m})$
\end_inset

.
 not optimal
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

fun tree-search(problem, strategy) returns solution, or failure
\end_layout

\begin_layout Plain Layout

	init search tree using initial state of $problem
\end_layout

\begin_layout Plain Layout

	loop
\end_layout

\begin_layout Plain Layout

		if there are no candidates for expansion; then
\end_layout

\begin_layout Plain Layout

			return failure
\end_layout

\begin_layout Plain Layout

		endif
\end_layout

\begin_layout Plain Layout

		choose a leaf node for expansion according to $strategy
\end_layout

\begin_layout Plain Layout

		if the node contains a goal state; then
\end_layout

\begin_layout Plain Layout

			return corresponding solution
\end_layout

\begin_layout Plain Layout

		else
\end_layout

\begin_layout Plain Layout

			expand node and add resulting nodes to search tree
\end_layout

\begin_layout Plain Layout

		endif
\end_layout

\begin_layout Plain Layout

	end
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
general tree search
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
A* (a star) search
\end_layout

\begin_layout Section
Adversarial search and games
\end_layout

\begin_layout Section
Constraint satisfaction problems
\end_layout

\begin_layout Section
Probablistic reasoning
\end_layout

\begin_layout Section
Dynamic bayesian networks and associated inference algorithms
\end_layout

\begin_layout Section
Optimal sequential decisions
\end_layout

\begin_layout Section
Markov decision processes
\end_layout

\begin_layout Section
Partially observed Markov decision processes
\end_layout

\begin_layout Section
Deep learning
\end_layout

\begin_layout Section
GANs
\end_layout

\begin_layout Section
Robustness to adversarial attacks
\end_layout

\begin_layout Section
Deep fakes
\end_layout

\begin_layout Section
Fair and ethical AI
\end_layout

\begin_layout Section
Safety of AI systems and applications in engineering and medicine
\end_layout

\end_body
\end_document
