#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{microtype}
\usepackage[margin=0.7in]{geometry}
%\usepackage{gentium}
\usepackage{times}
\usepackage{indentfirst}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Rama | Machine Intelligence 2022 Spring Course Notes
\end_layout

\begin_layout Author
M Zhou
\end_layout

\begin_layout Section
History of AI
\end_layout

\begin_layout Standard

\series bold
Rational Decision:
\series default
 maximizing expected utility.
\end_layout

\begin_layout Standard

\series bold
Brain:
\series default
 memory and simulation are key to decision making.
\end_layout

\begin_layout Standard

\series bold
Directions in AI:
\series default
 Search, Language, Vision, Expertise (expert system / recommender).
\end_layout

\begin_layout Standard

\series bold
Definition of AI: 
\series default
Narrow AI v.s.
 General AI.
\end_layout

\begin_layout Standard

\series bold
Issues with current AI:
\series default
 data hungry, black box, generalizability (domain shift), vulnerability,
 bias, fairness, privacy.
 Explainability.
 Going Big?
\end_layout

\begin_layout Section
Intelligent Agents & Environment
\end_layout

\begin_layout Standard
An agent is a computer system that is capable of independent action on behalf
 of its user or owner.
\end_layout

\begin_layout Standard
A multiagent system is one that consists of a number of agents, which interact
 with one-another.
 
\end_layout

\begin_layout Standard
Two key problems: agent design, and society design.
\end_layout

\begin_layout Standard
Performance measure: objective criterion for success of an agent's behavior.
\end_layout

\begin_layout Standard
Rational agent maximizes its expected performance measure for each possible
 percept sequence.
 
\end_layout

\begin_layout Standard
PEAS: Performance measure, Environment, Actuators, Sensors.
\end_layout

\begin_layout Standard
Environment: fully observable or partially observable, deterministic or
 stochastic, episodic or sequential, static or dynamic (does it change while
 the agent is thinking), discrete or continuous, single agent or multi-agent.
\end_layout

\begin_layout Standard
Agent Types: table-lookup driven agents, simple reflex agents, model based
 reflex agents (has memory), goal-based agents (e.g.
 solving & search), utility-based agents (rational action), learning agents.
\end_layout

\begin_layout Section
Problem Solving by Searching
\end_layout

\begin_layout Standard
The big O notation.
\end_layout

\begin_layout Standard
Search problem: (1) a state space; (2) a successor function; (3) a start
 state and a goal test;
\end_layout

\begin_layout Standard
A solution is a sequence of actions (a plan) which transforms the start
 state to a goal state.
\end_layout

\begin_layout Standard
General tree search algorithm: fringe (partial plans), expansion, exploration
 strategy.
\end_layout

\begin_layout Standard

\series bold
Depth-First Search (DFS
\series default
).
 expand a deepest node first.
 implementation: fringe is a LIFO stack.
 branching factor 
\begin_inset Formula $b$
\end_inset

, maximum depth 
\begin_inset Formula $m$
\end_inset

, the number of nodes in the entire tree is 
\begin_inset Formula $O(b^{m})$
\end_inset

.
 It takes time 
\begin_inset Formula $O(b^{m})$
\end_inset

, space of fringe 
\begin_inset Formula $O(bm)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Breadth-First Search (BFS).
 
\series default
expand a shallowest node frist.
 implementation: fringe is a FIFO queue.
 shallowest solution depth 
\begin_inset Formula $s$
\end_inset

, it takes time 
\begin_inset Formula $O(b^{s})$
\end_inset

, fringe takes space 
\begin_inset Formula $O(b^{s})$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Iterative Deepening search.
 (IDS)
\series default
 is the perferred uniformed search method when there is a large search space
 and the depth of the solution is not known.
 get DFS's space advantage with BFS's time/shallow-solution advantages.
 Run DFS with depth limit 1, if no solution run DFS w/ depth limit 2, and
 so on.
 time 
\begin_inset Formula $O(b^{d})$
\end_inset

, space 
\begin_inset Formula $O(bd)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Uniform Cost Search (UCS)
\series default
.
 strategy expand a cheapest node first (cost contour).
 implementation: fringe is a priority queue (priority: cumulative cost).
 This is optimal according to A*.
 This is called Dijkstra's algorithm by CS folks.
\end_layout

\begin_layout Standard

\series bold
Search Heuristic
\series default
: a function that estimates how close a state is to a goal.
 specific to particular search problem.
\end_layout

\begin_layout Standard

\series bold
Greedy Best-First search:
\series default
 expand a node that you think is closest to a goal state.
 best-first may take you straght to the wrong goal.
 May easily fall into local optimum.
 evaluates heuristic function at node 
\begin_inset Formula $n$
\end_inset

.
 GBF search expands the node that appears to have shortest path to goal,
 with a hope that these nodes may lead to solution quickly.
 This is similar to DFS, which prefers to follow a single path to goal (guided
 by the heuristic).
 It is not complete (may fall in loops).
 Time 
\begin_inset Formula $O(b^{m})$
\end_inset

, Space 
\begin_inset Formula $O(b^{m})$
\end_inset

.
 not optimal
\end_layout

\begin_layout Standard

\series bold
A* Search.

\series default
 idea: avoid expanding paths that are already expensive.
 Specifically, evluate function 
\begin_inset Formula $f(n)=g(n)+h(n)$
\end_inset

 at node 
\begin_inset Formula $n$
\end_inset

, where 
\begin_inset Formula $g(n)$
\end_inset

 is the cumulative cost to reach 
\begin_inset Formula $n$
\end_inset

, 
\begin_inset Formula $h(n)$
\end_inset

 is the heuristic (estimated cost) from 
\begin_inset Formula $n$
\end_inset

 to goal, 
\begin_inset Formula $f(\cdot)$
\end_inset

 is the total cost.
 It is combination of UCS (backward cost 
\begin_inset Formula $g(n)$
\end_inset

) and Greedy (forward cost 
\begin_inset Formula $h(n)$
\end_inset

).
 Under some reasonable conditions for the heuristics, A* is complete and
 optimal.
\end_layout

\begin_layout Standard
Heuristics admissibility: 
\begin_inset Formula $h(n)\leq h^{*}(n)$
\end_inset

, the heuristics is always less or equal to the true cost 
\begin_inset Formula $h^{*}(n)$
\end_inset

.
\end_layout

\begin_layout Standard
Heuristic consistency: 
\begin_inset Formula $f(n)$
\end_inset

 non-decreasing along any path.
 this is a stronger condition than admissibility.
 A consistent heuristic is also admissible.
\end_layout

\begin_layout Standard
We can use weighted A* search with 
\begin_inset Formula $f(n)=g(n)+wh(n)$
\end_inset

 where 
\begin_inset Formula $1<w<\infty$
\end_inset

.
 A* degenerates into uniform cost with 
\begin_inset Formula $w=0$
\end_inset

, or greedy best-first with 
\begin_inset Formula $w\rightarrow\infty$
\end_inset

.
\end_layout

\begin_layout Standard
Other variations of A*: (1) beam search.
 keeps only the k nodes with the best f-scores discarding any other expanded
 nodes.
 (2) iterative deepening A* search similar to iterative deepening search
 to DFS.
 (3) recursive best-first search.
 (4) branch and bound.
\end_layout

\begin_layout Standard
Most of the work in solving hard search problems optimally is in coming
 up with admissible heuristics.
\end_layout

\begin_layout Standard

\series bold
Summary:
\series default
 A* uses both backward costs and (esimates of) forward costs.
 A* is optimal with admissible or consistent heuristics.
 Heuristic design is the key: ofen use relaxed problems.
\end_layout

\begin_layout Section
Adversarial Search and Games
\end_layout

\begin_layout Standard
Local Search: select random initial state; make local modification to improve
 current state; repeat until goal.
\end_layout

\begin_layout Standard
Hill-Climbing Search (Greedy Local Search): move to best neighbor until
 no better neighbor.
 Can stuck at local optimum.
\end_layout

\begin_layout Standard
Simulated Annealing: Hill-Climbing, but occasionally take a step in reverse
 direction.
 As time passes, the probability to take a down-hill step is gradually reduced.
\end_layout

\begin_layout Standard
Local Beam Search: keep 
\begin_inset Formula $k$
\end_inset

 random states and share best successors among them.
\end_layout

\begin_layout Standard
Genetic Algorithm: maintain a population of candidate solutions, and make
 it evolve by a set of stochastic operators.
\end_layout

\begin_layout Standard
Local Search in Continuous Space: (1) discretize (2) stochastic descent
 (3) small step along gradient.
\end_layout

\begin_layout Standard
Determistic Games: states, players, actions, transition functions, terminal
 test, terminal utility.
 Solution is a policy.
\end_layout

\begin_layout Standard
Minimax algorithm (impl.
 DFS): look first at bottom tree, label bottom-most boards.
 Then label boards one level up, according to result of best possible move.
 Two important factors: deep look ahead, and good heuristic function.
\end_layout

\begin_layout Standard
Adversarial Search (Minimax): state-space search tree.
 Players alternate turns.
 minimax value for each node: best utility against a rational adversary.
\end_layout

\begin_layout Standard
Alpha-Beta Pruning.
 
\begin_inset Formula $\alpha$
\end_inset

 best value for MAX, 
\begin_inset Formula $\beta$
\end_inset

 the best value for MIN.
 Both values are updated, and nodes with worse values will be pruned.
\end_layout

\begin_layout Standard
Evaluation function scores non-terminals in depth-limited search.
 Ideal function: returns the actual minimax value.
\end_layout

\begin_layout Standard
Expectimax Search: compute the average score under optimal play for the
 MIN nodes.
 Max nodes remains the same as in Minimax search.
\end_layout

\begin_layout Standard
Utility: function from outcomes to real numbers describing agents' preferences.
\end_layout

\begin_layout Section
Constraint Satisfaction Problems
\end_layout

\begin_layout Standard
CSP is a special subset of search problems.
 Goal test is a set of constraints specifying combinations for variables.
 (Prunes the search space with constraints)
\end_layout

\begin_layout Standard
Backtrack search: backtrack to previous var setting, and try next possible
 setting.
 If none exists, move back to another level.
 One variable at a time, check constraints as you go.
 This is a variant of DFS.
\end_layout

\begin_layout Standard
CSP Formulation: initial state (empty), successor function, goal test.
\end_layout

\begin_layout Standard
Forward checking.
 keep track of remaining legal values for unassigned variables.
\end_layout

\begin_layout Standard
Arc consistency detects failure earlier than forward checking.
\end_layout

\begin_layout Section
Probablistic Reasoning
\end_layout

\begin_layout Standard
Probablistic inference: compute a desired probability from other known probabili
ties.
 Example: posterior marginal probability, most likely explanation.
\end_layout

\begin_layout Standard
Product rule: 
\begin_inset Formula $P(y)P(x|y)=P(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard
Chain rule: split a multi-variable joint probability into a chain of conditional
 probabilities.
\end_layout

\begin_layout Standard
Bayes rule: two ways of product rule for the same joint probability.
\end_layout

\begin_layout Standard
Independence: 
\begin_inset Formula $P(x)P(y)=P(x,y)$
\end_inset

.
\end_layout

\begin_layout Standard
Bayes Net: describe complex joint distributions using simple and local distribut
ions.
 A directed acyclic graph.
\end_layout

\begin_layout Standard
Construct Bayesian Net: (1) order variables using causality; (2) use these
 assumptions to create graph structure.
\end_layout

\begin_layout Standard
Inference by enumeration in Bayes' Net: computationally efficient.
\end_layout

\begin_layout Section
Dynamic bayesian networks and associated inference algorithms
\end_layout

\begin_layout Standard
Markov Models (chain/process): transition model.
 Stationary assumption: transition probability is constant.
 Markov assumption: future is independent of the past given the present.
\end_layout

\begin_layout Standard
Hidden Markov Models: the true state is not observed.
\end_layout

\begin_layout Standard
Filtering: 
\begin_inset Formula $P(X_{t}|e_{1:t})$
\end_inset

.
 Prediction, Smoothing, most likely explanation (Viterbi).
\end_layout

\begin_layout Standard
Learning HMMs: EM Algorithm.
\end_layout

\begin_layout Section
Optimal Sequential Decisions
\end_layout

\begin_layout Standard
Markov Decision Process: set 
\begin_inset Formula $S$
\end_inset

 of states, set 
\begin_inset Formula $A$
\end_inset

 of actions.
 Initial state, transition model, reward function.
 Result undertain during the search process.
\end_layout

\begin_layout Standard
Value Iteration: algorithm to find optimal policy and value for MDP.
 Select the action with maximum expected utility.
 Start with random utility on each state.
 Repeat simultaneously for every state untill the utility converges.
 Convergence depends on the biggest component of values.
\end_layout

\begin_layout Standard
Policy iteration: compute utility by executing the currrent policy, repeat
 untill no policy change.
\end_layout

\begin_layout Standard
Partially observable MDP: belief state (where the agent itself is).
 Updated by computing the conditional probability dist over all states given
 the sequence of observations and actions so far.
\end_layout

\begin_layout Section
Markov decision processes
\end_layout

\begin_layout Section
Partially observed Markov decision processes
\end_layout

\begin_layout Section
Deep learning
\end_layout

\begin_layout Section
GANs
\end_layout

\begin_layout Section
Robustness to adversarial attacks
\end_layout

\begin_layout Section
Deep fakes
\end_layout

\begin_layout Section
Fair and ethical AI
\end_layout

\begin_layout Section
Safety of AI systems and applications in engineering and medicine
\end_layout

\end_body
\end_document
