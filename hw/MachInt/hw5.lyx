#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{times}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HW #5
\end_layout

\begin_layout Author
Mo Zhou
\begin_inset Newline newline
\end_inset

<mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Discuss the mode collapse issue in GANs and propose a solution to mitigate
 this problem.
\end_layout

\begin_layout Description
Mode-Collapse.
 Mode collapse is the most severe form of non-convergence issue in GANs.
 It means
\begin_inset Formula 
\[
\min_{G}\max_{D}V(G,D)\neq\max_{D}\min_{G}V(G,D),
\]

\end_inset

where GAN converges to correct distribution when 
\begin_inset Formula $D$
\end_inset

 is in inner loop; while GAN places all mass on most likely point when 
\begin_inset Formula $G$
\end_inset

 is in inner loop.
 Besides, other GAN losses also yield mode collapse, and Reverse KL loss
 perfers to fit as many modes as the model can represent and no more â€“ it
 does not prefer fewer modes in general.
 In practice, GANs often seem to collapse to far fewer modes than the model
 can represent.
 As a result, mode collpase causes low output diversity.
\end_layout

\begin_layout Description
Solution.
 To mitigate mode collapse, we need to prevent the generator from broadening
 its scope by preventing it from optimizing for a single fixed discriminator.
 Possible solutions include:
\end_layout

\begin_deeper
\begin_layout Description
(1) Wasserstein loss.
 It alleviates mode collapse training the discriminator to optimality without
 worrying about vanishing gradients.
 If the discriminator does not get stuck in local minima, it learns to reject
 the outputs that the generator stabilizes on.
 So the generator has to output differently.
\end_layout

\begin_layout Description
(2) Unrolled GANs.
 We back-propagate through k updates of the discriminator to prevent mode
 collapse.
 Namely, we use a generator loss function that incorporates not only the
 current discriminator's classifications, but also the outputs of future
 discriminator versions.
 So the generator can't over-optimize for a single discriminator.
\end_layout

\end_deeper
\begin_layout Description
Reference.
 https://developers.google.com/machine-learning/gan/problems
\end_layout

\begin_layout Section
Consider a Generative Adversarial Network (GAN) which successfully produces
 images of apples.
 Which of the following propositions is 
\series bold
false
\series default
?
\end_layout

\begin_layout Enumerate
(i) The generator aims to learn the distribution of apple images.
 
\end_layout

\begin_layout Enumerate
(ii) The discriminator can be used to classify images as apple vs.
 non-apple.
 
\end_layout

\begin_layout Enumerate
(iii) After training the GAN, the discriminator loss eventually reaches
 a constant value.
 
\end_layout

\begin_layout Enumerate
(iv) The generator can produce unseen images of apples.
\end_layout

\begin_layout Standard

\bar under
Statement (i) is not necessarily false.

\bar default
 If the training data is a set of apple images, then the generator is expected
 to learn the distribution of apple images.
 But if the training data contains fruits not limited to apple (e.g., banana),
 then the generator is expected to learn a more general distribution of
 fruit images or even natural images.
\end_layout

\begin_layout Standard

\bar under
Statement (ii) is false.

\bar default
 The discriminator 
\begin_inset Formula $D$
\end_inset

 tells whether the given image is generated by generator 
\begin_inset Formula $G$
\end_inset

, regardless of the actual class of the image.
\end_layout

\begin_layout Standard

\bar under
Statement (iii) is not neccesarily true and it depends.

\bar default
 If the GAN suffers from non-convergence issue, the discriminator loss is
 not converging to a constant.
 On the contrary, if the GAN successfully and ideally converges, it will
 reach the saddle point in the adversarial game between 
\begin_inset Formula $D$
\end_inset

 and 
\begin_inset Formula $G$
\end_inset

.
 The saddle point will result in a constant discriminator loss.
\end_layout

\begin_layout Standard

\bar under
Statement (iv) is true.

\bar default
 The generator being able to generate apple images means it learned the
 distribution of apple images.
 Sampling different unseen apple images through different latent vectors
 is possible.
\end_layout

\begin_layout Section
Which of the following is a non-iterative method to generate adversarial
 examples? 
\end_layout

\begin_layout Enumerate
(i) Non-Saturating Cost Method
\end_layout

\begin_layout Enumerate
(ii) Input Optimization Method 
\end_layout

\begin_layout Enumerate
(iii) Adversarial Training 
\end_layout

\begin_layout Enumerate
(iv) Logit Pairing 
\end_layout

\begin_layout Enumerate
(v) Fast Gradient Sign Method
\end_layout

\begin_layout Enumerate
(vi) Real-time Cryptographic Dropout Method
\end_layout

\begin_layout Standard

\bar under
The answer is (v) FGSM.

\bar default
 FGSM is named 
\begin_inset Quotes eld
\end_inset

Fast
\begin_inset Quotes erd
\end_inset

 Gradient Sign Method because it's a single-step method.
 FGSM is much faster than its iterative variants including BIM, PGD, MIM,
 and APGD.
 However, FGSM cannot better exploit the landscape of the loss function
 to more effectively compromise the neural network.
\end_layout

\begin_layout Section
Prove that Class Activation Map method is a special case of GRAD-CAM.
\end_layout

\begin_layout Standard

\bar under
This question corresponds to the section 3.1 in paper 
\begin_inset Quotes eld
\end_inset

Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Standard
For a fully-convolutional architecture, CAM is a special ca se of Grad-CAM.
\end_layout

\begin_layout Section
Analyze the performance vs explainability tradeoffs for the following machine
 learning/AI algorithms.
\end_layout

\begin_layout Subsection
a.
 Bayes nets
\end_layout

\begin_layout Subsection
b.
 Deep learning networks
\end_layout

\begin_layout Section
What is the best way to combine the performance of deep learning methods
 and the interpretability of older methods?
\end_layout

\end_body
\end_document
