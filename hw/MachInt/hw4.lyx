#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{times}
\usepackage{microtype}
\usepackage[margin=1in]{geometry}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "basicstyle={\ttfamily},frame=shadowbox"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
EN 520.650 HW#4
\end_layout

\begin_layout Author
Mo Zhou
\begin_inset Newline newline
\end_inset

<mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Undiscounted MDP
\end_layout

\begin_layout Standard

\bar under
Consider an undiscounted MDP having three states, (1, 2, 3), with rewards −1, −2
, 0, respectively.
 State 3 is a terminal state.
 In states 1 and 2 there are two possible actions: a and b.
 The transition model is as follows:
\end_layout

\begin_layout Standard

\bar under
- In state 1, action a moves the agent to state 2 with probability 0.6 and
 makes the agent stay put with probability 0.4.
\end_layout

\begin_layout Standard

\bar under
- In state 2, action a moves the agent to state 1 with probability 0.6 and
 makes the agent stay put with probability 0.4.
\end_layout

\begin_layout Standard

\bar under
- In either state 1 or state 2, action b moves the agent to state 3 with
 probability 0.2 and makes the agent stay put with probability 0.8.
\end_layout

\begin_layout Subsection
Q1
\end_layout

\begin_layout Standard

\bar under
What can be determined qualitatively about the optimal policy in states
 1 and 2?
\end_layout

\begin_layout Standard
We have three states 
\begin_inset Formula $s_{1},s_{2},s_{3}$
\end_inset

, each with a reward 
\begin_inset Formula $r_{1}=-1,r_{2}=-2,r_{3}=0$
\end_inset

 respectively.
 There are two actions 
\begin_inset Formula $A\in\{a,b\}$
\end_inset

 for the policy 
\begin_inset Formula $\pi$
\end_inset

.
 The MDP is discounted, which means 
\begin_inset Formula $\gamma=1$
\end_inset

, and we will omit this discount constant for convenience.
 Since 
\begin_inset Formula 
\[
\pi(s)=\arg\max_{A\in\{a,b\}}\sum_{s'}P(s'|s,A)U(s'),
\]

\end_inset

we have
\begin_inset Formula 
\begin{align*}
\pi(s_{1}) & =\arg\max_{A\in\{a,b\}}\begin{cases}
0.4U(s_{1})+0.6U(s_{2}) & A=a\\
0.8U(s_{1})+0.2U(s_{3}) & A=b
\end{cases}\\
\pi(s_{2}) & =\arg\max_{A\in\{a,b\}}\begin{cases}
0.6U(s_{1})+0.4U(s_{2}) & A=a\\
0.8U(s_{2})+0.2U(s_{3}) & A=b
\end{cases}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Then we find utilites according to Value Iteration algorithm
\begin_inset Formula 
\[
U^{(t+1)}(s)=R(s)+\gamma\max_{A\in\{a,b\}}\sum_{s'}P(s'|s,A)U^{(t)}(s').
\]

\end_inset

So we have a zero unitily initilization, and the first step of VI is
\begin_inset Formula 
\begin{align*}
U^{(1)}(s_{1}) & =-1+\max\begin{cases}
0.4U^{(0)}(s_{1})+0.6U^{(0)}(s_{2})=0 & A=a\\
0.8U^{(0)}(s_{1})+0.2U^{(0)}(s_{3})=0 & A=b
\end{cases}=-1\\
U^{(1)}(s_{2}) & =-2+\max\begin{cases}
0.6U^{(0)}(s_{1})+0.4U^{(0)}(s_{2})=0 & A=a\\
0.8U^{(0)}(s_{2})+0.2U^{(0)}(s_{3})=0 & A=b
\end{cases}=-2\\
U^{(1)}(s_{3}) & =0
\end{align*}

\end_inset

Then the second step of VI is
\begin_inset Formula 
\begin{align*}
U^{(2)}(s_{1}) & =-1+\max\begin{cases}
0.4U^{(1)}(s_{1})+0.6U^{(1)}(s_{2})=-0.4-1.2=-1.6 & A=a\\
0.8U^{(1)}(s_{1})+0.2U^{(1)}(s_{3})=-0.8 & A=b
\end{cases}=-1.8\\
U^{(2)}(s_{2}) & =-2+\max\begin{cases}
0.6U^{(1)}(s_{1})+0.4U^{(1)}(s_{2})=-0.6-0.8=-1.4 & A=a\\
0.8U^{(1)}(s_{2})+0.2U^{(1)}(s_{3})=-1.6 & A=b
\end{cases}=-3.4\\
U^{(2)}(s_{3}) & =0
\end{align*}

\end_inset

The third step of VI is ....
 OK, I give up manual computation.
 Let's write some Julia code and compute it:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

uk = [0.,0.,0.]
\end_layout

\begin_layout Plain Layout

println("Initial U", uk);
\end_layout

\begin_layout Plain Layout

for iter = 1:100
\end_layout

\begin_layout Plain Layout

    uk1 = -1 + max(0.4*uk[1] + 0.6*uk[2],
\end_layout

\begin_layout Plain Layout

                   0.8*uk[1] + 0.2*uk[3])
\end_layout

\begin_layout Plain Layout

    uk2 = -2 + max(0.6*uk[1] + 0.4*uk[2],
\end_layout

\begin_layout Plain Layout

                   0.8*uk[2] + 0.2*uk[3])
\end_layout

\begin_layout Plain Layout

    uk3 = 0
\end_layout

\begin_layout Plain Layout

    uk[:] = [uk1, uk2, uk3]
\end_layout

\begin_layout Plain Layout

    println(iter, uk)
\end_layout

\begin_layout Plain Layout

end
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The result suggests that the utility is converging after 100 iterations:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

Initial U[0.0, 0.0, 0.0]
\end_layout

\begin_layout Plain Layout

1[-1.0, -2.0, 0.0]
\end_layout

\begin_layout Plain Layout

2[-1.8, -3.4, 0.0]
\end_layout

\begin_layout Plain Layout

3[-2.4400000000000004, -4.44, 0.0]
\end_layout

\begin_layout Plain Layout

4[-2.9520000000000004, -5.24, 0.0]
\end_layout

\begin_layout Plain Layout

...
\end_layout

\begin_layout Plain Layout

96[-4.999999997513385, -8.33333332960341, 0.0]
\end_layout

\begin_layout Plain Layout

97[-4.999999998010708, -8.333333330349394, 0.0]
\end_layout

\begin_layout Plain Layout

98[-4.999999998408566, -8.333333330946182, 0.0]
\end_layout

\begin_layout Plain Layout

99[-4.999999998726853, -8.333333331423614, 0.0]
\end_layout

\begin_layout Plain Layout

100[-4.999999998981483, -8.333333331805557, 0.0]
\end_layout

\end_inset


\end_layout

\begin_layout Standard
That means 
\begin_inset Formula $U(s_{1})\approx-5.0,U(s_{2})\approx-8.3,U(s_{3})\approx0$
\end_inset

.
 We can plug these values back into
\begin_inset Formula 
\begin{align*}
\pi(s_{1}) & =\arg\max_{A\in\{a,b\}}\begin{cases}
0.4U(s_{1})+0.6U(s_{2})=-6.98 & A=a\\
0.8U(s_{1})+0.2U(s_{3})=-4.0 & A=b
\end{cases}=b\\
\pi(s_{2}) & =\arg\max_{A\in\{a,b\}}\begin{cases}
0.6U(s_{1})+0.4U(s_{2})=-6.32 & A=a\\
0.8U(s_{2})+0.2U(s_{3})=-6.64 & A=b
\end{cases}=a
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Namely, the optimal policy for the two states are 
\begin_inset Formula $\pi^{*}(s_{1})=b$
\end_inset

, and 
\begin_inset Formula $\pi^{*}(s_{2})=a$
\end_inset

.
 This makes sense, because 
\begin_inset Formula $s_{1}$
\end_inset

 has less penalty than 
\begin_inset Formula $s_{2}$
\end_inset

, so once we cannot reach the terminal state, we should stay at 
\begin_inset Formula $s_{1}$
\end_inset

 as long as possible.
 Thus, when we start from 
\begin_inset Formula $s_{1}$
\end_inset

, we tend to choose action b, either stay or teminate.
 If we start from 
\series bold

\begin_inset Formula $s_{2}$
\end_inset


\series default
, we tend to take action a and move to 
\begin_inset Formula $s_{1}$
\end_inset

 to avoid great penalty once we cannot terminate this process quickly.
\end_layout

\begin_layout Subsection
Q2
\end_layout

\begin_layout Standard

\bar under
Apply policy iteration, showing each step in full, to determine the optimal
 policy and the values of states 1 and 2.
 Assume that the initial policy has action b in both states.
\end_layout

\begin_layout Standard
In Policy Iteration algorithm, we initialize 
\begin_inset Formula $\pi_{0}(s_{1})=b$
\end_inset

, and 
\begin_inset Formula $\pi_{0}(s_{2})=b$
\end_inset

.
 Initial 
\begin_inset Formula $U^{0}=[0,0,0]$
\end_inset

.
\end_layout

\begin_layout Standard
At step 1, we have new utilities
\begin_inset Formula 
\begin{align*}
U^{1}(s_{1}) & =-1+[0,0.8,0.2][0,0,0]^{T}=-1\\
U^{1}(s_{2}) & =-2+[0.8,0,0.2][0,0,0]^{T}=-2\\
U^{1}(s_{3}) & =0
\end{align*}

\end_inset

And the new MEU
\begin_inset Formula 
\begin{align*}
\pi_{1}(s_{1}) & =\arg\max_{A\in\{a,b\}}\begin{cases}
[0.4,0.6,0][-1,-2,0]^{T}=-1.6 & A=a\\{}
[0.8,0,0.2][-1,-2,0]^{T}=-0.8 & A=b
\end{cases}=b\\
\pi_{1}(s_{2}) & =\arg\max_{A\in\{a,b\}}\begin{cases}
[0.6,0.4,0][-1,-2,0]^{T}=-1.4 & A=a\\{}
[0,0.8,0.2][-1,-2,0]^{T}=-1.6 & A=b
\end{cases}=a
\end{align*}

\end_inset

We continue with step 2 since there was MEU change.
\begin_inset Formula 
\begin{align*}
U^{2}(s_{1}) & =-1+[0.8,0,0.2][-1,-2,0]^{T}=-1-0.8=-1.8\\
U^{2}(s_{2}) & =-2+[0.6,0.4,0][-1,-2,0]^{T}=-2-1.4=-3.4\\
U^{2}(s_{3}) & =0
\end{align*}

\end_inset

And
\begin_inset Formula 
\begin{align*}
\pi_{2}(s_{1}) & =\arg\max\begin{cases}
[0.4,0.6,0][-1.8,-3.4,0]^{T}=-2.76 & A=a\\{}
[0.8,0,0.2][-1.8,-3.4,0]^{T}=-1.44 & A=b
\end{cases}=b\\
\pi_{2}(s_{2}) & =\arg\max\begin{cases}
[0.6,0.4,0][-1.8,-3.4,0]^{T}=-2.44 & A=a\\{}
[0,0.8,0.2][-1.8,-3.4,0]^{T}=-2.72 & A=b
\end{cases}=a
\end{align*}

\end_inset

Now NEU does not change.
 PI Algorithm stops here.
 We get 
\begin_inset Formula $\pi^{*}(s_{1})=b,\pi^{*}(s_{2})=a$
\end_inset

.
\end_layout

\begin_layout Subsection
Q3
\end_layout

\begin_layout Standard

\bar under
What happens to policy iteration if the initial policy has action a in both
 states? Does discounting help? Does the optimal policy depend on the discount
 factor?
\end_layout

\begin_layout Section
Object Detectors
\end_layout

\begin_layout Standard

\bar under
Compare and contrast the various CNN-based object detectors using the following
 metrics: number of parameters, training and test times, performance on
 ImageNet dataset.
 (4 detectors are sufficient)
\end_layout

\begin_layout Section
Dropout & Network Architecture Search
\end_layout

\begin_layout Standard

\bar under
Discuss methods for dropout and searching for the best neural architecture.
 (searching for the optimal network path in the process of dropout and compare
 the advantages and disadvantages of the architectures in multiple neural
 networks.)
\end_layout

\begin_layout Section
Back-Propagation
\end_layout

\begin_layout Standard

\bar under
For the neural network shown below with initial weights indicated in the
 figure, calculate the next values of w7 and w3 assuming a single input
 i1=0.05, i2=0.10 with corresponding outputs o1=0.01 and o2=0.99.
 Hint: Follow the calculations done in class on 10/06/2021!
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
centering
\end_layout

\end_inset


\begin_inset Graphics
	filename h4p1.png
	special width=0.5\linewidth

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Figure for Question 4.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_body
\end_document
