#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{microtype}
\usepackage{times}
\usepackage[margin=1in]{geometry}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
EN 520.650 Machine Intelligence Final
\end_layout

\begin_layout Author
Mo Zhou
\begin_inset Newline newline
\end_inset

mzhou32@jhu.edu
\end_layout

\begin_layout Section
Q1
\end_layout

\begin_layout Section
Q2
\end_layout

\begin_layout Subsection
(a)
\end_layout

\begin_layout Standard
Assume input 
\begin_inset Formula $x$
\end_inset

 (vector), weight 
\begin_inset Formula $W_{l}$
\end_inset

 (matrix) and bias 
\begin_inset Formula $b_{l}$
\end_inset

 (vector) for the 
\begin_inset Formula $l$
\end_inset

-th layer, and activation function 
\begin_inset Formula $\sigma(\cdot)$
\end_inset

.
 We denote the pre-activation layer output as 
\begin_inset Formula $y_{l}$
\end_inset

 (vector), and the post-activation layer output as 
\begin_inset Formula $z_{l}$
\end_inset

, namely 
\begin_inset Formula $z_{l}=\sigma(y_{l})$
\end_inset

.
 There are in total 
\begin_inset Formula $L$
\end_inset

 layers, and index 
\begin_inset Formula $l\in\{1,\ldots,L\}$
\end_inset

.
\end_layout

\begin_layout Standard
Then, the forward pass of the network can be described asz
\begin_inset Formula 
\begin{align*}
y_{1} & =W_{1}x+b_{1}\\
z_{1} & =\sigma(y_{1})\\
y_{2} & =W_{2}z_{1}+b_{2}\\
z_{2} & =\sigma(y_{2})\\
 & \ldots\\
y_{l} & =W_{l}z_{l-1}+b_{l}\\
z_{l} & =\sigma(y_{l})\\
 & \ldots\\
y_{L} & =W_{L}z_{L-1}+b_{L}\\
z_{L} & =\sigma(y_{L})
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Now 
\begin_inset Formula $z_{L}$
\end_inset

 is the network output.
 If it is trained using cross-entropy loss as a classifier, then the maximum
 element in vector 
\begin_inset Formula $z_{L}$
\end_inset

 after softmax normalization corresponds to the predicted class.
\end_layout

\begin_layout Subsection
(b)
\end_layout

\begin_layout Standard
Assume batch size 1.
 Then layer 1 has matrix-vector product 
\begin_inset Formula $W_{1}x$
\end_inset

 with complexity 
\begin_inset Formula $O(dd_{1})$
\end_inset

, which involves 
\begin_inset Formula $d\times d_{1}$
\end_inset

 multiplications, and 
\begin_inset Formula $d_{1}$
\end_inset

 additions.
 The bias term involves 
\begin_inset Formula $d_{1}$
\end_inset

 additions.
 The activation step has complexity 
\begin_inset Formula $O(d_{1})$
\end_inset

 as well since it is element wise.
 So in total there will be 
\begin_inset Formula $dd_{1}+d_{1}d_{2}+\ldots+d_{l-1}d_{l}$
\end_inset

 multiplications, and matrix-matrix (in larger batch size) or matrix-vector
 (when batch size 1) multiplication is the dominating computation operation.
 
\end_layout

\begin_layout Subsection
(c)
\end_layout

\begin_layout Standard
We apply chain rule for the formulation.
 Assume cost function 
\begin_inset Formula $J$
\end_inset

.
 We propagate the gradient with chain rule in reversed order compared to
 the forward process.
 Assume 
\begin_inset Formula $\partial J/\partial z_{L}$
\end_inset

 is known, then
\begin_inset Formula 
\begin{align*}
\frac{\partial J}{\partial y_{L}} & =\frac{\partial J}{\partial z_{L}}\cdot\frac{\partial z_{L}}{\partial y_{L}}=\frac{\partial J}{\partial z_{L}}\cdot\sigma'(y_{L})\\
\frac{\partial J}{\partial z_{L-1}} & =\frac{\partial J}{\partial y_{L}}\cdot\frac{\partial y_{L}}{\partial z_{L-1}}=W_{L}^{T}\cdot\frac{\partial J}{\partial y_{L}}\\
\frac{\partial J}{\partial W_{L}} & =\frac{\partial J}{\partial y_{L}}\cdot\frac{\partial y_{L}}{\partial W_{L}}=\frac{\partial J}{\partial y_{L}}\cdot z_{L-1}^{T}\\
\frac{\partial J}{\partial b_{L}} & =\frac{\partial J}{\partial y_{L}}\\
 & ...\\
\frac{\partial J}{\partial W_{1}} & =\frac{\partial J}{\partial y_{1}}\cdot\frac{\partial y_{1}}{\partial W_{1}}=\frac{\partial J}{\partial y_{1}}\cdot x^{T}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
With the chain rule all the formulations in all layers are more or less
 alike.
 In the equation 
\begin_inset Formula $\sigma'(x)$
\end_inset

 is the derivative of activation function 
\begin_inset Formula $\sigma(x)$
\end_inset

.
 In the ReLU case, it is 
\begin_inset Formula $\sigma'(x)=x$
\end_inset

 when 
\begin_inset Formula $x>=0$
\end_inset

, or 
\begin_inset Formula $\sigma'(x)=0$
\end_inset

 when 
\begin_inset Formula $x<0$
\end_inset

.
\end_layout

\begin_layout Subsection
(d)
\end_layout

\begin_layout Standard
Generally backward computation is more expensive than forward pass.
 Because in this case we compute twice as many as matrix multiplications,
 one for the gradient with respect to the weights 
\begin_inset Formula $W_{l}$
\end_inset

, and one for the gradient with respect to 
\begin_inset Formula $z_{l}$
\end_inset

 and 
\begin_inset Formula $y_{l}$
\end_inset

, which is to be propagated.
 With a similar conclusion in answer to question (b), matrix multiplication
 is still the dominating operation.
 Matrix multiplication is the performance bottleneck in practice as well.
 Element-wise operations do not really contribute much complexity.
\end_layout

\begin_layout Section
Q3
\end_layout

\begin_layout Section
Q4
\end_layout

\begin_layout Section
Q5
\end_layout

\begin_layout Subsection
(a) True / false about attack
\end_layout

\begin_layout Itemize
(i) is true.
 Adversarial examples has transferrability.
\end_layout

\begin_layout Itemize
(ii) is false.
 FGSM is called 
\begin_inset Quotes eld
\end_inset

fast
\begin_inset Quotes erd
\end_inset

 because it's single-step.
\end_layout

\begin_layout Itemize
(iii) is false.
 Dropout is effective regularization, but not effective defense.
 Stronger attacks such as adaptive attack will be able to penetrate such
 countermeasure.
\end_layout

\begin_layout Itemize
(iv) is true.
 Most of the discussed attacks are white-box attacks.
 However, black box attacks are also possible, such as transferrability-based
 attacks (TI-FGSM and DI-FGSM), score-based black box attacks (e.g.
 Natural Evolution Strategy), and query-based attacks.
\end_layout

\begin_layout Subsection
(b) FGSM calculation
\end_layout

\begin_layout Standard
FGSM is a single step attack.
 Given
\begin_inset Formula 
\begin{align*}
x & =[1,2,3]^{T}\\
\partial J\partial x & =[0.5,-0.5,1]^{T}\\
\epsilon & =0.01
\end{align*}

\end_inset

we have a single step projected gradient ascent as
\begin_inset Formula 
\begin{align*}
x^{*} & =x+\epsilon\cdot\text{sign}(\partial J/\partial x)\\
 & =x+0.01[1,-1,1]^{T}\\
 & =[1,2,3]^{T}+[0.01,-0.01,0.01]^{T}\\
 & =[1.01,1.99,3.01]^{T}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
(c) FGSM statement
\end_layout

\begin_layout Standard
The statement is false.
 According to Ian Goodfellow in FGSM paper, actually when the dimension
 of 
\begin_inset Formula $x$
\end_inset

 is higher, the required 
\begin_inset Formula $\epsilon$
\end_inset

 is lower.
 Assume mean value 
\begin_inset Formula $\mu$
\end_inset

 for a layer input, mean weight 
\begin_inset Formula $\sigma$
\end_inset

 for the layer, and dimensionality 
\begin_inset Formula $d$
\end_inset

.
 The expected activation is 
\begin_inset Formula $\mu\sigma d$
\end_inset

.
 When a perturbation is added to the linear layer, the activation has been
 bumped by 
\begin_inset Formula $\epsilon\mu\sigma d$
\end_inset

.
 When 
\begin_inset Formula $d$
\end_inset

 is large enough, 
\begin_inset Formula $\epsilon$
\end_inset

 does not have to be large to create a huge activation bump.
\end_layout

\begin_layout Subsection
(c) GAN (there is a typo in question numbering)
\end_layout

\begin_layout Standard
We should use (B) non-saturating cost, in order to avoid making discriminator
 
\begin_inset Formula $D$
\end_inset

 
\begin_inset Quotes eld
\end_inset

too smart
\begin_inset Quotes erd
\end_inset

, and thus maintain a relatively good balance between generator 
\begin_inset Formula $G$
\end_inset

 and discriminator 
\begin_inset Formula $D$
\end_inset

.
 In this case, generator 
\begin_inset Formula $G$
\end_inset

 can still learn even when distriminator successfully rejects all generator
 samples.
\end_layout

\begin_layout Subsection
(d) GAN loss
\end_layout

\begin_layout Standard
For GAN, the equilibrium is a saddle point of the discriminator loss.
 This is reflected in the training process as both discriminator and generator
 are simultaneously learning, and still maintains a similar loss.
\end_layout

\begin_layout Section
Q6
\end_layout

\end_body
\end_document
