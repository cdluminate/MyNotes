#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{times}
\usepackage{microtype}
\usepackage[margin=0.618in]{geometry}
\usepackage{indentfirst}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Machine Learning for Signal Processing
\end_layout

\begin_layout Section
Lec 1 (w1) Representing Sound/Image
\end_layout

\begin_layout Standard
MLSP: sensor -> signal capture -> channel -> feature extraction -> modeling.
\end_layout

\begin_layout Standard
Four aspects: representation, modeling, classifiction, prediction.
\end_layout

\begin_layout Standard
Sampling:Nyquist freq: sample rate twise as high as the highest frequency
 we want to represent.
\end_layout

\begin_layout Section
Lec 2,3 (w1) Linear Algebra
\end_layout

\begin_layout Standard
Vectors and Matrices
\end_layout

\begin_layout Standard
Basic operations: addition, multiplication, inner product, outer product
\end_layout

\begin_layout Standard
Various matrix types.
\end_layout

\begin_layout Standard
Matrix properties: rank, determinant, inverse (pseudo inverse).
\end_layout

\begin_layout Standard
Solving simultaneous equations.
\end_layout

\begin_layout Standard
Projections.
\end_layout

\begin_layout Standard
Eigen Vectors and Eigen Values.
\end_layout

\begin_layout Standard
Singular Value Decomposition.
\end_layout

\begin_layout Section
Lec 4,5 (w2,3) Optimization
\end_layout

\begin_layout Standard
Direct solution: derivative = 0.
\end_layout

\begin_layout Standard
Gradient: Jacobian, Hessian.
\end_layout

\begin_layout Standard
Descent methods: newton's method.
 gradient method.
\end_layout

\begin_layout Standard
Online optimization.
\end_layout

\begin_layout Standard
Constrained optimization: Lagrange's method, projected gradients.
\end_layout

\begin_layout Standard
Regularization: L1 or L2 (Tikhonov)
\end_layout

\begin_layout Standard
Convex optimization and Lagrangian duals.
\end_layout

\begin_layout Section
Lec 6 (w3) Representing Signal
\end_layout

\begin_layout Standard
Basis-based representations: Haar base, Fourier base, Spectrograms, DCT,
 Wavelet
\end_layout

\begin_layout Standard
Good basis: non-redundancy, compactness
\end_layout

\begin_layout Section
Lec 7 (w4) Eigen Representations
\end_layout

\begin_layout Standard
SVD: 
\begin_inset Formula $A=USV^{T}$
\end_inset


\end_layout

\begin_layout Standard
EigenFace.
\end_layout

\begin_layout Standard
Eigen decomposition v.s.
 SVD: 
\begin_inset Formula $XX^{T}=R=EDE^{T}$
\end_inset

, 
\begin_inset Formula $X=USV^{T}$
\end_inset

, 
\begin_inset Formula $XX^{T}=USV^{T}VS^{T}U^{T}=US^{2}U^{T}$
\end_inset

, and thus 
\begin_inset Formula $E=U$
\end_inset

, 
\begin_inset Formula $S^{2}=D$
\end_inset

.
 
\begin_inset Formula $E$
\end_inset

 are the eigen bases.
 If we use top-K bases, it's called Karhunen Loeve Transform (KLT), instead
 of PCA.
\end_layout

\begin_layout Standard
Faster implementation: 
\begin_inset Formula $R=X^{T}X$
\end_inset

, 
\begin_inset Formula $RU=U\Lambda$
\end_inset

, 
\begin_inset Formula $V=XU$
\end_inset

, the eigen faces are 
\begin_inset Formula $V$
\end_inset

.
\end_layout

\begin_layout Standard
If we substract the mean value first, it becomes PCA from KLT.
 KLT retains maximum energy for any k, while PCA retains maximum variance
 for any k.
\end_layout

\begin_layout Standard
Generally, Eigen face refers to PCA faces instead of KLT faces.
\end_layout

\begin_layout Section
Lec 8 (w4) Non-negative Matrix Factorization
\end_layout

\begin_layout Standard
NMF: 
\begin_inset Formula $V=BW$
\end_inset

.
 All entries in 
\begin_inset Formula $V$
\end_inset

, 
\begin_inset Formula $B$
\end_inset

 and 
\begin_inset Formula $W$
\end_inset

 must be non-negative.
\end_layout

\begin_layout Section
Lec 9 (w5) Independent Component Analysis
\end_layout

\begin_layout Standard
The covariance matrix captures the directions of maximum variance.
\end_layout

\begin_layout Standard
Correlation is not causation.
 However, independence implies uncorrelatedness.
\end_layout

\begin_layout Standard
ICA is a little more than PCA: not merely decorrelated but also whitened.
\end_layout

\begin_layout Standard
PCA will indicate orthogonal directions of maximal variance (may not align
 with data); while ICA finds directions that are independent (more likely
 to align with data).
\end_layout

\begin_layout Standard
ICA issue: no sense of order, unlike PCA.
\end_layout

\begin_layout Section
Lec 10 (w5) Boosting & Face Detection
\end_layout

\begin_layout Standard
Boosting: emsenble method that sequentially (in parallel) combines many
 simple binary classifiers to construct a final complex classifier.
 Boosting is a form of voting.
\end_layout

\begin_layout Standard
Adaboost: keep adding classifiers incrementally, to fix what others missed.
\end_layout

\begin_layout Standard
Face Detection: Viola Jones Method (boosted classifier).
\end_layout

\begin_layout Section
Lec 11 (w6) Clustering
\end_layout

\begin_layout Standard
K-Means clustering.
\end_layout

\begin_layout Section
Lec 13 (w7) Overcomplete & Sparse Representations
\end_layout

\begin_layout Standard
If number of basis vectors is greater than the dimensions of the input,
 we have an overcomplete representation.
\end_layout

\begin_layout Standard
Obtainining sparse solutions: matching persuit (MP), and basis pursuit (BP).
\end_layout

\begin_layout Standard
Matching Pursuit: greedy algorithm.
 (like schmidt orthogonalization).
 Problem: computational complexity â€“ the entire dictionary has to be searched
 at every iteration.
\end_layout

\begin_layout Standard
Basis pursuit: LASSO (L-1 penalty besides the MSE reconstruction).
\end_layout

\begin_layout Standard
Dictionary: can be random.
 Or k-means initialization.
 Or K-SVD.
\end_layout

\begin_layout Standard
Overcomplete representation can sometimes do better than ICA.
\end_layout

\begin_layout Section
Lec 14 (w7) Classification
\end_layout

\end_body
\end_document
