#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[margin=1in]{geometry}
%\usepackage{indentfirst}
%\setlength{\parindent}{0pt}
\usepackage{bm}
\usepackage{times}
\newcommand{\pperp}{\perp\kern-5pt\perp}
\usepackage{tikz}
\usetikzlibrary[arrows,shapes,snakes,backgrounds,bayesnet]
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\end_preamble
\options a4
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "newtxmath" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype true
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize 10
\spacing single
\use_hyperref true
\pdf_bookmarks true
\pdf_bookmarksnumbered false
\pdf_bookmarksopen false
\pdf_bookmarksopenlevel 1
\pdf_breaklinks false
\pdf_pdfborder false
\pdf_colorlinks false
\pdf_backref false
\pdf_pdfusetitle true
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Section

\series bold
Probablistic Machine Learning
\end_layout

\begin_layout Standard

\series bold
JHU ECE EN 520.651 2021 Fall
\end_layout

\begin_layout Standard

\series bold
M.
 Zhou / Simplified Course Notes / Cheatsheet
\end_layout

\begin_layout Subsection
Probability Review
\end_layout

\begin_layout Standard

\series bold
Probablistic Model.

\series default
 
\begin_inset Formula $\Omega$
\end_inset

: sample space.
 For instance, single roll of dice 
\begin_inset Formula $\Omega=\{1,2,3,4,5,6\}$
\end_inset

; 
\begin_inset Formula $A$
\end_inset

: event, a set of outcomes.
 Venn diagrams can be used to visualize basic set operations; 
\begin_inset Formula $P(A)$
\end_inset

: probability measure, non-negative; For a event, the boundary case is 
\begin_inset Formula $\Omega$
\end_inset

 (certain set) and 
\begin_inset Formula $\phi$
\end_inset

 (null set).
\end_layout

\begin_layout Standard

\series bold
Sigma Field.

\series default
 Given a sample space 
\begin_inset Formula $\Omega$
\end_inset

 and events 
\begin_inset Formula $E,F$
\end_inset

, the colection of subsets of 
\begin_inset Formula $\Omega$
\end_inset

, defined as 
\begin_inset Formula $M$
\end_inset

, forms a field if (1) 
\begin_inset Formula $\phi\in M$
\end_inset

, 
\begin_inset Formula $\Omega\in M$
\end_inset

; (2) If 
\begin_inset Formula $E,F\in M$
\end_inset

, then 
\begin_inset Formula $E\cup F\in M$
\end_inset

 and 
\begin_inset Formula $E\cap F\in M$
\end_inset

; (3) If 
\begin_inset Formula $E\in M$
\end_inset

, then 
\begin_inset Formula $E^{C}\in M$
\end_inset

.
 A sigma field is closed under a countable number of unions, intersections,
 and complements.
 We care about fields and sigma fields because of consistency.
 When 
\begin_inset Formula $P(E)$
\end_inset

 and 
\begin_inset Formula $P(F)$
\end_inset

 are defined, but not 
\begin_inset Formula $P(E\cap F)$
\end_inset

, then the rule will be broken.
\end_layout

\begin_layout Standard

\series bold
Strategy for the smallest 
\begin_inset Formula $\sigma$
\end_inset

-field.

\series default
 (1) Include 
\begin_inset Formula $\phi$
\end_inset

 and 
\begin_inset Formula $\Omega$
\end_inset

; (2) Include disjoint parts; (3) Include all pairs, triplets, quadruplets,
 etc.
 Alternative strategy: use a binary mask, and the size of the set would
 be 
\begin_inset Formula $2^{N}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Probability.

\series default
 Set function 
\begin_inset Formula $P:E\in\mathcal{F}\mapsto P(E)\in\mathbb{R}^{+}$
\end_inset

.
 (1) 
\begin_inset Formula $P(E)\geq0$
\end_inset

 (2) 
\begin_inset Formula $P(\Omega)=1$
\end_inset

 i.e., normalization (3) 
\begin_inset Formula $P(E\cup F)=P(E)+P(F),\forall E,F\in\Omega,s.t.E\cap F=\phi$
\end_inset

.
 Then we can establish (4) 
\begin_inset Formula $P(\phi)=0$
\end_inset

 (5) 
\begin_inset Formula $P(E)=1-P(E^{C})$
\end_inset

 (6) 
\begin_inset Formula $P(E\cup F)=P(E)+P(F)-P(E\cap F)$
\end_inset

 (7) 
\begin_inset Formula $P(\cap_{i=1}^{n}E_{i})\leq\sum_{i=1}^{n}P(E_{i})$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Relationship Between Events: 
\series default
Joint probability 
\begin_inset Formula $P(A\cap B)$
\end_inset

; Conditional probability 
\begin_inset Formula $P(B|A)=P(A\cap B)/P(A)$
\end_inset

 which means 
\begin_inset Formula $P(B|A)P(A)=P(A|B)P(B)$
\end_inset

.
 Two events 
\begin_inset Formula $A\in\mathcal{F}$
\end_inset

and 
\begin_inset Formula $B\in\mathcal{F}$
\end_inset

are 
\bar under
independent
\bar default
 if 
\begin_inset Formula $P(A\cap B)=P(A)P(B)$
\end_inset

 for 
\begin_inset Formula $P(A)>0$
\end_inset

 and 
\begin_inset Formula $P(B)>0$
\end_inset

.
 Three events 
\begin_inset Formula $A,B,C$
\end_inset

 (non-zero probability) are 
\bar under
jointly independent
\bar default
 if (1) 
\begin_inset Formula $P(A\cap B)=P(A)P(B)$
\end_inset

 – all pairwise combinations alike; (2) 
\begin_inset Formula $P(A\cap B\cap C)=P(A)P(B)P(C)$
\end_inset

 – mutual independence.
 
\bar under
Independent Experiments
\bar default
: the outcome of one experiment is not affected by the past, present, or
 future values of the other experiment.
\end_layout

\begin_layout Standard

\series bold
Total Probability:
\series default
 Let 
\begin_inset Formula $A_{i}$
\end_inset

(
\begin_inset Formula $i=1,\ldots,n)$
\end_inset

 be mutually exclusive and exhaustive w/ nonzero probability, for all 
\begin_inset Formula $B\in\Omega$
\end_inset

, we can write
\begin_inset Formula 
\[
P(B)=\sum_{i=1}^{n}P(A_{i})P(B|A_{i})
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Bayes Rule:
\series default
 Given the above setup and 
\begin_inset Formula $P(B)>0$
\end_inset

, then
\begin_inset Formula 
\[
P(A_{j}|B)=\frac{P(A_{j}\cap B)}{P(B)}=\frac{P(A_{j})P(B|A_{j})}{\sum_{i=1}^{n}P(A_{i})P(B|A_{i})}
\]

\end_inset

where 
\begin_inset Formula $P(A_{i})$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

prior
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Formula $P(B|A_{i})$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

likelihood
\begin_inset Quotes erd
\end_inset

, and 
\begin_inset Formula $P(A_{i}|B)$
\end_inset

 is 
\begin_inset Quotes eld
\end_inset

posterior
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Subsection
Random Variables
\end_layout

\begin_layout Standard
Random variable 
\begin_inset Formula $X(\cdot)$
\end_inset

 is a function that maps outcome 
\begin_inset Formula $\omega\in\Omega$
\end_inset

 onto the real number line 
\begin_inset Formula $X(\omega)\in\mathbb{R}$
\end_inset

, i.e., 
\begin_inset Formula $X:\Omega\mapsto\mathbb{R}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Cumulative Distribution Function (CDF):
\series default

\begin_inset Formula 
\[
F_{X}(x)=P(\omega\in\Omega|X(\omega)\leq x)\triangleq P(X\leq x)
\]

\end_inset


\end_layout

\begin_layout Standard
(1) 
\begin_inset Formula $F_{X}(-\infty)=P(\phi)=0$
\end_inset

, 
\begin_inset Formula $F_{X}(+\infty)=P(\phi)=1$
\end_inset


\end_layout

\begin_layout Standard
(2) 
\begin_inset Formula $x_{1}\leq x_{2}$
\end_inset

, 
\begin_inset Formula $F_{X}(x_{1})<F_{X}(x_{2})$
\end_inset


\end_layout

\begin_layout Standard
(3) 
\begin_inset Formula $F_{X}(x)=\lim_{\epsilon\rightarrow0^{+}}F_{X}(x+\epsilon)$
\end_inset


\end_layout

\begin_layout Standard
Implication: 
\begin_inset Formula $P(a<X\leq b)=F_{X}(b)-F_{X}(a)$
\end_inset


\end_layout

\begin_layout Standard

\series bold
Probability Density Function (PDF):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
f_{X}(x)=\frac{d}{dx}F_{X}(x)
\]

\end_inset


\end_layout

\begin_layout Standard
(1) 
\begin_inset Formula $f_{X}(x)\geq0$
\end_inset


\end_layout

\begin_layout Standard
(2) 
\begin_inset Formula $\int_{-\infty}^{\infty}f_{X}(\xi)d\xi=F_{X}(\infty)-F_{X}(-\infty)=1$
\end_inset


\end_layout

\begin_layout Standard
(3) 
\begin_inset Formula $F_{X}(x)=\int_{-\infty}^{x}f_{X}(\xi)d\xi=P(X\leq x)$
\end_inset


\end_layout

\begin_layout Standard
(4) 
\begin_inset Formula $\int_{x_{1}}^{x_{2}}f_{X}(\xi)d\xi=F_{X}(x_{2})-F_{X}(x_{1})=P(x_{1}<X\leq x_{2})$
\end_inset


\end_layout

\begin_layout Standard
See appendix for a list of useful distributions.
\end_layout

\begin_layout Standard

\series bold
Mixed Random Variables.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(X=x)=\lim_{e\rightarrow0^{+}}\int_{x}^{x+e}f_{X}(\xi)d\xi
\]

\end_inset

 The value is generally 0 for continuous RV.
 When we need nonzero mass at 
\begin_inset Formula $x=x_{0}$
\end_inset

, we can use a Dirac delta 
\begin_inset Formula $\delta(x-x_{0})$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Conditional Distribution
\series default
 of 
\begin_inset Formula $X$
\end_inset

 given 
\begin_inset Formula $B$
\end_inset

 is,
\begin_inset Formula 
\[
F_{X}(x|B)=\frac{P(X\leq x,B)}{P(B)}\qquad f_{X}(x|B)=\frac{d}{dx}F_{X}(x|B)
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Joint Distribution
\series default
 of 
\begin_inset Formula $X:\Omega\mapsto\mathbb{R}$
\end_inset

 and 
\begin_inset Formula $Y:\Omega\mapsto\mathbb{R}$
\end_inset

 
\begin_inset Formula 
\[
F_{XY}(x,y)=P(\omega\in\Omega|X(\omega)\leq x,Y(\omega)\leq y)\triangleq P(X\leq x,Y\leq y)
\]

\end_inset

where 
\begin_inset Formula $F_{XY}(\infty,\infty)=1$
\end_inset

, 
\begin_inset Formula $F_{XY}(-\infty,-\infty)=0$
\end_inset

.
 
\begin_inset Formula $F(x,+\infty)=F_{X}(x)$
\end_inset

.
 
\begin_inset Formula $\forall x_{1}\leq x_{2},y_{1}\leq y_{2},F(x_{1},y_{1})\leq F(x_{2},y_{2})$
\end_inset

.
 PDF is
\begin_inset Formula 
\[
f_{XY}(x,y)=\frac{\partial^{2}}{\partial x\partial y}F_{XY}(x,y)
\]

\end_inset

and marginal is 
\begin_inset Formula $f_{X}(x)=\int_{-\infty}^{+\infty}f_{XY}(x,y)dy$
\end_inset

.
 The conditional is 
\begin_inset Formula $f_{X|Y}(x|y)=\frac{f_{XY}}{f_{Y}}=\frac{f_{X}f_{Y|X}}{\int_{-\infty}^{\infty}f_{X}(x')f_{Y|X}(y|x')dx'}$
\end_inset

 (a slice).
\end_layout

\begin_layout Standard

\series bold
Independence.

\series default
 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent if 
\begin_inset Formula $F_{XY}=F_{X}F_{Y}$
\end_inset

 or 
\begin_inset Formula $f_{XY}=f_{X}f_{Y}$
\end_inset

 or 
\begin_inset Formula $f_{X|Y}=f_{X}$
\end_inset

.
\end_layout

\begin_layout Subsection
Functions of Random Variables
\end_layout

\begin_layout Standard

\series bold
Discrete RV.

\series default
 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

.
 
\begin_inset Formula $P_{X}(x)=P(X=x)$
\end_inset

 s.t.
 
\begin_inset Formula $\sum_{x\in\mathcal{X}}P_{X}(x)=1$
\end_inset

.
 Then for 
\begin_inset Formula $Y=g(X)$
\end_inset

 we have 
\begin_inset Formula $P_{Y}(y)=\sum_{x:g(x)=y}P_{X}(x)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Continuous RV.

\series default
 Method I: direct computation.
 First compute the CDF of 
\begin_inset Formula $Y$
\end_inset

, then differentiate to obtain PDF.
 
\begin_inset Formula $P(Y\leq y)=P(g(X)\leq y)=P(X\leq g^{-1}(y))=F_{X}(g^{-1}(y))$
\end_inset

.
\end_layout

\begin_layout Standard
Method II.
 Root Formula.
 
\begin_inset Formula $x_{i}$
\end_inset

 are the roots of 
\begin_inset Formula $y=g(x)$
\end_inset

.
\begin_inset Formula 
\[
f_{Y}(y)\approx\sum_{i=1}^{N}f_{X}(x_{i})\big|\frac{dx_{i}}{d_{y}}\big|\rightarrow\sum_{i=1}^{N}f_{X}(x_{i})\frac{1}{|g'(x_{i})|}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Multivariate Function.

\series default
 We cannot derive a root formula for 
\begin_inset Formula $Z=g(X,Y)$
\end_inset

.
 We use direct computation: 
\begin_inset Formula $F_{Z}(z)=P(Z\leq z)=P(g(X,Y)\leq z)=\iint_{C_{Z}}f_{XY}(x,y)dxdy$
\end_inset

.
 Then 
\begin_inset Formula $f_{Z}(z)=\frac{d}{dz}F_{Z}(z)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Sum of Independent RVs
\series default
.
 Special case of multivar.
 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are independent.
 
\begin_inset Formula $Z=X+Y$
\end_inset

.
 Convolution for PDF:
\begin_inset Formula 
\[
f_{Z}(z)=\int_{-\infty}^{+\infty}f_{Y}(y)f_{X}(z-y)dy=\int_{-\infty}^{+\infty}f_{X}(x)f_{Y}(z-y)dx
\]

\end_inset


\end_layout

\begin_layout Subsection
Moment Generating Functions
\end_layout

\begin_layout Standard

\series bold
Expectation.

\series default
 1st order raw moment.
 For discrete RV 
\begin_inset Formula $E[X]=\sum_{-\infty}^{+\infty}x\cdot P_{X}(x)$
\end_inset

.
 For continuous RV 
\begin_inset Formula $E[X]=\int_{-\infty}^{+\infty}x\cdot f_{X}(x)dx$
\end_inset

.
 For 
\begin_inset Formula $Y=g(X)$
\end_inset

, 
\begin_inset Formula $E[Y]=\int_{-\infty}^{+\infty}g(x)f_{X}dx$
\end_inset

.
 For 
\begin_inset Formula $Z=g(X,Y)$
\end_inset

, 
\begin_inset Formula $E[Z]=\iint_{-\infty}^{+\infty}g(x,y)f_{XY}dxdy$
\end_inset

.
 
\bar under
Properties:
\bar default
 (1) 
\begin_inset Formula $E[X+Y]=E[X]+E[Y]$
\end_inset

; (2) 
\begin_inset Formula $E[XY]=E[X]E[Y]$
\end_inset

 for independent 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Conditional Expect.

\series default
 
\begin_inset Formula $E[Y|X=x]=\int_{-\infty}^{+\infty}yf_{Y|X}(y|x)dy$
\end_inset

.
 Law of iterated expectations 
\begin_inset Formula $E[Y]=E_{X}[E_{Y|X}[Y|X]]$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
High Order Moments.

\series default
 The 
\begin_inset Formula $r^{th}$
\end_inset

 moment of 
\begin_inset Formula $X$
\end_inset

 is 
\begin_inset Formula $m_{r}=E[X^{r}]=\int_{-\infty}^{+\infty}x^{r}f_{X}(x)dx$
\end_inset

.
 The 
\begin_inset Formula $r^{th}$
\end_inset

 central moment is 
\begin_inset Formula $c_{r}=E[(x-\mu_{x})^{r}]=\int_{-\infty}^{+\infty}(x-\mu_{x})^{r}f_{X}(x)dx$
\end_inset

.
 The variance is 
\begin_inset Formula 
\[
c_{2}\triangleq\sigma_{X}^{2}=E[(x-\mu_{x})^{2}]=E[X^{2}]-\mu_{X}^{2}
\]

\end_inset

Moments do not always exist.
 For example, Cauthy distribution does not have that.
 Some properties 
\begin_inset Formula $Var[X+a]=Var[X]$
\end_inset

, 
\begin_inset Formula $Var[aX]=a^{2}Var[X]$
\end_inset

, 
\begin_inset Formula $Var[aX+bY]=a^{2}Var[X]+b^{2}Var[Y]+2abCov[X,Y]$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Joint Moments.

\series default
 The 
\begin_inset Formula $(i,j)^{th}$
\end_inset

 joint and joint central moments are 
\begin_inset Formula $m_{ij}=E[X^{i}Y^{j}]$
\end_inset

 and 
\begin_inset Formula $c_{ij}=E[(X-\mu_{X})^{i}(Y-\mu_{Y})^{j}]$
\end_inset

.
 The covariance is
\begin_inset Formula 
\[
\sigma_{XY}=Cov(X,Y)=E[(X-\mu_{X})(Y-\mu_{Y})]=E[XY]-\mu_{X}\mu_{Y}
\]

\end_inset

Uncorrelatedness is weaker than independence.
\end_layout

\begin_layout Standard

\series bold
Moment Generating Functions.

\series default
 
\begin_inset Formula $M_{X}(S)=E[e^{SX}]$
\end_inset

.
 
\begin_inset Formula 
\[
\frac{d^{r}}{dS^{r}}M_{X}(S)\Big|_{S=0}=\int_{-\infty}^{+\infty}x^{r}f_{X}(x)dx\triangleq E[X^{r}]=m_{r}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Weak Law of Large Numbers.

\series default
 Let 
\begin_inset Formula $X_{1},\ldots,X_{N}\sim i.i.d$
\end_inset

 with mean 
\begin_inset Formula $\mu_{X}$
\end_inset

 and 
\begin_inset Formula $\sigma_{X}^{2}<\infty$
\end_inset

.
 The mean estimator 
\begin_inset Formula $\hat{\mu}_{X}=\frac{1}{N}\sum_{i=1}^{N}X_{i}$
\end_inset

 satisfies 
\begin_inset Formula $P(|\hat{\mu}_{X}-\mu_{X}|\geq\delta)\leq\frac{\sigma_{X}^{2}}{N\delta^{2}}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Central Limit Theorem.

\series default
 Characteristic function: 
\begin_inset Formula $\Phi_{X}(\omega)=\int_{-\infty}^{+\infty}e^{j\omega x}f_{X}(x)dx$
\end_inset

 is Fourier equivalent of MGF.
 Let 
\begin_inset Formula $X_{1},\ldots,X_{n}\sim i.i.d$
\end_inset

 with 
\begin_inset Formula $\mu_{X}=0$
\end_inset

, 
\begin_inset Formula $Var(X)=1$
\end_inset

.
 Define 
\begin_inset Formula $Z_{n}=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}X_{i}$
\end_inset

, then 
\begin_inset Formula $\lim_{n\rightarrow\infty}\Phi_{Z_{n}}(\omega)=\exp\{-\frac{1}{2}\omega^{2}\}$
\end_inset

.
\end_layout

\begin_layout Subsection
Random Vectors
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P(\ubar X\leq\ubar x)=P(X_{1}\leq x_{1},\ldots,X_{N}\leq x_{N})\quad f_{\ubar X}(\ubar x)=\frac{\partial^{N}F_{\ubar X}(\ubar x)}{\partial x_{1}\ldots\partial x_{N}}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Function of Rand Vectors.

\series default
 
\begin_inset Formula $y_{i}=g_{i}(x_{1},\ldots,x_{n})$
\end_inset

, 
\begin_inset Formula $x_{i}=\phi_{i}(y_{1},\ldots)$
\end_inset

.
 Root formula 
\begin_inset Formula $f_{\ubar Y}(\ubar y)=\sum_{r=1}^{R}f_{\ubar X}(\ubar x^{r})\frac{1}{|J_{r}|},\,s.t.\,\ubar y=\ubar g(\ubar x^{r})$
\end_inset

.
\begin_inset Formula 
\[
\frac{\partial x}{\partial y}=\begin{vmatrix}\frac{\partial\phi_{1}}{\partial y_{1}} & \cdots & \frac{\partial\phi_{n}}{\partial y_{1}}\\
\vdots &  & \vdots\\
\frac{\partial\phi_{1}}{\partial y_{n}} & \cdots & \frac{\partial\phi_{n}}{\partial y_{n}}
\end{vmatrix}=\begin{vmatrix}\frac{\partial g_{1}}{\partial y_{1}} & \cdots & \frac{\partial g_{n}}{\partial y_{1}}\\
\vdots &  & \vdots\\
\frac{\partial g_{1}}{\partial y_{n}} & \cdots & \frac{\partial g_{n}}{\partial y_{n}}
\end{vmatrix}^{-1}=|J|^{-1}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\ubar X=[X_{1},\ldots,X_{N}]^{T}$
\end_inset

, Mean 
\begin_inset Formula $E[\ubar X]=[\mu_{1},\ldots,\mu_{N}]^{N}=\ubar\mu_{X}$
\end_inset

.
 
\begin_inset Formula $\mu_{i}=E[X_{i}]=\int_{-\infty}^{+\infty}\cdots\int_{-\infty}^{+\infty}x_{i}f_{\ubar X}dx_{1}\cdots dx_{N}=\int_{-\infty}^{+\infty}x_{i}f_{X_{i}}dx_{i}$
\end_inset

.
 Covariance 
\begin_inset Formula $\mathbb{K}=E[(\ubar X-\mu_{X})(\ubar X-\mu_{X})^{T}]\in\mathbb{R}^{N}$
\end_inset

.
 The correlation matrix 
\begin_inset Formula $\bm{R}=E[XX^{T}]=\mathbb{K}+\mu_{X}\mu_{X}^{T}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Covariance Matrix.

\series default
 
\begin_inset Formula $\mathbb{K}$
\end_inset

 is positive semi-definite, i.e., 
\begin_inset Formula $z^{T}\mathbb{K}z\geq0$
\end_inset

.
 
\begin_inset Formula $\mathbb{K}$
\end_inset

 is symmetric.
 
\begin_inset Formula $\mathbb{K}=U\Lambda U^{T}$
\end_inset

 where 
\begin_inset Formula $U^{T}U=I$
\end_inset

 and any eivenvalue
\begin_inset Formula $\lambda_{i}\geq0$
\end_inset

, 
\begin_inset Formula $\forall i=1,\ldots,N$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Whitening Transform.

\series default
 
\begin_inset Formula $\ubar X\sim f_{\ubar X}$
\end_inset

 with 
\begin_inset Formula $\mu_{x}=0$
\end_inset

 and PD covariance 
\begin_inset Formula $\mathbb{K}_{X}=U\Lambda U^{T}$
\end_inset

.
 Find a linear transformation 
\begin_inset Formula $\ubar y=C\ubar x$
\end_inset

 so that 
\begin_inset Formula $\mathbb{K}_{Y}=I$
\end_inset

.
 Thus, 
\begin_inset Formula $\mathbb{K}_{Y}=E[YY^{T}]=E[CXX^{T}C^{T}]=C\mathbb{K}_{X}C^{T}=I$
\end_inset

, and hence 
\begin_inset Formula $C=\Lambda^{-\frac{1}{2}}U^{T}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Multivariate Gaussian.

\series default
 
\begin_inset Formula 
\[
f_{\ubar X}(\ubar x)=\frac{1}{(2\pi)^{n/2}|\mathbb{K}_{X}|^{1/2}}\exp\{-\frac{1}{2}(\ubar x-\mu_{x})^{T}\mathbb{K}_{X}^{-1}(\ubar x-\mu_{X})\}
\]

\end_inset


\begin_inset Formula $y=Ax$
\end_inset

 is also gaussian w/ 
\begin_inset Formula $\mu_{Y}=A\mu_{x}$
\end_inset

 and 
\begin_inset Formula $\mathbb{K}_{Y}=A\mathbb{K}_{X}A^{T}$
\end_inset

.
\end_layout

\begin_layout Subsection
Bayesian Hypothesis Testing
\end_layout

\begin_layout Standard

\series bold
Bayesian Hypothesis Testing.

\series default
 Observation vector 
\begin_inset Formula $\ubar y$
\end_inset

, unknown state of the world 
\begin_inset Formula $H$
\end_inset

, prior 
\begin_inset Formula $P_{H}(H_{m})$
\end_inset

, likelihood 
\begin_inset Formula $P_{\ubar Y|H}(\ubar y,H_{m})$
\end_inset

, posterior 
\begin_inset Formula $P_{H|Y}(H_{m}|y)$
\end_inset

.
 From Bayes rule,
\begin_inset Formula 
\[
P_{H|Y}(H_{m}|y)=\frac{P_{H}(H_{m})P_{Y|H}(y|H_{m})}{\sum_{m'}P_{H}(H_{m'})P_{Y|H}(y|H_{m'})}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Binary Hypothesis Testing.

\series default
 
\begin_inset Formula $H\in\{H_{0},H_{1}\}$
\end_inset

.
 
\begin_inset Formula $P_{H}(H_{0})\triangleq P_{0}$
\end_inset

, 
\begin_inset Formula $H_{0}:P_{Y|H}(y|H_{0})$
\end_inset

.
 A decision rule 
\begin_inset Formula $\hat{H}(\cdot)$
\end_inset

 maps observation 
\begin_inset Formula $\ubar y$
\end_inset

 onto a hypothesis 
\begin_inset Formula $H\in\{H_{0},H_{1}\}$
\end_inset

.
 Cost function 
\begin_inset Formula $c_{ij}\triangleq\tilde{C}(H_{j},H_{i})$
\end_inset

 where 
\begin_inset Formula $H_{j}$
\end_inset

 true state, 
\begin_inset Formula $H_{i}$
\end_inset

 our guess.
 A cost function is valid when 
\begin_inset Formula $c_{01}>c_{11}$
\end_inset

 and 
\begin_inset Formula $c_{10}>c_{00}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Likelihood Ratio Test (LRT).

\series default
 Minimize 
\begin_inset Formula $\phi(\hat{H})=E_{Y,H}[\tilde{C}(H,\hat{H}(y))]=E_{Y}[E_{H|Y}[\tilde{C}(H,\hat{H}(y))|Y=y]]=\sum_{y}p_{Y}(y)E_{H|Y}[\tilde{C}(H,\hat{H}(y))|Y=y]$
\end_inset

.
 
\begin_inset Formula 
\[
L(y)\triangleq\frac{P_{Y|H}(y|H_{1})}{P_{Y|H}(y|H_{0})}\gtrless_{(H_{0})}^{(H_{1})}\frac{P_{0}(c_{10}-c_{00})}{P_{1}(c_{01}-c_{11})}\triangleq\eta
\]

\end_inset

MAP decision rule: symmetric cost 
\begin_inset Formula $c_{00}=c_{11}=0$
\end_inset

, 
\begin_inset Formula $c_{01}=c_{10}=1$
\end_inset

.
 
\begin_inset Formula $P_{1}P(y|H_{1})\gtrless_{(H_{0})}^{(H_{1})}P_{0}P(y|H_{0})$
\end_inset

, i.e., 
\begin_inset Formula $P(H_{1}|y)\gtrless_{(H_{0})}^{(H_{1})}P(H_{0}|y)$
\end_inset

.
 ML decision rule: symmetric cost and 
\begin_inset Formula $P_{0}=P_{1}=\frac{1}{2}$
\end_inset

.
 
\begin_inset Formula $P(y|H_{1})\gtrless_{(H_{0})}^{(H_{1})}P(y|H_{0})$
\end_inset

.
 
\end_layout

\begin_layout Subsection
NonBayesian Hypothe sis Testing
\end_layout

\begin_layout Standard

\series bold
Classical Hypothesis Testing.

\series default
 
\begin_inset Formula $L(y)\triangleq\frac{P(y|H_{1})}{P(y|H_{0})}\gtrless_{(H_{0})}^{(H_{1})}\eta$
\end_inset

.
 This is the generalization of the Bayesian case.
\end_layout

\begin_layout Standard

\series bold
Operating Characteristics.

\series default
 
\begin_inset Formula $P_{D}=P(\hat{H}(y)=H_{1}|H_{1})=\int_{y_{1}}P(y|H_{1})dy$
\end_inset

.
 
\begin_inset Formula $P_{F}=P(\hat{H}(y)=H_{1}|H_{0})=\int_{y_{1}}P(y|H_{0})dy$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
ROC.

\series default
 
\begin_inset Formula $P_{F}$
\end_inset

-
\begin_inset Formula $P_{D}$
\end_inset

 curve.
 Axes limits 
\begin_inset Formula $[0,1]$
\end_inset

.
 
\begin_inset Formula $\eta=0$
\end_inset

 (upper right corner), 
\begin_inset Formula $\eta\rightarrow\infty$
\end_inset

 (lower left corner).
 Mototonically non-decreasing in 
\begin_inset Formula $\eta$
\end_inset

.
 Lies above the diagonal.
\end_layout

\begin_layout Standard

\series bold
Neyman-Pearson HT.

\series default
 
\begin_inset Formula $\max_{\hat{H}(\cdot)}P_{D}\ s.t.\ P_{F}\leq\alpha$
\end_inset

.
 The optimal solution can be expressed as LRT, where 
\begin_inset Formula $\eta=\lambda$
\end_inset

 such that 
\begin_inset Formula $P_{F}=\alpha$
\end_inset

.
 It is intersection of ROC w/ 
\begin_inset Formula $P_{F}=\alpha$
\end_inset

 line.
\end_layout

\begin_layout Standard

\series bold
Randomized Decision Rule
\series default
 for discrete-valued data.
 
\begin_inset Formula $\eta_{i+1}>\eta_{i}$
\end_inset

.
 
\begin_inset Formula $P_{D}=p\cdot P_{D}(\eta_{i})+(1-p)P_{D}(\eta_{i+1})$
\end_inset

.
 
\begin_inset Formula $P_{F}=p\cdot P_{F}(\eta_{i})+(1-p)P_{F}(\eta_{i+1})$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Efficient Frontier for LRT.

\series default
 The achievable 
\begin_inset Formula $(P_{F},P_{D})$
\end_inset

 operating points.
 On efficient frontier, (1) the 
\begin_inset Formula $(0,0)$
\end_inset

 and 
\begin_inset Formula $(1,1)$
\end_inset

 points always lie here; (2) 
\begin_inset Formula $P_{D}\geq P_{F}$
\end_inset

; (3) is concave (randomized decision should not beat NP); (4) 
\begin_inset Formula $\frac{dP_{D}}{dP_{F}}=\eta$
\end_inset

.
 
\end_layout

\begin_layout Subsection
Minmax Hypothesis Testing
\end_layout

\begin_layout Standard
Setup.
 Adversarial game w/ cost but w/o priors
\begin_inset Formula 
\[
\hat{H}_{M}(\cdot)=\arg\min_{f(\cdot)}[\max_{p=Pr(H=H_{1})\in[0,1]}E_{Y,H}[\tilde{C}(H,f(Y))]]
\]

\end_inset

The class of 
\begin_inset Formula $f(\cdot)$
\end_inset

 is restricted to LRTs, 
\begin_inset Formula $\hat{H}_{B}(y,q)=H_{1}\cdot\mathbf{1}\{\mathcal{L}(y)>\frac{1-q}{q}\frac{c_{10}-c_{00}}{c_{01}-c_{11}}\}+H_{0}\{o.w.\}$
\end_inset

.
 The solution is
\begin_inset Formula 
\[
P_{D}(q^{*})=\frac{c_{10}-c_{00}}{c_{01}-c_{11}}-P_{F}(q^{*})[\frac{c_{10}-c_{00}}{c_{01}-c_{11}}]
\]

\end_inset


\end_layout

\begin_layout Subsection
Bayesian Parameter Estimation
\end_layout

\begin_layout Standard

\series bold
Problem Setting.

\series default
 Hidden parameter 
\begin_inset Formula $X\in\mathcal{X}$
\end_inset

 continuous (RV); Noisy observation 
\begin_inset Formula $Y\in\mathcal{Y}$
\end_inset

 either continuous or discrete; Prior belief 
\begin_inset Formula $P_{X}(x)$
\end_inset

; Likelihood (observation model) 
\begin_inset Formula $P_{Y|X}(y|x)$
\end_inset

; The goal is to construct an estimator 
\begin_inset Formula $\hat{x}(\cdot)$
\end_inset

 that produces an estimate of 
\begin_inset Formula $x$
\end_inset

 given the observation 
\begin_inset Formula $Y=y$
\end_inset

.
 Similar to Bayesian HT, an objective criterion 
\begin_inset Formula $C(a,\hat{a)}$
\end_inset

 is required to build and evaluate this estimator, so
\begin_inset Formula 
\begin{align*}
\hat{x}(\cdot) & =\arg\min_{f(\cdot)}E_{XY}[\tilde{C}(x,f(y))]\\
 & =\arg\min_{a}\int_{-\infty}^{\infty}C(x,a)P_{X|Y}(x|y)dx\\
 & =\arg\min_{a}E_{X|Y}[C(x,a)|Y=y]
\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
Minimum Absolute Error (MAE),
\series default
 
\begin_inset Formula $C(a,\hat{a})=|a-\hat{a}|$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\hat{x}_{MAE}(y) & =\arg\min_{a}\int_{-\infty}^{\infty}|x-a|P_{X|Y}(x|y)dx\\
\Rightarrow & \int_{-\infty}^{a}P_{X|Y}(x|y)dx-\int_{a}^{\infty}P_{X|Y}(x|y)dx=0
\end{align*}

\end_inset

 
\begin_inset Formula $\hat{x}_{MAE}(y)$
\end_inset

 is the MEDIAN of 
\begin_inset Formula $P_{X|Y}$
\end_inset

 – not always unique.
\end_layout

\begin_layout Standard

\series bold
Minimum Uniform Cost,
\series default
 
\begin_inset Formula $C(a,\hat{a})=\mathbf{1}\{|a-\hat{a}|>\epsilon\}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{x}_{MUC}(y)=\arg\max_{a}\int_{a-\epsilon}^{a+\epsilon}P_{X|Y}(x|y)dx
\]

\end_inset

The 
\begin_inset Formula $\hat{x}_{MUC}(y)$
\end_inset

 is the center of the 
\begin_inset Formula $2\epsilon$
\end_inset

 interval with the most mass of 
\begin_inset Formula $P_{X|Y}$
\end_inset

.
 When 
\begin_inset Formula $\epsilon$
\end_inset

 tends to zero, MUC is defined as the MAP estimator, 
\begin_inset Formula $\lim_{\epsilon\rightarrow0}\hat{x}_{MUC}(y)=\arg\max_{a}P_{X|Y}(x|y)\triangleq\hat{x}_{MAP}(y)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Bayes Least Squares
\series default
 (BLS):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
C(\hm{a},\hat{\hm{a}})=\|\hm{a}-\hat{\hm{a}}\|=(a-\hat{a})^{T}(a-\hat{a})
\]

\end_inset

We plug the cost into the optimization problem, then derive the internal
 part and letting the derivative be zero, then
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{\hm{x}}_{BLS}(\hm{y})=\int_{-\infty}^{\infty}xP_{X|Y}dx=E[\hm{X}|\bm{y}]
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Performance Characeteristics.

\series default
 Vector by default.
 
\bar under
Error.

\bar default
 Given an instance 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $e(x,y)=\hat{x}(y)-x$
\end_inset

.
 
\bar under
Bias
\bar default
 
\begin_inset Formula $b=E_{XY}[e(x,y)]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e(x,y)P_{x,y}dxdy$
\end_inset

.
 
\bar under
Variance
\bar default
 
\begin_inset Formula $\Lambda_{e}=E_{xy}\{[e(x,y)-b][e(x,y)-b]^{T}\}$
\end_inset

.
 
\begin_inset Formula $MSE=E_{xy}[e(x,y)e(x,y)^{T}]=\Lambda_{e}+bb^{T}$
\end_inset

.
 (1) 
\series bold

\begin_inset Formula $\hat{x}_{BLS}(y)$
\end_inset

 
\series default
is unbiased, 
\begin_inset Formula $b=0$
\end_inset

.
 (2) 
\begin_inset Formula $\hat{x}=\hat{x}_{BLS}$
\end_inset

 iff 
\begin_inset Formula $e(x,y)$
\end_inset

 is orthogobal to any function of 
\begin_inset Formula $y$
\end_inset

, 
\begin_inset Formula $E_{XY}[(\hat{x}(y)-x)g^{T}(y)]=0$
\end_inset

.
\end_layout

\begin_layout Subsection
Linear Least Squares Estimation
\end_layout

\begin_layout Standard
The previous estimators require complete characterization of 
\begin_inset Formula $P_{X}$
\end_inset

 and 
\begin_inset Formula $P_{Y|X}$
\end_inset

, and posterior is difficult to compute.
\end_layout

\begin_layout Standard

\series bold
Linear Least-Squares Estimation.
\series default

\begin_inset Formula 
\[
\hat{x}_{LLS}=\arg\min_{f(\cdot)}E_{XY}[\|x-f(y)\|^{2}]\ s.t.\ f(y)=\mathbb{A}\ubar y+\ubar d
\]

\end_inset

A linear estimator 
\begin_inset Formula $\hat{x}(y)$
\end_inset

 is LLS iff unbiased and orthogonal to data:
\begin_inset Formula 
\[
E_{XY}[\hat{x}(y)-x]=0\quad E_{XY}[(\hat{x}(y)-x)y^{T}]=0_{N\times N}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Constructing LLS.

\series default
 Use defintion if moment unavailable.
\begin_inset Formula 
\[
\hat{x}_{LLS}(\ubar y)=\ubar\mu_{X}+\Lambda_{XY}\Lambda_{Y}^{-1}(\ubar y-\ubar\mu_{Y})
\]

\end_inset

Error covariance 
\begin_inset Formula $\Lambda_{LLS}=\Lambda_{X}-\Lambda_{XY}\Lambda_{Y}^{-1}\Lambda_{XY}^{T}$
\end_inset

.
 When 
\begin_inset Formula $X$
\end_inset

 and 
\begin_inset Formula $Y$
\end_inset

 are jointly Gaussian, 
\begin_inset Formula $\hat{x}_{LLS}=\hat{x}_{BLS}$
\end_inset

.
\end_layout

\begin_layout Subsection
NonBayesian Parameter Estimation
\end_layout

\begin_layout Standard
\begin_inset Formula $Y$
\end_inset

 is parameterized by 
\begin_inset Formula $x$
\end_inset

, 
\begin_inset Formula $P_{Y|X}(y|x)\rightarrow P_{Y}(y;x)$
\end_inset

.
 An estimator is valid if it does not depend explicitly on the parameter
 we are trying to estimate.
 We want to minimize MSE, i.e., 
\begin_inset Formula $tr(e(y)e(y)^{T})=\Lambda_{e}(x)+b_{\hat{x}}b_{\hat{x}}^{T}$
\end_inset

.
 Estimator 
\begin_inset Formula $\hat{x}(y)$
\end_inset

 is unbiased if 
\series bold

\begin_inset Formula $b_{x}(x)=0$
\end_inset


\series default
.
 
\begin_inset Formula $\Lambda_{e}(x)=\Lambda_{\hat{x}}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Minimum Variance Unbiased Estimators.

\series default
 
\begin_inset Formula 
\[
\hat{x}_{MVU}(y)=\arg\min_{\hat{x}\in\mathcal{A}}\lambda_{\hat{x}}(x)=\arg\min_{\hat{x}\in\mathcal{A}}E_{Y}[e^{2}(y)]\ \forall x
\]

\end_inset

where admissible estimators 
\begin_inset Formula $\mathcal{A}=\{\hat{x}(\cdot)$
\end_inset

: valid and unbiased
\begin_inset Formula $\}$
\end_inset

.
 It might not exist.
\end_layout

\begin_layout Standard

\series bold
Cramer-Rao Bound (CRB).

\series default
 Score function 
\begin_inset Formula $S(y;x)=\frac{\partial}{\partial x}\ln P_{Y}(y;x)$
\end_inset

.
 Fisher Information 
\begin_inset Formula $J_{Y}(x)=E_{Y}[S^{2}(y;x)]=-E_{Y}[\frac{\partial^{2}}{\partial x^{2}}\ln P_{Y}(y;x)]$
\end_inset

.
 If 
\begin_inset Formula $\hat{x}(\cdot)$
\end_inset

 is valid and unbiased, then 
\begin_inset Formula $\lambda_{\hat{x}}\geq\frac{1}{J_{Y}(x)}\ \forall x$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Efficient Estimators.

\series default
 
\begin_inset Formula $\hat{x}(y)=x+\frac{1}{J_{Y}(x)}\frac{\partial}{\partial x}\ln P_{Y}(y;x)$
\end_inset

.
 (1) 
\begin_inset Formula $\hat{x}_{eff}(y)$
\end_inset

 is guaranteed to be unbiased.
 (2) if 
\begin_inset Formula $\hat{x}_{eff}$
\end_inset

 exists and is valid, then it is unique.
 (3) if 
\begin_inset Formula $\hat{x}_{eff}(y)$
\end_inset

 exists, then it meets CRB and is MVU.
\end_layout

\begin_layout Subsection
Maximum Likelihood Estimation
\end_layout

\begin_layout Standard
Proxy for efficient estimator with nice properties:
\begin_inset Formula 
\[
\hat{x}_{ML}(y)=\arg\max_{x\in\mathcal{X}}P_{Y}(y;x)
\]

\end_inset

ML estimator is simpler to compute or numerically approximate.
 Meanwhile, it is intuitive because picking 
\begin_inset Formula $x$
\end_inset

 that gives you the largest chance of observing data 
\begin_inset Formula $Y=y$
\end_inset

.
 If 
\begin_inset Formula $\hat{x}_{eff}(y)$
\end_inset

 exists, then it equals 
\begin_inset Formula $\hat{x}_{ML}(y)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Invertible Mapping.

\series default
 
\begin_inset Formula $\theta=g(x)\rightarrow\hat{\theta}_{ML}(y)=g(\hat{x}_{ML}(y))$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Vector Parameters.

\series default
 Score function 
\begin_inset Formula $\vec{S}_{\vec{Y}}(\vec{x)}=[\frac{\partial\log P_{Y}(\vec{y};x)}{\partial x_{1}},\ldots,\frac{\partial\log P_{Y}(\vec{y};x)}{\partial x_{N}}]$
\end_inset

.
 Fisher information 
\begin_inset Formula $J_{\vec{Y}}(\vec{x})=E_{Y}[\vec{S}_{Y}(\vec{x})^{T}\vec{S}_{Y}(\vec{x})]$
\end_inset

.
 CRB: Covariance matrix 
\begin_inset Formula $\Lambda_{\hat{x}}(x)$
\end_inset

 of any unbiased estimator satisfies the following inequality: 
\begin_inset Formula $\Lambda_{\hat{x}(x)}\geq J_{Y}^{-1}(x)\leftrightarrow(\Lambda_{\hat{x}(x)}-J_{Y}^{-1}(x))$
\end_inset

 is PSD.
 (1) 
\begin_inset Formula $J_{Y}(x)=-E_{Y}[\frac{\partial^{2}}{\partial x^{2}}\ln P_{Y}(y;x)]$
\end_inset

; (2) 
\begin_inset Formula $\hat{x}_{eff}(y)$
\end_inset

 exists if it can be expressed 
\begin_inset Formula $\hat{x}_{eff}(y)=x+J_{Y}^{-1}(x)[\frac{\partial}{\partial x}\ln P_{Y}(y;x)]^{T}$
\end_inset

; (3) if 
\begin_inset Formula $\hat{x}_{eff}(y)$
\end_inset

 exists, then it is the ML estimator.
 
\end_layout

\begin_layout Standard

\series bold
Misc.

\series default
 MAP estimator maximizes the joint distribution.
\end_layout

\begin_layout Subsection
Exponential Families
\end_layout

\begin_layout Standard
A parameterized family of distributions 
\begin_inset Formula $\{Pr(\cdot;x),x\in\mathcal{X}\}$
\end_inset

 over the alphabet 
\begin_inset Formula $\mathcal{Y}$
\end_inset

 is a one-parameter exponential family if it can be written as
\begin_inset Formula 
\[
P_{Y}(y;x)=\exp\{\lambda(x)t(y)-\alpha(x)+\beta(y)\}\ \forall x\in\mathcal{X},y\in\mathcal{Y}
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Construction.

\series default
 I.
 Geometric mean, 
\begin_inset Formula $p_{1}(y)$
\end_inset

 and 
\begin_inset Formula $p_{2}(y)$
\end_inset

 strictly positive on 
\begin_inset Formula $\mathcal{Y}$
\end_inset

.
 
\begin_inset Formula $P_{Y}(y;x)=\frac{1}{Z(x)}p_{1}(y)^{x}p_{2}(y)^{1-x},\ x\in\mathcal{X}=[0,1]$
\end_inset

.
 So 
\begin_inset Formula $\log P_{Y}(y;x)=x\log(\frac{p_{1}(y)}{p_{2}(y)})+\log p_{2}(y)-\log Z(x)$
\end_inset

.
\end_layout

\begin_layout Standard
II.
 Tilting based on distribution 
\begin_inset Formula $q(y)$
\end_inset

.
 
\begin_inset Formula $P_{Y}(y;x)=q(y)e^{xy}/Z(x)$
\end_inset

.
 
\begin_inset Formula $\log P_{Y}(y;x)=xy+\log q(y)-\log Z(x)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Moment Generation.

\series default
 A linear exponential family has 
\begin_inset Formula $\lambda(x)=x$
\end_inset

.
 (1) 
\begin_inset Formula $\alpha'(x)=E_{Y}[t(Y)]$
\end_inset

; (2) 
\begin_inset Formula $\alpha''(x)=Var[t(Y)]$
\end_inset

; (3) 
\begin_inset Formula $J_{Y}(x)=Var[t(Y)]$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Efficient Estimators.

\series default
 If 
\begin_inset Formula $\hat{x}_{eff}$
\end_inset

 exists, then 
\begin_inset Formula $P_{Y}(y;x)$
\end_inset

 is a member of an exponential family with 
\begin_inset Formula $\lambda(x)=\int^{x}J_{Y}(u)du$
\end_inset

 and 
\begin_inset Formula $t(y)=\hat{x}_{ML}(y)=\hat{x}_{eff}(y)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Conjugate Priors.

\series default
 Let 
\begin_inset Formula $Q=\{q(\cdot;\theta):\theta\in\Theta\subset\mathbb{R}^{K}\}$
\end_inset

 be a family of distributions based on 
\begin_inset Formula $K$
\end_inset

 parameters 
\begin_inset Formula $\theta=[\theta_{1},\ldots,\theta_{K}]^{T}$
\end_inset

 such that 
\begin_inset Formula $q(x;\theta)$
\end_inset

 is continuously invertible in 
\begin_inset Formula $\theta$
\end_inset

.
 Then 
\begin_inset Formula $Q$
\end_inset

 is a conjugate prior family for 
\begin_inset Formula $P_{Y|X}$
\end_inset

 if 
\begin_inset Formula $P_{X}(\cdot)\in Q\rightarrow P_{X|Y}(\cdot|y)\in Q$
\end_inset

.
\end_layout

\begin_layout Subsection
Directed Graphical Models
\end_layout

\begin_layout Standard
Defines a family of joint probability distributions over set of RVs.
 The setup if Directed Acyclic Graph 
\begin_inset Formula $\mathcal{G}(\mathcal{V},\mathcal{E})$
\end_inset

 with nodes (RVs) and edges.
 For each node 
\begin_inset Formula $i\in\mathcal{V}$
\end_inset

, let 
\begin_inset Formula $\pi_{i}$
\end_inset

 be the set of parent nodes.
 Then the family of distributions satisfy 
\begin_inset Formula $P_{X}(x_{1},\ldots,x_{n})=\prod_{i=1}^{n}P_{X}(x_{i}|x_{\pi_{i}})$
\end_inset

.
 In this graph, absenceof edge means conditional independence.
\end_layout

\begin_layout Standard

\series bold
Bayes' Ball Algorithm.

\series default
 It determines conditional independence given obserations.
 Examine whether 
\begin_inset Formula $x$
\end_inset

 and 
\begin_inset Formula $z$
\end_inset

 are marginally independent 
\begin_inset Formula $p(x,z)=p(x)p(z)$
\end_inset

, or conditionally independent 
\begin_inset Formula $p(x,z|y)=p(x|y)p(z|y)$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of X] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of Y] (Z) {$Z$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[->] (X) -- (Y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[->] (Y) -- (Z);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Can pass through $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[obs,right=of X] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of Y] (Z) {$Z$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[->] (X) -- (Y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[->] (Y) -- (Z);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Blocked at $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of X,yshift=0.2cm] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of Y,yshift=-0.2cm] (Z) {$Z$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[<-] (X) -- (Y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[->] (Y) -- (Z);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Can pass through $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[obs,right=of X,yshift=0.2cm] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of Y,yshift=-0.2cm] (Z) {$Z$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[<-] (X) -- (Y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[->] (Y) -- (Z);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Blocked at $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of X,yshift=-0.2cm] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of Y,yshift=0.2cm] (Z) {$Z$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[->] (X) -- (Y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[<-] (Y) -- (Z);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Blocked at $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[obs,right=of X,yshift=-0.2cm] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of Y,yshift=0.2cm] (Z) {$Z$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[->] (X) -- (Y);
\end_layout

\begin_layout Plain Layout


\backslash
draw[<-] (Y) -- (Z);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Can pass through $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (Z) {?};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of Z] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of X] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[<->] (Z) -- (X);
\end_layout

\begin_layout Plain Layout


\backslash
draw[<-] (Y) -- (X);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Ball stops at leaf $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (Z) {?};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of Z] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[obs,right=of X] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[<->] (Z) -- (X);
\end_layout

\begin_layout Plain Layout


\backslash
draw[<-] (Y) -- (X);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Ball bounces back at $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[latent,right=of X] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[<-] (X) -- (Y);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Ball bounces back at root $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (X) {$X$};
\end_layout

\begin_layout Plain Layout


\backslash
node[obs,right=of X] (Y) {$Y$};
\end_layout

\begin_layout Plain Layout


\backslash
draw[<-] (X) -- (Y);
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}
\end_layout

\begin_layout Plain Layout

~ Ball stops at root $Y$.
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Plate Notation.
 
\series default
See 
\begin_inset CommandInset href
LatexCommand href
name "wikipedia."
target "https://en.wikipedia.org/wiki/Plate_notation"
literal "false"

\end_inset

 Circles: random variable; Squares: non-random parameters.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture}[thick]
\end_layout

\begin_layout Plain Layout


\backslash
node[latent] (Xk) {$X_k$};
\end_layout

\begin_layout Plain Layout


\backslash
node[obs,below=of Xk] (Ynk) {$Y_{nk}$};
\end_layout

\begin_layout Plain Layout


\backslash
node[const,right=of Xk] (tx) {$
\backslash
theta_x$};
\end_layout

\begin_layout Plain Layout


\backslash
node[const,right=of Ynk] (ty) {$
\backslash
theta_y$};
\end_layout

\begin_layout Plain Layout


\backslash
plate {p1} {(Ynk)} {N};
\end_layout

\begin_layout Plain Layout


\backslash
plate {p2} {(Xk)(p1)} {K};
\end_layout

\begin_layout Plain Layout


\backslash
draw[->,>=latex] (Xk) -- (Ynk);
\end_layout

\begin_layout Plain Layout


\backslash
draw[->,>=latex] (tx) -- (Xk);
\end_layout

\begin_layout Plain Layout


\backslash
draw[->,>=latex] (ty) -- (Ynk);
\end_layout

\begin_layout Plain Layout


\backslash
node[right=of tx,align=left] (a1) {$p(x_
\backslash
cdot,y_
\backslash
cdot)$
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$=
\backslash
prod_k [p_X(x_k;
\backslash
theta_k)$
\backslash

\backslash

\end_layout

\begin_layout Plain Layout

$
\backslash
prod_n p_{Y|X}(y_{nk}|x_k;
\backslash
theta_y)]$};
\end_layout

\begin_layout Plain Layout


\backslash
end{tikzpicture}%
\end_layout

\end_inset


\end_layout

\begin_layout Subsection
Mixture Models
\end_layout

\begin_layout Standard

\series bold
K-Means.

\series default
 Centroid of 
\begin_inset Formula $k$
\end_inset

-th cluster 
\begin_inset Formula $\mu_{k}$
\end_inset

.
 Cluster assignment of the 
\begin_inset Formula $n$
\end_inset

-th point 
\begin_inset Formula $Z_{n}\in\{1,\ldots,K\}$
\end_inset

.
 We define 
\begin_inset Formula $\ubar{Z}_{n}=[Z_{n}^{1},\ldots,Z_{n}^{k}]$
\end_inset

 as binary indicator vector.
 (1) Given centroids, we can assign clusters as 
\begin_inset Formula $Z_{n}^{k}=\bm{1}\{k=\arg\min_{k'}\|y_{n}-\mu_{k'}\|\}$
\end_inset

.
 (2) Given assignments, we compute the centroids as 
\begin_inset Formula $\mu_{k}=(\sum_{n}Z_{n}^{k}y_{n})/\sum_{n}Z_{n}^{k}$
\end_inset

.
 In this algorithm, we are actually doing coordinate descent to minimize
 the L2 objective 
\begin_inset Formula $J=\sum_{n}\sum_{k}Z_{n}^{k}\|y_{n}-\mu_{k}\|^{2}$
\end_inset

.
 This leads to solution to a Gaussian mixture model w/ equal variances.
\end_layout

\begin_layout Standard

\series bold
Mixture Model.

\series default
 Observe i.i.d.
 scalars 
\begin_inset Formula $Y_{1},\ldots,Y_{N}$
\end_inset

.
 Then 
\begin_inset Formula $P_{Y}(Y_{1},\ldots,Y_{N}|\theta)=\prod_{n}\sum_{k}\pi_{k}p_{k}(y_{n};\theta_{k})$
\end_inset

.
 Goal is to find ML parameter estimates for 
\begin_inset Formula $\pi_{k}$
\end_inset

 and 
\begin_inset Formula $\theta_{k}$
\end_inset

.
 The log likelihood is 
\begin_inset Formula $\ell(\ubar Y,\theta)=\sum_{n}\log(\sum_{k}\pi_{k}p_{k}(y_{n};\theta_{k}))$
\end_inset

.
 This is not easy to optimize.
 We introduce auxilliary one-hot RV 
\begin_inset Formula $Z_{n}$
\end_inset

,
\begin_inset Formula 
\[
P(Y_{1},\ldots,Y_{N},Z_{1},\ldots,Z_{N};\theta)=\prod_{n}\prod_{k}[\pi_{k}p_{k}(y_{n};\theta_{k})]^{Z_{n}^{k}}
\]

\end_inset

and the corresponding 
\begin_inset Formula $\ell_{c}(\ubar Y,\ubar Z;\theta)$
\end_inset

 is easier to optimize as the summation inside log has been eliminated.
 Then we want to maximize the lower bound of 
\begin_inset Formula $\ell_{c}(\ubar y,\theta)$
\end_inset

, namely 
\begin_inset Formula $E_{Z|Y}[\ell_{c}(\ubar y,\ubar z;\theta)]$
\end_inset

.
 We let the posterior 
\begin_inset Formula $\tau_{n}^{k}=E[Z_{n}^{k}]$
\end_inset

.
\begin_inset Formula 
\[
E_{Z|Y}[\ell_{c}(y,z;\theta)]=\sum_{n=1}^{N}\sum_{k=1}^{K}E[z_{n}^{k}](\log\pi_{k}+\log p_{k}(y_{n};\theta_{k}))
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Clustering Algorithm.

\series default
 Step (E): find posterior 
\begin_inset Formula $\{\tau_{n}^{k}\}$
\end_inset

 given 
\begin_inset Formula $\ubar y$
\end_inset

 and parameters 
\begin_inset Formula $\{\pi_{*},\theta_{*}\}$
\end_inset

.
\begin_inset Formula 
\begin{align*}
\tau_{n}^{k} & =E_{Z|Y}[Z_{n}^{k}]=Pr(Z_{n}^{k}=1|y_{n};\theta)\\
 & =\frac{Pr(Z_{n}=k)p_{k}(y_{n};\theta_{k})}{P_{Y}(y_{n};\theta)}=\frac{\pi_{k}p_{k}(y_{n};\theta_{k})}{\sum_{k'}\pi_{k'}p_{k'}(y_{n};\theta_{k})}
\end{align*}

\end_inset

Step (M): Find mixing weights 
\begin_inset Formula $\{\pi_{k}\}$
\end_inset

 given 
\begin_inset Formula $\ubar y$
\end_inset

, 
\begin_inset Formula $\{\tau_{n}^{k}\}$
\end_inset

 and 
\begin_inset Formula $\{\theta_{*}\}$
\end_inset

 using Lagrangian,
\begin_inset Formula 
\[
\phi(\ubar y,\ubar z;\theta)=E_{Z|Y}[\ell(\ubar y,\ubar z;\theta)]-\lambda(\sum_{k}\pi_{k}-1)
\]

\end_inset

And setting 
\begin_inset Formula $\partial\phi/\partial\pi_{k}$
\end_inset

 to zero yields 
\begin_inset Formula $\pi_{k}=\frac{1}{N}\sum_{n}\tau_{n}^{k}$
\end_inset

.
 Then we optimize 
\begin_inset Formula $\{\theta_{*}\}$
\end_inset

 based on specified densities.
\end_layout

\begin_layout Standard

\series bold
Gaussian Mixture.

\series default
 
\begin_inset Formula $p_{k}(y_{n};\theta_{k})=\mathcal{N}(y_{n};\mu_{k},\sigma_{k}^{2})$
\end_inset

.
\begin_inset Formula 
\[
\mu_{k}=\frac{\sum_{n}\tau_{n}^{k}y_{n}}{\sum_{n}\tau_{n}^{k}}\qquad\sigma_{k}^{2}=\frac{\sum_{n}\tau_{n}^{k}(y_{n}-\mu_{k})^{2}}{\sum_{n}\tau_{n}^{k}}
\]

\end_inset


\end_layout

\begin_layout Subsection
Generalized Mixture Model 
\end_layout

\begin_layout Standard

\series bold
Setup.

\series default
 Observed data 
\begin_inset Formula $y\in\mathcal{Y}$
\end_inset

, unknown (deterministic) parameters 
\begin_inset Formula $x\in\mathcal{X}$
\end_inset

, latent/hidden random variables 
\begin_inset Formula $z\in\mathcal{Z}$
\end_inset

.
 Objective: 
\begin_inset Formula $\hat{x}=\arg\max_{x}\log P_{Y}(y;x)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Jensen's Inequality.
\series default

\begin_inset Formula 
\[
\log(\lambda z_{1}+(1-\lambda)z_{2})\geqslant\lambda\log(z_{1})+(1-\lambda)\log(z_{2})
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Construct Lower Bound.

\series default
 Introduce 
\begin_inset Formula $q(z|y)$
\end_inset

 an arbitrary distribution.
 
\begin_inset Formula $\ell(y;x)=\log P_{Y}(y;x)=\log(\sum_{z}P_{Y,Z}(y,z;x))=\log(\sum_{z}q(z|y)\frac{P_{Y,Z}(y,z;x)}{q(z|y)})\geqslant\sum_{z}q(z|y)\log(\frac{P_{Y,Z}(y,z;x)}{q(z|y)})\triangleq\mathcal{L}(q,x)$
\end_inset

.
 
\end_layout

\begin_layout Standard

\series bold
EM Algorithm.

\series default
 Coordinate ascent on 
\begin_inset Formula $q(\cdot|y)$
\end_inset

 and 
\begin_inset Formula $x$
\end_inset

 at 
\begin_inset Formula $(t+1)^{st}$
\end_inset

 iteration.
 E-step 
\begin_inset Formula $q^{(t+1)}=\arg\max_{q(\cdot)}\mathcal{L}(q,x^{(t)})$
\end_inset

.
 M-step 
\begin_inset Formula $x^{(t+1)}=\arg\max_{x}\mathcal{L}(q^{(t+1)},x)$
\end_inset

.
 Step order can be switched depending on initialization.
\end_layout

\begin_layout Standard

\series bold
M-Step.

\series default
 
\begin_inset Formula $x^{(t+1)}=\arg\max_{x}\sum_{z}q^{(t+1)}(z|y)[\log P(y,z;x)-\log q^{(t+1)}(z|y)]$
\end_inset

 
\begin_inset Formula $=\arg\max_{x}\sum_{z}q^{(t+1)}(z|y)\log P(y,z;x)=\arg\max_{x}E_{q^{(t+1)}}[\log P(y,z|x)]$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
E-Step.

\series default
 
\begin_inset Formula $\mathcal{L}(q,x^{(t)})=\sum_{z}q(z|y)\log(\frac{p_{Y}(y;x^{(t)})p_{Z|Y}(z|y;x^{(t)})}{q(z|y)})=\sum_{z}q(z|y)\log(p_{Y}(y;x^{(t)}))+\sum_{z}q(z|y)\log(\frac{p_{Z|Y}(z|y;x^{(t)})}{q(z|y)})$
\end_inset

.
 The left item 
\begin_inset Formula $=\log P_{Y}(y;x^{(t)})\sum_{z}q(z|y)=\ell(y;x^{(t)})$
\end_inset

.
 Gibb's Inequality: 
\begin_inset Formula $E_{p}[\log p(z)]\geqslant E_{q}[\log p(z)]$
\end_inset

 with equality iff 
\begin_inset Formula $p(z)=q(z)$
\end_inset

.
 So right item 
\begin_inset Formula $<0$
\end_inset

 unless 
\begin_inset Formula $q(z|y)=p(z|y;x^{(t)})$
\end_inset

.
 So the solution is 
\begin_inset Formula $q^{(t+1)}(z|y)=P_{Z|Y}(z|y;x^{(t)})$
\end_inset

, and 
\begin_inset Formula $\mathcal{L}(q^{(t+1)},x^{(t)})=\log P_{Y}(y;x^{(t)})$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
MAP Estimation.

\series default
 
\begin_inset Formula $\hat{x}_{MAP}(y)=\arg\max_{x}p(x|y)=\arg\max_{x}\frac{p(x,y)}{p(y)}=\arg\max_{x}p(x,y)$
\end_inset

.
\end_layout

\begin_layout Subsection
Deterministic Approximations
\end_layout

\begin_layout Standard
The goal of 
\begin_inset Quotes eld
\end_inset

belief approximation
\begin_inset Quotes erd
\end_inset

 is to find a simpler distribution 
\begin_inset Formula $q(\cdot)$
\end_inset

 that is sufficiently 
\begin_inset Quotes eld
\end_inset

close
\begin_inset Quotes erd
\end_inset

 to posterior 
\begin_inset Formula $P_{X|Y}(\cdot|y)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
KL Divergence.
 
\series default
Given a true distribution 
\begin_inset Formula $p(\cdot)$
\end_inset

, and an approximating distribution 
\begin_inset Formula $q(\cdot)$
\end_inset

,
\begin_inset Formula 
\[
D(p\|q)=E_{p}[\log\frac{p(x)}{q(x)}]=\sum_{x\in\mathcal{X}}p(x)\log\frac{p(x)}{q(x)}
\]

\end_inset

Gibb's inequality 
\begin_inset Formula $\rightarrow$
\end_inset

 
\begin_inset Formula $D(p\|q)=E_{p}[\log p(x)]-E_{p}[\log q(x)]\geq0$
\end_inset

.
 Only when 
\begin_inset Formula $p=q$
\end_inset

, 
\begin_inset Formula $D(p\|q)=0$
\end_inset

.
 Besides, 
\begin_inset Formula $D(p\|q)\neq D(q\|p)$
\end_inset

.
 
\begin_inset Formula $I(X,Y)=D(P_{XY}\|P_{X}P_{Y})$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Laplace's Method.
\series default

\begin_inset Formula 
\[
\hat{P}_{X}(x)=\frac{\hat{P}_{0}(x)}{Z_{\hat{P}}}=P_{0}(x)\exp\{-\frac{1}{2}J(\hat{x})(x-\hat{x})^{2}\}
\]

\end_inset

where 
\begin_inset Formula $-J(\hat{x})=\frac{d^{2}}{dx^{2}}\log P_{0}(x)|_{x=\hat{x}}$
\end_inset

 is the observed fisher info, partition function 
\begin_inset Formula $Z_{p}=P_{0}(x)\sqrt{2\pi(1/J(\hat{x}))}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
P_{X|Y}(x|y)\approx\mathcal{N}(x;\hat{x}_{MAP}(y),\hat{\sigma}^{2}(y))
\]

\end_inset


\begin_inset Formula 
\[
\hat{\sigma}^{2}(y)=[-\frac{\partial^{2}}{\partial x^{2}}\log P_{X}(x)|_{\hat{x}_{MAP}}-\frac{\partial^{2}}{\partial x^{2}}\log P_{Y|X}(y|x)|_{\hat{x}_{MAP}}]^{-1}
\]

\end_inset

The empirical fisher info is 
\begin_inset Formula $J_{Y=y}(\hat{x}_{MAP}(y))$
\end_inset

.
 For large dataset use 
\begin_inset Formula $\hat{x}_{ML}$
\end_inset

 instead.
\end_layout

\begin_layout Standard

\series bold
Variational Methods.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\hat{p}(x) & =\arg\min_{q\in Q}D(q\|p_{X|Y})\\
\hat{p}(x) & =\arg\min_{q\in Q}D(q\|p_{X,Y})
\end{align*}

\end_inset

And variational free energy is 
\begin_inset Formula $\mathcal{FE}=D(q||p_{XY})$
\end_inset

.
\end_layout

\begin_layout Subsection
Stochastic Approximations
\end_layout

\begin_layout Standard
Goal: approximate 
\begin_inset Formula $E_{p}[f(x)]$
\end_inset

 for general 
\begin_inset Formula $f(\cdot)$
\end_inset

.
\end_layout

\begin_layout Standard
Approach: suppose we had samples of 
\begin_inset Formula $x_{1},\ldots,x_{n}\sim iid\,p(x)$
\end_inset

.
\begin_inset Formula 
\begin{align*}
E_{p}[f(x)] & \approx\hat{f}=\frac{1}{n}\sum_{i=1}^{n}f(x_{i})\\
E_{p}[\hat{f}] & =\frac{1}{n}\sum_{i=1}^{n}E_{p}[f(x_{i})]=E_{p}[f(x)]\\
Var[\hat{f}] & =\frac{1}{n^{2}}\sum_{i=1}^{n}Var[f(x_{i})]=\frac{Var[f(x)]}{n}\rightarrow_{n\rightarrow\infty}0
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
We obtain samples from 
\begin_inset Formula $q(\cdot)$
\end_inset

, which is easy to handle, and transform them into samples of 
\begin_inset Formula $p(\cdot)$
\end_inset

.
\begin_inset Formula 
\[
q(x)>0\ \forall\ x\in\mathcal{X}\ s.t.\ p(x)>0
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Importance Sampling.

\series default
 We call 
\begin_inset Formula $q(\cdot)$
\end_inset

 as 
\begin_inset Quotes eld
\end_inset

proposal
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

sampling
\begin_inset Quotes erd
\end_inset

 distribution.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
q(x)=\frac{q_{0}(x)}{Z_{q}}=\frac{q_{0}(x)}{\int_{x\in\mathcal{X}}q_{0}(x)dx}\qquad w(x_{i})=\frac{p_{0}(x_{i})}{q_{0}(x_{i})}
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{f}\triangleq\sum_{i=1}^{n}\frac{w(x_{i})}{\sum_{j=1}^{n}w(x_{j})}f(x_{i})
\]

\end_inset


\end_layout

\begin_layout Standard

\series bold
Rejection Sampling.

\series default
 Draw samples from 
\begin_inset Formula $p(\cdot)$
\end_inset

, with unnormalized 
\begin_inset Quotes eld
\end_inset

proposal distribution
\begin_inset Quotes erd
\end_inset

 
\begin_inset Formula $q_{0}(\cdot)$
\end_inset

 where 
\begin_inset Formula $cq_{0}(x)>p_{0}(x)$
\end_inset

 and 
\begin_inset Formula $c$
\end_inset

 is constant.
 (1) sample 
\begin_inset Formula $x\sim q(\cdot)$
\end_inset

.
 (2) sample 
\begin_inset Formula $u\sim U[0,cq_{0}(x)]$
\end_inset

.
 (3) keep sample if 
\begin_inset Formula $u\leq p_{0}(x)$
\end_inset

, otherwise discard.
\end_layout

\begin_layout Standard

\series bold
Markov-Chain Monte Carlo (MCMC).

\series default
 Generate samples from 
\begin_inset Formula $p(\cdot)$
\end_inset

, and does not require proposal distribution 
\begin_inset Formula $q(\cdot)$
\end_inset

 to be close to 
\begin_inset Formula $p(\cdot)$
\end_inset

.
 It produce correlated samples.
 Metropolis-Hastings: proposal distribution at time 
\begin_inset Formula $(n+1)$
\end_inset

 is 
\begin_inset Formula $q(\cdot;x_{n})$
\end_inset

, parametrized by previous state.
 (1) given 
\begin_inset Formula $x_{n}$
\end_inset

, generate candidate 
\begin_inset Formula $x^{\prime}$
\end_inset

 from 
\begin_inset Formula $q(\cdot;x_{n})$
\end_inset

.
 (2) compute acceptance probability 
\begin_inset Formula $\alpha(x_{n}\rightarrow x^{\prime})=\min\{1,\frac{p_{0}(x^{\prime})q(x_{n};x^{\prime})}{p_{0}(x_{n})q(x^{\prime};x_{n})}\}$
\end_inset

.
 (3) transition to 
\begin_inset Formula $x^{\prime}$
\end_inset

 wp.
 
\begin_inset Formula $\alpha$
\end_inset

 otherwise stay in 
\begin_inset Formula $x_{n}$
\end_inset

.
 State transition probability is hence 
\begin_inset Formula $q(\cdot;x_{n})\alpha(x_{n};x)$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Gibbs Sampling.
 
\series default

\begin_inset Formula 
\[
p(x)\alpha(x;y)q(y;x)=p(x)\min\{1,\frac{p(y)q(x;y)}{p(x)q(y;x)}\}q(y;x)
\]

\end_inset


\end_layout

\begin_layout Subsection
Hidden Markov Models
\end_layout

\begin_layout Standard
Graphical Models -> (Discrete states from ML perspective) -> HMMs; Parametrizati
on for Homogeneous HMM does not depend on time.
\end_layout

\begin_layout Standard
Binary indicator vector and initial state probabilities
\begin_inset Formula 
\[
q_{t}=[q_{t}^{1},\ldots,q_{t}^{M}]^{T}\qquad\pi=[\pi^{1},\ldots,\pi^{M}]^{T}\qquad p(q_{0}^{i}=1)=\pi^{i}
\]

\end_inset


\end_layout

\begin_layout Standard
We augment the prior 
\begin_inset Formula $\pi$
\end_inset

 w/ 
\begin_inset Formula $M\times M$
\end_inset

 transition probability matrix 
\begin_inset Formula $\mathbb{\mathbf{A}}$
\end_inset

, where 
\begin_inset Formula 
\[
a_{ij}=p(q_{t+1}=j|q_{t}=i)
\]

\end_inset


\end_layout

\begin_layout Standard
With 
\begin_inset Formula $q$
\end_inset

 as hidden state, we observe 
\begin_inset Formula $y$
\end_inset

 .
 The likelihood is 
\begin_inset Formula $p(y_{t}|q_{t};\eta)$
\end_inset

.
 And the joint density of HMM is
\begin_inset Formula 
\[
p(\bm{q},\bm{y})=p(q_{0})\prod_{t=0}^{T-1}p(q_{t+1}|q_{t})\prod_{t=0}^{T}p(y_{t}|q_{t};\eta)
\]

\end_inset


\end_layout

\begin_layout Subsection
Kalman Filtering
\end_layout

\begin_layout Standard
Graphical Models -> (Continuous states from SP perspective) -> Kalman Filter
 (Tracking / Online Learning).
\end_layout

\begin_layout Standard

\series bold
Kalman Filtering.

\series default
 Vector 
\begin_inset Formula $x_{t}$
\end_inset

 and 
\begin_inset Formula $y_{t}$
\end_inset

 are true and measured position at time 
\begin_inset Formula $t$
\end_inset

 respectively.
 
\begin_inset Formula $u_{t}$
\end_inset

 is driving input, deterministic and known.
\begin_inset Formula 
\begin{align*}
x_{t} & =A_{t}x_{t-1}+B_{t}u_{t}+\mathcal{E}_{t} & \mathcal{E}_{t}\sim iid\ \mathcal{N}(0,Q_{t})\\
y_{t} & =C_{t}x_{t}+D_{t}u_{t}+\delta_{t} & \delta_{t}\sim iid\ \mathcal{N}(0,R_{t})
\end{align*}

\end_inset

(1) Predict (prime): compute 
\begin_inset Formula $p(x_{t}|y_{1:t-1},u_{1:t};\theta)$
\end_inset

 
\begin_inset Formula $\sim\mathcal{N}$
\end_inset

(
\begin_inset Formula $\mu_{t}^{\prime}$
\end_inset

, 
\begin_inset Formula $\Sigma_{t}^{\prime}$
\end_inset

).
 
\begin_inset Formula $\mu_{t}^{\prime}=E[x_{t}]=A_{t}\hat{\mu}_{t-1}+B_{t}\mu_{t}$
\end_inset

.
 
\begin_inset Formula $\Sigma_{t}^{\prime}=E[(x_{t}-\mu_{t}^{\prime})(x_{t}-\mu_{t}^{\prime})^{T}]=A_{t}\hat{\Sigma}_{t-1}A_{t}^{T}+Q_{t}$
\end_inset

.
 (2) Refine to 
\begin_inset Formula $p(\hat{x}_{t}|y_{1:t};\theta)$
\end_inset

 (hat).
 For Gaussians, BLS=LLS=
\begin_inset Formula $\hat{\mu}_{t}$
\end_inset

.
 So 
\begin_inset Formula $\hat{\mu}_{t}=\mu_{t}^{\prime}+\Lambda_{XY}\Lambda_{Y}^{-1}(y_{t}-E[y_{t}])$
\end_inset

, where 
\begin_inset Formula $E[y_{t}]=C_{t}\mu_{t}^{\prime}+D_{t}u_{t}$
\end_inset

, 
\begin_inset Formula $\Lambda_{Y}=C_{t}\Sigma_{t}^{\prime}C_{t}^{T}+R_{t}$
\end_inset

, 
\begin_inset Formula $\Lambda_{XY}=E[(x_{t}-\mu_{t}^{\prime})(y-E[y_{t}])^{T}]=\Sigma_{t}^{\prime}C_{t}^{T}$
\end_inset

.
 Let Kalman Gain Matrix be 
\begin_inset Formula $\mathbb{K}_{t}=\Sigma_{t}^{\prime}C_{t}^{T}[C_{t}\Sigma_{t}^{\prime}C_{t}^{T}+R_{t}]^{-1}$
\end_inset

, and residual 
\begin_inset Formula $r_{t}=y_{t}-C_{t}\hat{\mu}_{t-1}-D_{t}u_{t}$
\end_inset

.
 Hence 
\begin_inset Formula $\hat{\mu}_{t}=\mu_{t}^{\prime}+\mathbb{K}_{t}r_{t}$
\end_inset

.
 
\begin_inset Formula $\hat{\Sigma}_{t}=\Sigma_{t}^{\prime}-\Lambda_{XY}\Lambda_{Y}^{-1}\Lambda_{XY}^{T}=(I-\mathbb{K}_{t}C_{t})\Sigma_{t}^{\prime}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Extended Kalman Filter.

\series default
 Non-linear function 
\begin_inset Formula $g(\cdots)$
\end_inset

 and 
\begin_inset Formula $h(\cdots)$
\end_inset

.
 
\begin_inset Formula $x_{t}=g(x_{t-1},u_{t})+\mathcal{E}_{t}$
\end_inset

, 
\begin_inset Formula $y_{t}=h(x_{t},u_{t})+\delta_{t}$
\end_inset

.
 
\begin_inset Formula $\{x_{t},y_{t}\}$
\end_inset

 are nolonger jointly Gaussian, but we can linearly approximate.
 (1) Let Jacobian 
\begin_inset Formula $G_{ij}=\frac{\partial g_{i}(x,u)}{\partial x_{j}}$
\end_inset

 and 
\begin_inset Formula $G_{t}=G|_{x=\hat{\mu_{t-1}}}$
\end_inset

, 
\begin_inset Formula $x_{t}\approx g(\hat{\mu}_{t-1},u_{t})+G_{t}(x_{t-1}-\hat{\mu}_{t-1})+\mathcal{E}_{t}$
\end_inset

.
 So 
\begin_inset Formula $\mu_{t}^{\prime}=g(\hat{\mu}_{t-1},u_{t})$
\end_inset

.
 
\begin_inset Formula $\Sigma_{t}^{\prime}=G_{t}\hat{\Sigma}_{t}G_{t}^{T}+Q_{t}$
\end_inset

.
 (2) Let Jacobian 
\begin_inset Formula $H_{ij}=\frac{\partial h_{i}(x,u)}{\partial x_{j}}$
\end_inset

 and 
\begin_inset Formula $H_{t}=H_{x=\mu_{t}^{\prime}}$
\end_inset

, 
\begin_inset Formula $y_{t}\approx h(\mu_{t}^{\prime},u_{t})+H_{t}(x_{t}-\mu_{t}^{\prime})+\delta_{t}$
\end_inset

.
 So 
\begin_inset Formula $\mathbb{K}_{t}=\Sigma_{t}^{\prime}H_{t}^{T}(H_{t}\Sigma_{t}^{\prime}H_{t}^{T}+R_{t})^{-1}$
\end_inset

, 
\begin_inset Formula $\hat{\mu}_{t}=\mu_{t}^{\prime}+\mathbb{K}_{t}(y_{t}-h(\mu_{t}^{\prime},u_{t}))$
\end_inset

, 
\begin_inset Formula $\hat{\Sigma}_{t}=(I=\mathbb{K}_{t}H_{t})\Sigma_{t}^{\prime}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Conjugate Priors.

\series default
 (successive belief revision).
 Let 
\begin_inset Formula $Q=\{q(\cdot;\theta);\theta\in\mathbb{R}^{K}\}$
\end_inset

 denote a family of distributions specified by some dimension 
\begin_inset Formula $K$
\end_inset

.
 
\begin_inset Formula $Q$
\end_inset

 is a conjugate prior family for the above iid model if 
\begin_inset Formula $\forall y\in\mathcal{Y}$
\end_inset

, 
\begin_inset Formula $p_{X}(\cdot)\in Q\rightarrow P_{X|Y}(\cdot|y)\in Q$
\end_inset

.
 If 
\begin_inset Formula $\mathcal{X}$
\end_inset

 is finite, then a conjugate prior family always exists.
 Namely 
\begin_inset Formula $q(\cdot)$
\end_inset

 is categorical of dimension 
\begin_inset Formula $|\mathcal{X}|$
\end_inset

.
 Suppose data is coming sequentially,
\begin_inset Formula 
\[
p_{X|Y_{1}}=\frac{p_{X}p_{Y_{1}|X}}{\int_{x}P_{X}P_{Y_{1}|X}dx}\quad P_{X|Y_{1},Y_{2}}=\frac{p_{X|Y_{1}}p_{Y_{2}|X,Y_{1}}}{\int_{x}p_{X|Y_{1}}p_{Y_{2}|X,Y_{1}}dx}
\]

\end_inset

The 
\begin_inset Formula $X$
\end_inset

 separates 
\begin_inset Formula $Y_{1}$
\end_inset

 and 
\begin_inset Formula $Y_{2}$
\end_inset

 so 
\begin_inset Formula $Y_{1}$
\end_inset

 can be removed from terms involving 
\begin_inset Formula $Y_{2}$
\end_inset

 from above right side Eq.
\end_layout

\begin_layout Subsection
Dirichlet Process
\end_layout

\begin_layout Standard

\series bold
Probability Simplex.

\series default
 Categorical 
\begin_inset Formula $Z\in\{1,\ldots,K\}$
\end_inset

.
 
\begin_inset Formula $P_{Z}=[P_{Z}(1),\ldots,P_{Z}(K)]^{T}$
\end_inset

 s.t.
 
\begin_inset Formula $P_{Z}(k)\geq0$
\end_inset

 and 
\begin_inset Formula $\sum P_{Z}(k)=1$
\end_inset

.
 It lies on an affine hyperplane of dimension 
\begin_inset Formula $(K-1)$
\end_inset

 known as probability simplex.
 More 
\begin_inset Quotes eld
\end_inset

uniform
\begin_inset Quotes erd
\end_inset

 distributions lie at the center, while 
\begin_inset Quotes eld
\end_inset

skewed
\begin_inset Quotes erd
\end_inset

 distribution concentrate on edges or at vertices.
\end_layout

\begin_layout Standard

\series bold
Dirichlet Distribution.

\series default
 Continuous-valued distribution w/ support over 
\begin_inset Formula $P_{K}$
\end_inset

.
 
\begin_inset Formula $X=[x_{1},\ldots,x_{K}]^{T}$
\end_inset

, 
\begin_inset Formula $\forall x_{k}\geq0$
\end_inset

, 
\begin_inset Formula $\sum_{k}x_{k}=1$
\end_inset

; 
\begin_inset Formula $\alpha=[\alpha_{1},\ldots,\alpha_{K}]^{T}$
\end_inset

, 
\begin_inset Formula $\alpha_{k}\geq0$
\end_inset

.
\begin_inset Formula 
\begin{align*}
Dir(X;\alpha)=\frac{1}{B(\alpha)}\prod_{k=1}^{K}x_{k}^{\alpha_{k}-1}\quad & B(\alpha)=\frac{\prod_{k=1}^{K}\Gamma(\alpha_{k})}{\Gamma(\sum_{k}\alpha_{k})}\\
\Gamma(\alpha)=\int_{0}^{\infty}x^{\alpha-1}e^{-x}dx\quad & \Gamma(\alpha+1)=\alpha\Gamma(\alpha)
\end{align*}

\end_inset

We define 
\begin_inset Formula $\alpha_{0}=\sum_{k}\alpha_{k}$
\end_inset

 which controls the strength or 
\begin_inset Quotes eld
\end_inset

concentration
\begin_inset Quotes erd
\end_inset

 of the distribution.
 Ratio among 
\begin_inset Formula $\{\alpha_{1},\ldots,\alpha_{K}\}$
\end_inset

 controls peak location.
 
\begin_inset Formula $E[x_{k}]=\frac{\alpha_{k}}{\alpha_{0}}$
\end_inset

; 
\begin_inset Formula $Var(x_{k})=\frac{\alpha_{k}(\alpha_{0}-\alpha_{k})}{\alpha_{0}^{2}(\alpha_{0}+1)}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Multinomial Conjugacy.

\series default
 
\begin_inset Formula $p_{X}(x;\alpha)=Dir(x;\alpha)$
\end_inset

.
 
\begin_inset Formula $Z\in\{1,\ldots,K\}$
\end_inset

 and 
\begin_inset Formula $P(z=k|x)=x_{k}$
\end_inset

, 
\begin_inset Formula $p(z|x)=\prod_{k}(x_{k})^{z_{k}}$
\end_inset

.
 Observe iid 
\begin_inset Formula $z_{N}$
\end_inset

, 
\begin_inset Formula $p(x,z)=p_{X}(x;\alpha)\prod_{k}p(z_{i}|x)$
\end_inset

.
 So 
\begin_inset Formula $p(x|z)=Dir(x;\{\alpha_{k}+\sum_{i=1}^{N}z_{i}^{k}\}_{k})$
\end_inset

.
 Observing data changes both the center and concentration of underlying
 parameter.
\end_layout

\begin_layout Standard

\series bold
Dirichlet Process.

\series default
 Denote 
\begin_inset Formula $G\sim DP(\alpha,H)$
\end_inset

.
 
\begin_inset Formula $P(\theta_{k};\lambda)\triangleq H(\theta_{k})$
\end_inset

, 
\begin_inset Formula $X\sim Dir(\frac{\alpha}{K},\ldots,\frac{\alpha}{K})$
\end_inset

.
 
\begin_inset Formula $G(\theta)=\sum_{k=1}^{K}x_{k}\delta_{\theta_{k}}(\theta)$
\end_inset

.
 
\begin_inset Formula $G(\cdot)$
\end_inset

 is mixture of delta funcs centered on 
\begin_inset Formula $\{\theta_{k}\}$
\end_inset

.
 
\begin_inset Formula $Pr(\bar{\theta}_{i}=\theta_{k})=x_{k}$
\end_inset

.
 Sampling will always result in 
\begin_inset Formula $K$
\end_inset

 clusters.
 Dir process is Distribution over probability measures 
\begin_inset Formula $G:\theta\mapsto R^{+}$
\end_inset

 defined by 
\begin_inset Formula $[G(T_{1}),\ldots,G(T_{K})]^{T}\sim Dir(\alpha H(T_{1}),\ldots,\alpha H(T_{K}))$
\end_inset

 for any partition 
\begin_inset Formula $\{T_{1},\ldots,T_{K}\}$
\end_inset

 of parameter domain 
\begin_inset Formula $\theta$
\end_inset

.
 
\end_layout

\begin_layout Standard

\series bold
Extend Dirichlet Conjugacy.

\series default
 
\begin_inset Formula $X\sim Dir(\alpha_{1},\ldots,\alpha_{K})$
\end_inset

, 
\begin_inset Formula $Z_{i}\sim Multi(x)$
\end_inset

.
 
\begin_inset Formula $x|z_{1}\ldots z_{n}\sim Dir(\alpha_{1}+N_{1},\ldots,\alpha_{k}+N_{k})$
\end_inset

.
 And 
\begin_inset Formula $G|\bar{\theta}_{1},\ldots,\bar{\theta}_{N},\alpha,H\sim DP(\alpha+N,\frac{1}{\alpha+N}(\alpha H+\sum_{i=0}^{N}\delta_{\bar{\theta}_{i}}))$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Stick-Breaking Construction.

\series default
 Infinite sequence of mixture weights 
\begin_inset Formula $X=\{x_{k}\}_{k=1}^{\infty}$
\end_inset

, 
\begin_inset Formula $\beta_{k}\sim Beta(1,\alpha)$
\end_inset

.
 
\begin_inset Formula $x_{k}=\beta_{k}(1-\sum_{i=1}^{k-1}x_{i})$
\end_inset

.
 This is denoted as 
\begin_inset Formula $X\sim GEM(\alpha)$
\end_inset

.
\end_layout

\begin_layout Subsection
Gaussian Processes 
\end_layout

\begin_layout Standard
Problem setup.
 
\begin_inset Formula $X_{i}$
\end_inset

 input feature.
 
\begin_inset Formula $Y_{i}=f(X_{i})$
\end_inset

 output value.
 
\begin_inset Formula $f(\cdot)$
\end_inset

 is an unknown function.
 Goal: given 
\begin_inset Formula $D=\{(x_{i},y_{i})\}$
\end_inset

 predict output 
\begin_inset Formula $y_{\ast}$
\end_inset

 for new 
\begin_inset Formula $x_{*}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Prediction.

\series default
 (1) Infer the posterior distribution of 
\begin_inset Formula $f(x)$
\end_inset

; (2) marginalize over 
\begin_inset Formula $f(x)$
\end_inset

 to obtain 
\begin_inset Formula $y_{*}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
 & P(Y_{*}|X_{1:N},Y_{1:N},X_{*})\\
= & \int P(f(x),Y_{*}|X_{1:N},Y_{1:N},X_{*})df(x)\\
= & \int P(f(x)|X_{1:N},Y_{1:N})P(Y_{*}|f(x),X_{*})df(x)
\end{align*}

\end_inset


\end_layout

\begin_layout Standard

\series bold
Gaussian Process.

\series default
 Prior for 
\begin_inset Formula $f(\cdot)$
\end_inset

 w/o explicit parametrization.
 The distribution 
\begin_inset Formula $P(\cdot)$
\end_inset

 over 
\begin_inset Formula $f(x)$
\end_inset

 is a Gaussian Process if for any finite 
\begin_inset Formula $\{x_{1},\ldots,x_{N}\}$
\end_inset

 the vector 
\begin_inset Formula $f=[f(x_{1}),\ldots,f(x_{N})]^{T}$
\end_inset

 is Gaussian.
 
\begin_inset Formula $\mu=[E[f(x_{1}),\ldots]=[\mu(x_{1}),\ldots]$
\end_inset

.
 
\begin_inset Formula $\mathbb{K}=E[(f-\mu)(f-\mu)^{T}]$
\end_inset

 with 
\begin_inset Formula $K_{ij}=K(x_{i},x_{j})$
\end_inset

 PSD kernel Fn.
\end_layout

\begin_layout Standard

\series bold
GP Regression.

\series default
 
\begin_inset Formula $f(\cdot)\sim GP(\mu(x),K(x,x'))$
\end_inset

.
 (1) Noise free observations 
\begin_inset Formula $Y_{i}=f(x_{i})\triangleq f_{i}$
\end_inset

, training data 
\begin_inset Formula $D=\{(x_{1},f_{1}),\ldots,(x_{N},f_{N})\}$
\end_inset

.
 New observation 
\begin_inset Formula $x_{*}$
\end_inset

.
 
\begin_inset Formula $f=[f(x_{1}),\ldots]^{T}=[f_{1},\ldots]^{T}\sim\mathcal{N}(\mu,\mathbb{K})$
\end_inset

.
 
\begin_inset Formula $[f,f(x_{*})=f_{*}]^{T}\sim\mathcal{N}([\mu,\mu_{*}]^{T},[\mathbb{K},k_{*};k_{*}^{T},k_{*}^{\prime}])$
\end_inset

 where 
\begin_inset Formula $\mathbb{K}=K(x_{1:N},x_{1:N})$
\end_inset

, 
\begin_inset Formula $k_{*}=K(x_{1:N},x_{*})$
\end_inset

, 
\begin_inset Formula $k_{*}^{\prime}=K(x_{*},x_{*})$
\end_inset

.
 Posterior 
\begin_inset Formula $P(f_{*}|x_{*},D)\sim\mathcal{N}(m,\sigma^{2})$
\end_inset

 where 
\begin_inset Formula $m=E[f_{*}]+\Lambda_{*f}^{T}\Lambda_{f}^{-1}(f-\mu)=\mu_{*}+k_{*}^{T}\mathbb{K}^{-1}(f-\mu)$
\end_inset

, 
\begin_inset Formula $\sigma^{2}=k_{*}'-k_{*}^{T}\mathbb{K}^{-1}k_{*}$
\end_inset

.
 Generalize to multiple testing points 
\begin_inset Formula $x_{1}^{*},\ldots,x_{M}^{*}$
\end_inset

, 
\begin_inset Formula $[f,f_{*}]\sim\mathcal{N}([\mu,\mu_{*}]^{T},[\mathbb{K},\mathbb{K_{*}};\mathbb{K}_{*}^{T},\mathbb{K}_{*}^{\prime}])$
\end_inset

.
 
\begin_inset Formula $P(f|x_{1}^{*},\ldots,x_{M}^{*},D)=\mathcal{N}(f_{*};m,\Sigma)$
\end_inset

.
 
\begin_inset Formula $m=\mu_{*}+\mathbb{K}_{*}^{T}\mathbb{K}^{-1}(f-\mu)$
\end_inset

.
 
\begin_inset Formula $\Sigma=K_{*}^{\prime}-K_{*}^{T}K^{-1}K_{*}$
\end_inset

.
\end_layout

\begin_layout Standard
(2) Noisy observations.
 
\begin_inset Formula $Y_{i}=f(x_{i})+\epsilon_{i}$
\end_inset

, where 
\begin_inset Formula $\epsilon_{i}\sim iid\ \mathcal{N}(0,\sigma_{y}^{2})$
\end_inset

.
 
\begin_inset Formula $E[Y_{i}]=\mu_{i}$
\end_inset

, 
\begin_inset Formula $Cov(Y_{i},Y_{j})=K(x_{i},x_{j})+\sigma_{Y}^{2}\delta_{ij}$
\end_inset

.
 So 
\begin_inset Formula $Cov(Y_{1:N})=K+\sigma_{Y}^{2}I\triangleq K_{Y}$
\end_inset

.
 Single observation 
\begin_inset Formula $[Y_{1:N},f_{*}]^{T}\sim\mathcal{N}([\mu,\mu_{*}]^{T},[K_{Y},k_{*};k_{*}^{T},k_{*}^{\prime}])$
\end_inset

.
 
\begin_inset Formula $p(f_{*}|x_{*},D)=\mathcal{N}(f_{*};m,\sigma^{2})$
\end_inset

 where 
\begin_inset Formula $m=\mu_{*}+k_{*}^{T}K_{Y}^{-1}(y-\mu)$
\end_inset

, 
\begin_inset Formula $\sigma^{2}=k_{*}^{\prime}-k_{*}^{T}K_{Y}^{-1}k_{*}$
\end_inset

.
\end_layout

\begin_layout Standard

\series bold
Kernel Function.
 
\series default
(prediction performance).
 RBF Kernel.
 
\begin_inset Formula $K(x,x')=\beta\exp\{-\frac{1}{2r^{2}}\|x-x'\|_{F}^{2}\}$
\end_inset

.
 Or generalized form 
\begin_inset Formula $K(x,x')=\beta\exp\{-\frac{1}{2r^{2}}(x-x')^{T}M(x-x')\}$
\end_inset

 which can emphasize certain directions in data.
\end_layout

\begin_layout Subsection
A.
 Useful Distributions
\end_layout

\begin_layout Standard
(1) Normal 
\begin_inset Formula $X\sim\mathcal{N}(x;\mu,\sigma^{2})$
\end_inset

 (Continuous RV), 
\begin_inset Formula $E[X]=\mu$
\end_inset

, 
\begin_inset Formula $Var[X]=\sigma^{2}$
\end_inset


\begin_inset Formula 
\[
f_{X}(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\{-\frac{1}{2}\frac{(x-\mu)^{2}}{\sigma^{2}}\}
\]

\end_inset


\end_layout

\begin_layout Standard
(2) Uniform 
\begin_inset Formula $X\sim Unf(x;[a,b])$
\end_inset

 (Continuous), 
\begin_inset Formula $E[X]=\frac{a+b}{2}$
\end_inset

, 
\begin_inset Formula $Var[X]=\frac{(b-a)^{2}}{12}$
\end_inset

.
\begin_inset Formula 
\[
f_{X}(x)=\frac{1}{b-a}\mathbb{I}\{x\in[a,b]\}+0\mathbb{I}\{x\notin[a,b]\}
\]

\end_inset


\end_layout

\begin_layout Standard
(3) Exponential 
\begin_inset Formula $X\sim Exp(x;\lambda)$
\end_inset

 (Continuous), 
\begin_inset Formula $E[X]=1/\lambda$
\end_inset

, 
\begin_inset Formula $Var[X]=1/\lambda^{2}$
\end_inset

.
\begin_inset Formula 
\[
f_{X}(x)=\lambda e^{-\lambda x}u(x)
\]

\end_inset


\end_layout

\begin_layout Standard
(4) Bernoulli 
\begin_inset Formula $X\sim B(x;p)$
\end_inset

 (discrete RV), 
\begin_inset Formula $E[X]=p$
\end_inset

, 
\begin_inset Formula $Var[X]=p(1-p)$
\end_inset

.
\begin_inset Formula 
\[
f_{X}(x)=p^{x}(1-p)^{1-x}
\]

\end_inset


\end_layout

\begin_layout Standard
(5) Poisson 
\begin_inset Formula $X\sim Poisson(x;\mu)$
\end_inset

 (discrete), 
\begin_inset Formula $E[X]=\mu$
\end_inset

, 
\begin_inset Formula $Var[X]=\mu$
\end_inset

.
\begin_inset Formula 
\[
f_{X}(x)=\frac{\mu^{x}}{x!}e^{-\mu},x=0,1,2,\ldots
\]

\end_inset


\end_layout

\begin_layout Standard
(6) Beta 
\begin_inset Formula $B(\alpha,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$
\end_inset

 where 
\begin_inset Formula $\Gamma(z)=\int_{0}^{\infty}u^{z-1}e^{-u}du$
\end_inset

, namely 
\begin_inset Formula $\Gamma(z+1)=z\Gamma(z)$
\end_inset

.
 
\begin_inset Formula $E[X]=\frac{\alpha}{\alpha+\beta}$
\end_inset

, 
\begin_inset Formula $Var[X]=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}$
\end_inset

.
\begin_inset Formula 
\[
f_{X}(x)=\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}
\]

\end_inset


\end_layout

\begin_layout Subsection
B.
 Tricks
\end_layout

\begin_layout Standard
\begin_inset Formula $(AB)^{-1}=B^{-1}A^{-1}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $A$
\end_inset

 PSD means 
\begin_inset Formula $x\in R^{n}\notin\{0\}$
\end_inset

, 
\begin_inset Formula $x^{T}Ax\geq0$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $E[Ax]=AE[x]$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $Var[Ax]=AVar[x]A^{T}$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $E[tr(AB)]=tr(E[AB])$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $\det|\alpha A|=\alpha^{n}A$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $tr(AB)=tr(BA)$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula $|Cov(x,y)|\leq\sqrt{Var(x)Var(y)}$
\end_inset


\end_layout

\begin_layout Standard
Gaussian Integral 
\begin_inset Formula $\int_{-\infty}^{+\infty}e^{-x^{2}}dx=\sqrt{\pi}$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula $\int_{-\infty}^{+\infty}ae^{-\frac{(x-b)^{2}}{2c^{2}}}dx=ac\sqrt{2\pi}$
\end_inset

.
\end_layout

\begin_layout Standard
Integral by parts.
 
\begin_inset Formula $\int udv=uv-\int vdu$
\end_inset

.
\end_layout

\begin_layout Standard
steady markov: 
\begin_inset Formula $\pi=A\pi$
\end_inset

.
\end_layout

\end_body
\end_document
