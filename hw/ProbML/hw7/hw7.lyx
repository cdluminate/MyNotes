#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
todonotes
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "newtxmath" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype true
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\listings_params "language=Matlab,frame=leftline,numbers=left,basicstyle={\ttfamily}"
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
EN 520.665 HW#7
\end_layout

\begin_layout Author
Mo Zhou <mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Problem 7.1
\end_layout

\begin_layout Subsection
(a)
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

data = load('data.mat');
\end_layout

\begin_layout Plain Layout

X = data.X;
\end_layout

\begin_layout Plain Layout

scatter3(X(:,1), X(:,2), X(:,3));
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename scatter3x.svg
	scale 70

\end_inset


\end_layout

\begin_layout Standard
The natural number of clusters 
\begin_inset Formula $K=5$
\end_inset

 in this dataset according to the scatter plot.
\end_layout

\begin_layout Subsection
(b)
\end_layout

\begin_layout Standard
We first create the 
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
verb|KMeans.m|
\end_layout

\end_inset

 file, as follows:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

% K-Means function to locate K clusters from input dataset
\end_layout

\begin_layout Plain Layout

% Author Name: Mo Zhou
\end_layout

\begin_layout Plain Layout

function [C, labels] = KMeans(X, K, maxIter)
\end_layout

\begin_layout Plain Layout

% Input:
\end_layout

\begin_layout Plain Layout

%  X: dataset, observations in rows, variables in columns.
\end_layout

\begin_layout Plain Layout

%  K: "K"-means
\end_layout

\begin_layout Plain Layout

% Output:
\end_layout

\begin_layout Plain Layout

%  C: centroids
\end_layout

\begin_layout Plain Layout

%  labels: class labels assigned to each observation in X
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

N = size(X, 1);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% Randomly initializing K centroids
\end_layout

\begin_layout Plain Layout

Ri = randi(N, [K, 1]);
\end_layout

\begin_layout Plain Layout

C = X(Ri, :);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% start k-means loop
\end_layout

\begin_layout Plain Layout

for iter = 1:maxIter
\end_layout

\begin_layout Plain Layout

    % assign data to clusters
\end_layout

\begin_layout Plain Layout

    dist = zeros(N, K);
\end_layout

\begin_layout Plain Layout

    for i = 1:K
\end_layout

\begin_layout Plain Layout

        d = vecnorm(C(i, :) - X, 2, 2);
\end_layout

\begin_layout Plain Layout

        dist(:, i) = d;
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

    [~, argmin] = min(dist, [], 2);
\end_layout

\begin_layout Plain Layout

    % re-compute centroids
\end_layout

\begin_layout Plain Layout

    for i = 1:K
\end_layout

\begin_layout Plain Layout

        idx = find(argmin == i);
\end_layout

\begin_layout Plain Layout

        ci = mean(X(idx, :), 1);
\end_layout

\begin_layout Plain Layout

        C(i, :) = ci;
\end_layout

\begin_layout Plain Layout

    end
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

% assign labels to X
\end_layout

\begin_layout Plain Layout

dist = zeros(N, K);
\end_layout

\begin_layout Plain Layout

for i = 1:K
\end_layout

\begin_layout Plain Layout

    d = vecnorm(C(i, :) - X, 2, 2);
\end_layout

\begin_layout Plain Layout

    dist(:, i) = d;
\end_layout

\begin_layout Plain Layout

end
\end_layout

\begin_layout Plain Layout

[~, argmin] = min(dist, [], 2);
\end_layout

\begin_layout Plain Layout

labels = argmin;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

end
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Then we cluster the given data and visualize:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

[C, labels] = KMeans(X, 5, 10);
\end_layout

\begin_layout Plain Layout

scatter3(X(:,1), X(:,2), X(:,3), [], labels);
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename scatter3xkmeans.svg
	scale 70

\end_inset


\end_layout

\begin_layout Section
Problem 7.2
\end_layout

\begin_layout Subsection
(a)
\end_layout

\begin_layout Standard
The posterior density 
\begin_inset Formula $P(x|\bm{y})$
\end_inset

 is
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P(x|y_{1},y_{2},\ldots,y_{N}) & =\frac{P(X)\prod_{i}^{N}P(y_{i}|x)}{P(y_{1},y_{2},\ldots,y_{N})}\\
 & =\frac{x^{N}e^{-x}\prod_{i}^{N}y_{i}^{x-1}u(x)}{P(\bm{y})}\quad\forall i,y_{i}\in[0,1]
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and we have
\begin_inset Formula 
\begin{align*}
\log P(x|\bm{y}) & =N\log(x)-x+(x-1)\sum_{i}y_{i}-\log P(\hm{y})\qquad x>0,y_{i}\in[0,1]\\
\frac{\partial}{\partial x}\log P(x|\bm{y}) & =\frac{N}{x}-1+\sum_{i}y_{i}\\
\frac{\partial^{2}}{\partial x^{2}}\log P(x|\bm{y}) & =-\frac{N}{x^{2}}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
(b)
\end_layout

\begin_layout Standard
First the likelihood is
\begin_inset Formula 
\begin{align*}
P(y_{1},\ldots,y_{N}|x) & =\prod_{i}^{N}p(y_{i}|x)=x^{N}\prod_{i}^{N}y_{i}^{x-1}\\
\log P(\bm{y}|x) & =N\log(x)+(x-1)\sum_{i}^{N}y_{i}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
And the ML estimator is
\begin_inset Formula 
\begin{align*}
\hat{x}_{ML} & =\arg\max_{x}P(\bm{y}|x)\\
\frac{\partial}{\partial x}\log P(\bm{y}|x) & =0=\frac{N}{x}+\sum_{i}^{N}y_{i}\\
\Rightarrow & x=-\frac{N}{\sum_{i}^{N}y_{i}}=\hat{x}_{ML}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
The observed Fisher information
\begin_inset Formula 
\[
J(\hat{x})=-\frac{\partial^{2}}{\partial x^{2}}\log P(\bm{y}|x)=\frac{N}{x^{2}}
\]

\end_inset


\end_layout

\begin_layout Standard
Then the Laplace's approximation for the posterior is
\begin_inset Formula 
\begin{align*}
\hat{x}_{MAP} & =\frac{N}{1-\sum_{i}y_{i}}\Leftarrow\frac{\partial}{\partial x}\log P(x|\bm{y})=0\\
\frac{\partial^{2}}{\partial x^{2}}\log P(x) & =0\\
\hat{\sigma}^{2} & =\Big[-\frac{\partial^{2}}{\partial x^{2}}\log P(x)|_{\hat{x}_{MAP}}-\frac{\partial^{2}}{\partial x^{2}}\log P(\bm{y}|x)|_{\hat{x}_{MAP}}\Big]^{-1}\\
 & =(\hat{x}_{MAP})^{2}/N\\
\log P(x|\bm{y}) & \approx\mathcal{N}(x;\hat{x}_{MAP},\hat{\sigma}^{2})\\
 & =\frac{1}{\sqrt{2\pi(\frac{1}{J(\hat{x})})}}\exp\{-\frac{1}{2}J(\hat{x})(x-\hat{x})^{2}\}\\
 & =\frac{1}{\sqrt{2\pi\frac{\hat{x}^{2}}{N}}}\exp\{-\frac{N}{2\hat{x}^{2}}(x-\hat{x})^{2}\}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
(c)
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

N = 1;
\end_layout

\begin_layout Plain Layout

e = exp(1);
\end_layout

\begin_layout Plain Layout

x = 0:.1:10;
\end_layout

\begin_layout Plain Layout

y = .5;
\end_layout

\begin_layout Plain Layout

p = x.^(N).*e.^(-x).*(y.^(x-1)).^N;
\end_layout

\begin_layout Plain Layout

figure;
\end_layout

\begin_layout Plain Layout

hold on;
\end_layout

\begin_layout Plain Layout

plot(x, p);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

xhat = N/(1-(y*N));
\end_layout

\begin_layout Plain Layout

sigma2 = xhat.^2./N;
\end_layout

\begin_layout Plain Layout

jx = N./(xhat^2);
\end_layout

\begin_layout Plain Layout

approx = 1./(2*pi./jx) * exp(-0.5.*jx.*(x-xhat).^2);
\end_layout

\begin_layout Plain Layout

plot(x, approx);
\end_layout

\begin_layout Plain Layout

title(sprintf("N=%d", N));
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Graphics
	filename 72n1.pdf
	width 32text%

\end_inset


\begin_inset Graphics
	filename 72n10.pdf
	width 32text%

\end_inset


\begin_inset Graphics
	filename 72n100.pdf
	width 32text%

\end_inset


\end_layout

\begin_layout Section
Problem 7.3
\end_layout

\begin_layout Subsection
(a)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
P(y_{1},y_{2}) & =\epsilon_{1}^{y_{1}}(1-\epsilon_{1})^{1-y_{1}}\epsilon_{2}^{y_{2}}(1-\epsilon_{2})^{1-y_{2}}\\
q(y_{1},y_{2}) & =\epsilon^{y_{1}+y_{2}}(1-\epsilon)^{2-y_{1}-y_{2}}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
D(q\|P_{Y_{1}Y_{2}}) & =E_{q}[\log\frac{q}{P_{Y_{1}Y_{2}}}]\\
 & =E_{q}[(y_{1}+y_{2})\log\epsilon+(2-y_{1}-y_{2})\log(1-\epsilon)-\sum_{i}(y_{i}\log\epsilon_{i}+(1-y_{i})\log(1-\epsilon_{i}))]\\
(E_{q}[y_{i}]=\epsilon)\Rightarrow & =2\epsilon\log\epsilon+(2-2\epsilon)\log(1-\epsilon)-\sum_{i}(\epsilon\log(\epsilon_{i})+(1-\epsilon)\log(1-\epsilon_{i}))\\
\frac{\partial}{\partial\epsilon}D(q\|P_{Y_{1}Y_{2}}) & =2\log\frac{\epsilon}{1-\epsilon}-\sum_{i}\log\frac{\epsilon_{i}}{1-\epsilon_{i}}=0\\
\frac{\epsilon}{1-\epsilon} & =\prod_{i}\exp\frac{\epsilon_{i}}{2(1-\epsilon_{i})}\triangleq Q\\
\epsilon & =\frac{Q}{1+Q}=\frac{\prod_{i}\exp\frac{\epsilon_{i}}{2(1-\epsilon_{i})}}{1+\prod_{i}\exp\frac{\epsilon_{i}}{2(1-\epsilon_{i})}}
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
(b)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
D(P_{Y_{1}Y_{2}}\|q) & =E_{P}[\log\frac{P_{Y_{1}Y_{2}}}{q}]\\
(E_{P}[y_{i}]=\epsilon_{i})\Rightarrow & =-(\epsilon_{1}+\epsilon_{2})\log\epsilon-(2-\epsilon_{1}-\epsilon_{2})\log(1-\epsilon)+constant\\
\frac{\partial}{\partial\epsilon}D(P_{Y_{1}Y_{2}}\|q) & =\frac{2-\epsilon_{1}-\epsilon_{2}}{1-\epsilon}-\frac{\epsilon_{1}+\epsilon_{2}}{\epsilon}=0\\
\epsilon & =\frac{\epsilon_{1}+\epsilon_{2}}{2}
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Problem 7.4
\end_layout

\begin_layout Subsection
(a)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
D(p_{XY}\|q) & =E_{p}[\log\frac{p_{XY}}{q}]\\
 & =E_{p}[\log(p_{XY})-\log q_{X}-\log q_{Y}]\\
 & =E_{p}[\log p_{X|Y}+\log p_{Y}-\log q_{X}-\log q_{Y}]
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
Still have no idea on how to minimize the KL divergence after factorizing
 
\begin_inset Formula $p_{XY}$
\end_inset

...
\end_layout

\begin_layout Subsection
(b)
\end_layout

\begin_layout Section
Problem 7.5
\end_layout

\begin_layout Subsection
(a)
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
w(0) & =p(0)/q(0)=3/4\\
w(1) & =3/2\\
w(2) & =3/4
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\hat{p}\triangleq\sum_{i=1}^{n}\frac{w(x_{i})}{\sum_{j=1}^{n}w(x_{j})}p(x_{i})
\]

\end_inset


\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

w = [3/4, 3/2, 3/4];
\end_layout

\begin_layout Plain Layout

x = [1,2,0,2,2,1,0,0,1,2];
\end_layout

\begin_layout Plain Layout

c = sum(w(x+1));
\end_layout

\begin_layout Plain Layout

p = [1/4, 1/2, 1/4];
\end_layout

\begin_layout Plain Layout

sum(w(x+1)/c .* p(x+1))
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(i) The empirical estimate of mean is 
\begin_inset Formula $\hat{\mu}=0.3654$
\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

>> x = [1,2,0,2,2,1,0,0,1,2];
\end_layout

\begin_layout Plain Layout

>> mean(x)
\end_layout

\begin_layout Plain Layout

ans = 1.1000
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(ii) when the samples are drawn from 
\begin_inset Formula $p(x)$
\end_inset

, the estimate changes into 
\begin_inset Formula $1.1$
\end_inset

.
\end_layout

\begin_layout Section
Problem 7.6
\end_layout

\end_body
\end_document
