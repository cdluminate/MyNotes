#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 2
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
EN 520.665 HW#2
\end_layout

\begin_layout Author
Mo Zhou <mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Q1
\end_layout

\begin_layout Standard
Consider a first order Gaussian Markov random field with a single coefficient.
 Derive the constraint on the coefficient so that the Markov random field
 is stable.
 Calculate the eigenvalues and eigenvector of the covariance matrix of the
 data obeying this MRF model assuming doubly-periodic boundary conditions.
\end_layout

\begin_layout Section
Q2
\end_layout

\begin_layout Standard
Compare the computational complexity of Gibbs and Metropolis sampling algorithm
 for synthesizing an NxN texture image.
\end_layout

\begin_layout Section
Q3 
\end_layout

\begin_layout Standard
Calculate the number of parameters in AlexNet, VGGnet, ResNet and InceptionNet.
\end_layout

\begin_layout Section
Q4
\end_layout

\begin_layout Standard

\bar under
Show that if the transfer function of the hidden units is linear, a three-layer
 network is equivalent to a two-layer one.
 Explain why, therefore, that a three-layer network with linear hidden units
 cannot solve a non-linearly separable problem such as XOR or n-bit parity.
 
\end_layout

\begin_layout Standard
Let's use the matrix notation for the neural network.
 Denote 
\begin_inset Formula $W_{l}$
\end_inset

 as the weight of the 
\begin_inset Formula $l$
\end_inset

-th layer, 
\begin_inset Formula $I$
\end_inset

 as the input, 
\begin_inset Formula $A_{l}$
\end_inset

 as the pre-activation output of the 
\begin_inset Formula $l$
\end_inset

-th layer, and 
\begin_inset Formula $B_{l}$
\end_inset

 as the activated output of the 
\begin_inset Formula $l$
\end_inset

-th layer, and 
\begin_inset Formula $\sigma_{l}$
\end_inset

 as the activation function of the 
\begin_inset Formula $l$
\end_inset

-th layer.
 We omit the bias term or convenience like did in many textbooks (the bias
 can be concatenated to the weight matrix, and a constant 
\begin_inset Formula $1$
\end_inset

 will be appended to the layer's input).
 In such setting, a three-layer neural network will look like:
\begin_inset Formula 
\[
A_{3}=W_{3}\sigma_{2}(W_{2}\sigma_{1}(W_{1}I)).
\]

\end_inset


\end_layout

\begin_layout Standard
If the transfer function, or say activation function of the hidden units
 is linear, it means 
\begin_inset Formula $\sigma_{2}$
\end_inset

 is linear transformation that could be represented by matrix 
\begin_inset Formula $T_{2}$
\end_inset

 (any linear transformation can be represented by matrix multiplication).
 If the question meant 
\begin_inset Quotes eld
\end_inset

doing nothing
\begin_inset Quotes erd
\end_inset

 for activation, then 
\begin_inset Formula $T_{2}$
\end_inset

 can be an identity matrix.
 So let's plug 
\begin_inset Formula $T_{2}$
\end_inset

 into the above equation, and obtain
\begin_inset Formula 
\[
A_{3}=W_{3}T_{2}W_{2}\sigma_{1}(W_{1}I)).
\]

\end_inset


\end_layout

\begin_layout Standard
Clearly, without a non-linear activation function for the hidden layer,
 a three-layer neural network could degenerate into a two-layer neural network
 where its weight matrix for the second layer 
\begin_inset Formula $W_{2}^{\prime}=W_{3}T_{2}W_{2}$
\end_inset

.
 This is mathematically equivalent.
 
\end_layout

\begin_layout Section
Q5
\end_layout

\begin_layout Standard

\bar under
What problem(s) will result from using a learning rate that is too high?
 How would you detect these problems? What problem(s) will result from using
 a learning rate that is too low? How would you detect these problems?
\end_layout

\begin_layout Standard
(1) When the learning rate is excessively high, the optimizer may drive
 the parameters off their desired range, and lead to an diverging loss curve.
 In the worst case, it may even cause the loss value being NaN or Inf due
 to numerical precision issues or division by zero.
 The loss curve on the training set during the training process is a good
 way to diagnose and detect such problem.
\end_layout

\begin_layout Standard
(2) When the learning rate is too low, the loss may decrease as usual, but
 at a extremely slow rate.
 An ideal loss curve should look like a 
\begin_inset Formula $e^{-\alpha x}$
\end_inset

 function.
 So a good way to detect such issue is to plot and check the loss curve.
\end_layout

\begin_layout Standard
In general practice of deep learning, there is not any default learning
 rate that is guaranteed to work.
 An appropriate learning rate depends on the parameter initlaization method,
 the optimizer, the data and loss function, etc.
 A good practice to search for a good learning rate for a neural network
 is to start from the powers of 
\begin_inset Formula $10^{-1}$
\end_inset

, for instance 
\begin_inset Formula $[10^{-1},10^{-2},\ldots,10^{-5}]$
\end_inset

.
 Empirically 
\begin_inset Formula $10^{-3}$
\end_inset

 may be a good start for training a network from scratch, and 
\begin_inset Formula $10^{-5}$
\end_inset

 may be a good start for fine-tunining a network.
 An appropriate learning rate could be the largest learning rate at which
 the loss curve could converge during the training process.
\end_layout

\end_body
\end_document
