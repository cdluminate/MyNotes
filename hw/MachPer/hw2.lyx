#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "default" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
EN 520.665 HW#2
\end_layout

\begin_layout Author
Mo Zhou <mzhou32@jhu.edu>
\end_layout

\begin_layout Section
Q1
\end_layout

\begin_layout Standard

\bar under
Consider a first order Gaussian Markov random field with a single coefficient.
 Derive the constraint on the coefficient so that the Markov random field
 is stable.
 Calculate the eigenvalues and eigenvector of the covariance matrix of the
 data obeying this MRF model assuming doubly-periodic boundary conditions.
\end_layout

\begin_layout Standard
We use the notations in the lecture note.
 The first order GMRF with a single coefficient is
\begin_inset Formula 
\[
y(s)=\sum_{r\in N}\theta(s\mod r)+e(s)
\]

\end_inset

where 
\begin_inset Formula $N=\{(0,-1),(0,1),(-1,0),(1,0)\}$
\end_inset

.
 The model 
\begin_inset Formula $y(s)$
\end_inset

 can be represented in vector-matrix form using a symmetric block-circulation
 matrix 
\begin_inset Formula $B(\theta)$
\end_inset

 as
\begin_inset Formula 
\[
B(\theta)y=e
\]

\end_inset


\end_layout

\begin_layout Standard
In order for this to be stable, the covariance matrix must be positive definite.
 As a result, we know that the eigenvalues 
\begin_inset Formula $\mu_{k}$
\end_inset

 are also greater than 
\begin_inset Formula $0$
\end_inset

.
 For block-circulant matrix, the eigenvalues are the discrete Fourier of
 the first row.
 Then, we have the following stability condition for 
\begin_inset Formula $\{y(s)\}$
\end_inset

:
\begin_inset Formula 
\[
\mu_{k}=1-\vec{\theta}^{T}\phi_{k}>0,k\in K
\]

\end_inset


\end_layout

\begin_layout Standard
Since there is only one parameter, elements in vector 
\begin_inset Formula $\vec{\theta}$
\end_inset

 are the same value 
\begin_inset Formula $\theta$
\end_inset

.
 According to page 89 of the corresponding paper, when 
\begin_inset Formula $y(s)$
\end_inset

 is represented in a toroidal lattice, then 
\begin_inset Formula $B^{-1}(\theta)$
\end_inset

 is a block-circulent matrix with eigenvalues 
\begin_inset Formula $1/\mu_{s}$
\end_inset

 and eigen vectors
\begin_inset Formula 
\[
f_{S}=\text{Col.}[1,\lambda_{i},\lambda_{i}^{2}t_{j},\ldots,\lambda_{i}^{M-1}t_{j}],s=(i,j)
\]

\end_inset


\end_layout

\begin_layout Standard
which is an 
\begin_inset Formula $M^{2}$
\end_inset

 vector.
\end_layout

\begin_layout Section
Q2
\end_layout

\begin_layout Standard
Compare the computational complexity of Gibbs and Metropolis sampling algorithm
 for synthesizing an NxN texture image.
\end_layout

\begin_layout Standard
According to the reference material, i.e.
 page 66 of 
\begin_inset Quotes eld
\end_inset

3.
 Low Level MRF Models
\begin_inset Quotes erd
\end_inset

, the Metropolis sampler repeats 
\begin_inset Quotes eld
\end_inset

step (2)
\begin_inset Quotes erd
\end_inset

 for 
\begin_inset Formula $N$
\end_inset

 times.
 The 
\begin_inset Quotes eld
\end_inset

step (2)
\begin_inset Quotes erd
\end_inset

 traverses all elements in point set 
\begin_inset Formula $\mathcal{S}$
\end_inset

.
 The steps in (2) has constant complexity.
 In summary, the complexity of Metropolis sampler is
\begin_inset Formula 
\[
O_{\text{Metropolis}}(N\times|\mathcal{S}|)
\]

\end_inset

where 
\begin_inset Formula $|\mathcal{S}|$
\end_inset

 means the size of the set 
\begin_inset Formula $\mathcal{S}$
\end_inset

.
\end_layout

\begin_layout Standard
According to the same page of the reference material, the step (3) of Gibbs
 sampler repeats step (2) for 
\begin_inset Formula $N$
\end_inset

 times.
 Step (2) traverses through the point set 
\begin_inset Formula $\mathcal{S}$
\end_inset

.
 Step (2.1) computes probability values for all 
\begin_inset Formula $l\in\mathbf{\mathcal{L}}$
\end_inset

.
 The complexity of this algorithm is
\begin_inset Formula 
\[
O_{\text{Gibbs}}(N\times|\mathcal{S}|\times|\mathcal{L}|)
\]

\end_inset


\end_layout

\begin_layout Standard
If I understood correctly, the Gibbs sampler is 
\begin_inset Formula $|\mathcal{L}|$
\end_inset

 times more complex than the Metropolis sampler.
\end_layout

\begin_layout Section
Q3 
\end_layout

\begin_layout Standard

\bar under
Calculate the number of parameters in AlexNet, VGGnet, ResNet and InceptionNet.
\end_layout

\begin_layout Standard
(1) AlexNet.
 Number of parameters is: Conv1 (11*11*3*96+96) + Conv2 (5*5*48*256+256)
 + Conv3 (3*3*256*384+384) + Conv4 (3*3*192*384+384) + Conv5 (3*3*192*256+256)
 + Fc6 (6*6*256*4096 + 4096) + Fc7 (4096 * 4096 + 4096) + Fc8 (4096 * 1000
 + 1000).
 Torchvision's AlexNet is slightly different from the original paper, hence
 a slight difference in number of parameters.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

#!/usr/bin/python3
\end_layout

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

print('Nparams of AlexNet:',
\end_layout

\begin_layout Plain Layout

    np.sum([11*11*3*96 + 96,
\end_layout

\begin_layout Plain Layout

    5*5*48*256 + 256,
\end_layout

\begin_layout Plain Layout

    3*3*256*384 + 384,
\end_layout

\begin_layout Plain Layout

    3*3*192*384 + 384,
\end_layout

\begin_layout Plain Layout

    3*3*192*256 + 256,
\end_layout

\begin_layout Plain Layout

    6*6*256*4096 + 4096,
\end_layout

\begin_layout Plain Layout

    4096*4096 + 4096,
\end_layout

\begin_layout Plain Layout

    4096*1000 + 1000]))
\end_layout

\begin_layout Plain Layout

import torchvision as V
\end_layout

\begin_layout Plain Layout

print('Nparams of Torchvision.AlexNet:',
\end_layout

\begin_layout Plain Layout

        np.sum([x.numel() for x in
\end_layout

\begin_layout Plain Layout

        V.models.alexnet().parameters()]))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Nparams of AlexNet: 60965224
\end_layout

\begin_layout Plain Layout

Nparams of Torchvision.AlexNet: 61100840 
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(2) VGG16.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

print('Nparams of VGG16:',
\end_layout

\begin_layout Plain Layout

    np.sum([
\end_layout

\begin_layout Plain Layout

    3**2 * 3 * 64 + 64,
\end_layout

\begin_layout Plain Layout

    3**2 * 64 * 64 + 64,
\end_layout

\begin_layout Plain Layout

    #
\end_layout

\begin_layout Plain Layout

    3**2 * 64 * 128 + 128,
\end_layout

\begin_layout Plain Layout

    3**2 * 128 * 128 + 128,
\end_layout

\begin_layout Plain Layout

    #
\end_layout

\begin_layout Plain Layout

    3**2 * 128 * 256 + 256,
\end_layout

\begin_layout Plain Layout

    3**2 * 256 * 256 + 256,
\end_layout

\begin_layout Plain Layout

    3**2 * 256 * 256 + 256,
\end_layout

\begin_layout Plain Layout

    #
\end_layout

\begin_layout Plain Layout

    3**2 * 256 * 512 + 512,
\end_layout

\begin_layout Plain Layout

    3**2 * 512 * 512 + 512,
\end_layout

\begin_layout Plain Layout

    3**2 * 512 * 512 + 512,
\end_layout

\begin_layout Plain Layout

    #
\end_layout

\begin_layout Plain Layout

    3**2 * 512 * 512 + 512,
\end_layout

\begin_layout Plain Layout

    3**2 * 512 * 512 + 512,
\end_layout

\begin_layout Plain Layout

    3**2 * 512 * 512 + 512,
\end_layout

\begin_layout Plain Layout

    #
\end_layout

\begin_layout Plain Layout

    25088 * 4096 + 4096,
\end_layout

\begin_layout Plain Layout

    4096 * 4096 + 4096,
\end_layout

\begin_layout Plain Layout

    4096 * 1000 + 1000,
\end_layout

\begin_layout Plain Layout

    ]))
\end_layout

\begin_layout Plain Layout

import torchvision as V
\end_layout

\begin_layout Plain Layout

print('Nparams of Torchvision.VGG16:',
\end_layout

\begin_layout Plain Layout

        np.sum([x.numel() for x in
\end_layout

\begin_layout Plain Layout

        V.models.vgg16().parameters()]))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Nparams of VGG16: 138357544
\end_layout

\begin_layout Plain Layout

Nparams of Torchvision.VGG16: 138357544
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(3) ResNet.
 ResNet has several frequently used depth: ResNet18, ResNet34, ResNet50,
 ResNet101, ResNet152.
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

import torchvision as V
\end_layout

\begin_layout Plain Layout

M = ['resnet'+str(x) for x in (18, 34, 50, 101, 152)]
\end_layout

\begin_layout Plain Layout

for m in M:
\end_layout

\begin_layout Plain Layout

    params = getattr(V.models, m)().parameters()
\end_layout

\begin_layout Plain Layout

    print(f'Nparams for {m}:', np.sum([x.numel()
\end_layout

\begin_layout Plain Layout

        for x in params]))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Nparams for resnet18: 11689512
\end_layout

\begin_layout Plain Layout

Nparams for resnet34: 21797672
\end_layout

\begin_layout Plain Layout

Nparams for resnet50: 25557032
\end_layout

\begin_layout Plain Layout

Nparams for resnet101: 44549160
\end_layout

\begin_layout Plain Layout

Nparams for resnet152: 60192808
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(4) Inception V3
\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{verbatim}
\end_layout

\begin_layout Plain Layout

import numpy as np
\end_layout

\begin_layout Plain Layout

import torchvision as V
\end_layout

\begin_layout Plain Layout

params = V.models.inception_v3().parameters()
\end_layout

\begin_layout Plain Layout

print('Nparams for Inception V3:', np.sum([x.numel() for x in params]))
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

Nparams for Inception V3: 27161264
\end_layout

\begin_layout Plain Layout


\backslash
end{verbatim}
\end_layout

\end_inset


\end_layout

\begin_layout Section
Q4
\end_layout

\begin_layout Standard

\bar under
Show that if the transfer function of the hidden units is linear, a three-layer
 network is equivalent to a two-layer one.
 Explain why, therefore, that a three-layer network with linear hidden units
 cannot solve a non-linearly separable problem such as XOR or n-bit parity.
 
\end_layout

\begin_layout Standard
Let's use the matrix notation for the neural network.
 Denote 
\begin_inset Formula $W_{l}$
\end_inset

 as the weight of the 
\begin_inset Formula $l$
\end_inset

-th layer, 
\begin_inset Formula $I$
\end_inset

 as the input, 
\begin_inset Formula $A_{l}$
\end_inset

 as the pre-activation output of the 
\begin_inset Formula $l$
\end_inset

-th layer, and 
\begin_inset Formula $B_{l}$
\end_inset

 as the activated output of the 
\begin_inset Formula $l$
\end_inset

-th layer, and 
\begin_inset Formula $\sigma_{l}$
\end_inset

 as the activation function of the 
\begin_inset Formula $l$
\end_inset

-th layer.
 We omit the bias term or convenience like did in many textbooks (the bias
 can be concatenated to the weight matrix, and a constant 
\begin_inset Formula $1$
\end_inset

 will be appended to the layer's input).
 In such setting, a three-layer neural network will look like:
\begin_inset Formula 
\[
A_{3}=W_{3}\sigma_{2}(W_{2}\sigma_{1}(W_{1}I)).
\]

\end_inset


\end_layout

\begin_layout Standard
If the transfer function, or say activation function of the hidden units
 is linear, it means 
\begin_inset Formula $\sigma_{2}$
\end_inset

 is linear transformation that could be represented by matrix 
\begin_inset Formula $T_{2}$
\end_inset

 (any linear transformation can be represented by matrix multiplication).
 If the question meant 
\begin_inset Quotes eld
\end_inset

doing nothing
\begin_inset Quotes erd
\end_inset

 for activation, then 
\begin_inset Formula $T_{2}$
\end_inset

 can be an identity matrix.
 So let's plug 
\begin_inset Formula $T_{2}$
\end_inset

 into the above equation, and obtain
\begin_inset Formula 
\[
A_{3}=W_{3}T_{2}W_{2}\sigma_{1}(W_{1}I)).
\]

\end_inset


\end_layout

\begin_layout Standard
Clearly, without a non-linear activation function for the hidden layer,
 a three-layer neural network could degenerate into a two-layer neural network
 where its weight matrix for the second layer 
\begin_inset Formula $W_{2}^{\prime}=W_{3}T_{2}W_{2}$
\end_inset

.
 This is mathematically equivalent.
 
\end_layout

\begin_layout Section
Q5
\end_layout

\begin_layout Standard

\bar under
What problem(s) will result from using a learning rate that is too high?
 How would you detect these problems? What problem(s) will result from using
 a learning rate that is too low? How would you detect these problems?
\end_layout

\begin_layout Standard
(1) When the learning rate is excessively high, the optimizer may drive
 the parameters off their desired range, and lead to an diverging loss curve.
 In the worst case, it may even cause the loss value being NaN or Inf due
 to numerical precision issues or division by zero.
 The loss curve on the training set during the training process is a good
 way to diagnose and detect such problem.
\end_layout

\begin_layout Standard
(2) When the learning rate is too low, the loss may decrease as usual, but
 at a extremely slow rate.
 An ideal loss curve should look like a 
\begin_inset Formula $e^{-\alpha x}$
\end_inset

 function.
 So a good way to detect such issue is to plot and check the loss curve.
\end_layout

\begin_layout Standard
In general practice of deep learning, there is not any default learning
 rate that is guaranteed to work.
 An appropriate learning rate depends on the parameter initlaization method,
 the optimizer, the data and loss function, etc.
 A good practice to search for a good learning rate for a neural network
 is to start from the powers of 
\begin_inset Formula $10^{-1}$
\end_inset

, for instance 
\begin_inset Formula $[10^{-1},10^{-2},\ldots,10^{-5}]$
\end_inset

.
 Empirically 
\begin_inset Formula $10^{-3}$
\end_inset

 may be a good start for training a network from scratch, and 
\begin_inset Formula $10^{-5}$
\end_inset

 may be a good start for fine-tunining a network.
 An appropriate learning rate could be the largest learning rate at which
 the loss curve could converge during the training process.
\end_layout

\end_body
\end_document
